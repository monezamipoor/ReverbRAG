{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from retriever import FurnishedRoomSTFTDataset, compute_audio_distance, RIRRetrievalMLP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─── Config ──────────────────────────────────────────────────────────\n",
    "root            = '../data/RAF/FurnishedRoom'\n",
    "ckpt_path       = './outputs/20250730_171523/rir_retrieval_model.ckpt'\n",
    "grid_vec_p      = \"./features.pt\"\n",
    "use_global_grid = True\n",
    "metric          = \"MAG\"\n",
    "# query_ids = [\n",
    "#     \"005501\",\"032691\",\"016513\",\n",
    "#     \"002445\",\"027043\",\"019617\",\"011118\",\n",
    "#     \"002524\",\"015172\",\"036618\",\"017953\",\n",
    "# ]\n",
    "\n",
    "query_ids = [\"038176\",\"017044\",\"024526\",\"032983\", \"015883\", \"036124\", \"036298\", \"013798\", \"038419\", \"003398\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Load model & global grid ────────────────────────────────────────\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model = RIRRetrievalMLP(**ckpt[\"model_config\"]).to(device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "if use_global_grid:\n",
    "    grid_vec = torch.load(grid_vec_p).to(device)\n",
    "\n",
    "# ─── Load full evaluation & reference sets ───────────────────────────\n",
    "q_ds = FurnishedRoomSTFTDataset(root, split=\"test\",\n",
    "                                return_wav=True, mode=\"reference\")\n",
    "g_ds = FurnishedRoomSTFTDataset(root, split=\"reference\",\n",
    "                                return_wav=True, mode=\"reference\")\n",
    "\n",
    "# build quick-access lists/dicts\n",
    "q_items = [q_ds[i] for i in range(len(q_ds)) if q_ds[i]['id'] in query_ids]\n",
    "q_id2item = {s['id']: s for s in q_items}\n",
    "\n",
    "g_items = [g_ds[i] for i in range(len(g_ds))]\n",
    "g_ids    = [s['id'] for s in g_items]\n",
    "\n",
    "# ─── Precompute gallery embeddings & audio features ─────────────────\n",
    "# Embeddings\n",
    "Gg    = grid_vec.unsqueeze(0).expand(len(g_items), -1).to(device)\n",
    "mic_g = torch.stack([s['mic_pose']    for s in g_items]).to(device)\n",
    "src_g = torch.stack([s['source_pose'] for s in g_items]).to(device)\n",
    "rot_g = torch.stack([s['rot']         for s in g_items]).to(device)\n",
    "with torch.no_grad():\n",
    "    Zg = model(Gg, mic_g, src_g, rot_g)\n",
    "\n",
    "# Audio features\n",
    "flat_g = torch.stack([s['stft'] for s in g_items]).to(device)\n",
    "wav_g  = torch.stack([s['wav']             for s in g_items]).to(device)\n",
    "\n",
    "# ─── Loop over queries & plot ────────────────────────────────────────\n",
    "for qid in query_ids:\n",
    "    q = q_id2item[qid]\n",
    "\n",
    "    # — embeddings prediction —\n",
    "    Gq    = grid_vec.unsqueeze(0).to(device)\n",
    "    mic_q = q['mic_pose'].unsqueeze(0).to(device)\n",
    "    src_q = q['source_pose'].unsqueeze(0).to(device)\n",
    "    rot_q = q['rot'].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        Zq = model(Gq, mic_q, src_q, rot_q)\n",
    "\n",
    "    S = (Zq @ Zg.T).squeeze(0)\n",
    "    pred_top3 = S.argsort(descending=True)[:3]\n",
    "\n",
    "    # — ground-truth nearest via audio metric —\n",
    "    flat_q = q['stft'].unsqueeze(0).to(device)\n",
    "    wav_q  = q['wav'].unsqueeze(0).to(device)\n",
    "    all_flat = torch.cat([flat_q, flat_g], dim=0)\n",
    "    all_wav  = torch.cat([wav_q,  wav_g],  dim=0)\n",
    "    D_full = compute_audio_distance(all_flat, all_wav, metric=metric)\n",
    "    D_qg   = D_full[0, 1:]              # distances query→each gallery\n",
    "    gt_top3 = D_qg.argsort()[:3]\n",
    "\n",
    "    # — Plot setup —\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 6))\n",
    "    fig.suptitle(f\"Query ID: {qid}\", fontsize=16)\n",
    "\n",
    "    # Row 1: Query + GT-1/2/3\n",
    "    axes[0,0].plot(q['wav'].cpu())\n",
    "    axes[0,0].set_title(\"Query\")\n",
    "    # axes[0,0].axis('off')\n",
    "\n",
    "    for i, gi in enumerate(gt_top3.tolist()):\n",
    "        wav = g_items[gi]['wav'].cpu()\n",
    "        gid = g_ids[gi]\n",
    "        # compute cross-distances correctly on a 2×2 block:\n",
    "        flat_pair = torch.cat([flat_q, flat_g[gi:gi+1]], dim=0)\n",
    "        wav_pair  = torch.cat([wav_q,  wav_g[gi:gi+1]],  dim=0)\n",
    "        D2_MAG = compute_audio_distance(flat_pair, wav_pair, metric=\"MAG\")\n",
    "        D2_SC  = compute_audio_distance(flat_pair, wav_pair, metric=\"SPL\")\n",
    "        D2_LSD = compute_audio_distance(flat_pair, wav_pair, metric=\"MSE\")\n",
    "        mag = D2_MAG[0,1].item()\n",
    "        sc  = D2_SC[0,1].item()\n",
    "        lsd = D2_LSD[0,1].item()\n",
    "\n",
    "        axes[0, i+1].plot(wav)\n",
    "        axes[0, i+1].set_title(f\"GT#{i+1}: {gid}\\nMAG={mag:.2f} SPL={sc:.4f} MSE={lsd:.2e}\")\n",
    "        # axes[0, i+1].axis('off')\n",
    "\n",
    "    # Row 2: Query + Pred-1/2/3\n",
    "    axes[1,0].plot(q['wav'].cpu())\n",
    "    axes[1,0].set_title(\"Query\")\n",
    "    # axes[1,0].axis('off')\n",
    "\n",
    "    for i, gi in enumerate(pred_top3.tolist()):\n",
    "        wav   = g_items[gi]['wav'].cpu()\n",
    "        gid   = g_ids[gi]\n",
    "        gt_rank = (D_qg.argsort() == gi).nonzero(as_tuple=True)[0].item() + 1\n",
    "\n",
    "        flat_pair = torch.cat([flat_q, flat_g[gi:gi+1]], dim=0)\n",
    "        wav_pair  = torch.cat([wav_q,  wav_g[gi:gi+1]],  dim=0)\n",
    "        D2_MAG = compute_audio_distance(flat_pair, wav_pair, metric=\"MAG\")\n",
    "        D2_SC  = compute_audio_distance(flat_pair, wav_pair, metric=\"SPL\")\n",
    "        D2_LSD = compute_audio_distance(flat_pair, wav_pair, metric=\"MSE\")\n",
    "        mag = D2_MAG[0,1].item()\n",
    "        sc  = D2_SC[0,1].item()\n",
    "        lsd = D2_LSD[0,1].item()\n",
    "\n",
    "        axes[1, i+1].plot(wav)\n",
    "        axes[1, i+1].set_title(\n",
    "            f\"Pred#{i+1}: {gid}\\nGT-Rank={gt_rank} MAG={mag:.2f} SPL={sc:.4f} MSE={lsd:.2e}\"\n",
    "        )\n",
    "        # axes[1, i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single‐cell evaluation notebook (fixed splits)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from retriever import (\n",
    "    FurnishedRoomSTFTDataset,\n",
    "    RIRRetrievalMLP,\n",
    "    compute_audio_distance\n",
    ")\n",
    "\n",
    "# ------------- Config -------------\n",
    "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "root         = '../data/RAF/FurnishedRoom'\n",
    "model_path   = './outputs/20250729_021957/rir_retrieval_model.pth'\n",
    "feature_path = './features.pt'\n",
    "\n",
    "sort_metric  = 'MAG'          # GT‑distance metric\n",
    "sim_metric   = 'cosine'       # model similarity\n",
    "TOP_K        = 3\n",
    "QUERY_IDS    = [\n",
    "    \"005501\",\"032691\",\"016513\",\n",
    "    \"002445\",\"027043\",\"019617\",\"011118\",\n",
    "    \"002524\",\n",
    "    \"015172\",\n",
    "    \"036618\",\n",
    "    \"017953\",\n",
    "]\n",
    "\n",
    "# ------------- Load model -------------\n",
    "model = RIRRetrievalMLP().to(device).eval()\n",
    "ckpt  = torch.load(model_path, map_location=device)\n",
    "state = model.state_dict()\n",
    "state.update({k: v for k, v in ckpt.items() if k in state and v.shape == state[k].shape})\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# ------------- Prepare datasets -------------\n",
    "#  * Gallery = references split, loaded from data‑split‑references.json\n",
    "ds_ref    = FurnishedRoomSTFTDataset(root, split='reference', return_wav=True, mode='reference')\n",
    "loader_ref= DataLoader(ds_ref, batch_size=64, pin_memory=True)\n",
    "\n",
    "#  * Queries = validation split from the *same* JSON\n",
    "ds_q      = FurnishedRoomSTFTDataset(root, split='validation', return_wav=True, mode='reference')\n",
    "\n",
    "# ------------- Precompute reference features -------------\n",
    "ref_ids, ref_flats, ref_wavs, ref_embs = [], [], [], []\n",
    "eats = torch.load(feature_path).to(device)   # global grid feature\n",
    "\n",
    "def pair_metrics(flat_q, wav_q, ref_flat, ref_wav):\n",
    "    # flat_q: [1, M], wav_q: [T]\n",
    "    pair_flat = torch.cat([flat_q, ref_flat.unsqueeze(0)], dim=0)\n",
    "    pair_wav  = torch.cat([wav_q.unsqueeze(0), ref_wav.unsqueeze(0)], dim=0)\n",
    "    mag = compute_audio_distance(pair_flat, pair_wav, metric='MAG2')[0,1].item()\n",
    "    sc  = compute_audio_distance(pair_flat, pair_wav, metric='SPL' )[0,1].item()\n",
    "    lsd = compute_audio_distance(pair_flat, pair_wav, metric='EDC')[0,1].item()\n",
    "    return mag, sc, lsd\n",
    "\n",
    "for batch in loader_ref:\n",
    "    B = len(batch['id'])\n",
    "    ref_ids.extend(batch['id'])\n",
    "\n",
    "    stft = batch['stft'].to(device)           # [B, F, T]\n",
    "    wav  = batch['wav'].to(device)            # [B, T]\n",
    "    flat = stft               # [B, M]\n",
    "    G    = eats.unsqueeze(0).expand(B, -1)    # [B, E]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model(\n",
    "            G,\n",
    "            batch['mic_pose'].to(device),\n",
    "            batch['source_pose'].to(device),\n",
    "            batch['rot'].to(device)\n",
    "        )\n",
    "\n",
    "    ref_flats.append(flat.cpu())\n",
    "    ref_wavs.append(wav.cpu())\n",
    "    ref_embs.append(emb.cpu())\n",
    "\n",
    "ref_flats = torch.cat(ref_flats)              # [N_ref, M]\n",
    "ref_wavs  = torch.cat(ref_wavs)               # [N_ref, T]\n",
    "ref_embs  = torch.cat(ref_embs)               # [N_ref, D]\n",
    "\n",
    "# ------------- Diversity of references (all metrics) -------------\n",
    "for metric in ['MAG', 'LSD', 'SC']:\n",
    "    D = compute_audio_distance(\n",
    "        ref_flats.to(device),\n",
    "        ref_wavs.to(device),\n",
    "        metric=metric\n",
    "    ).cpu().numpy()                           # [N_ref, N_ref]\n",
    "\n",
    "    N = D.shape[0]\n",
    "    mask = ~np.eye(N, dtype=bool)\n",
    "    vals = D[mask]\n",
    "\n",
    "    mean_d   = vals.mean()\n",
    "    std_d    = vals.std()\n",
    "    median_d = np.median(vals)\n",
    "    uniq, cnt= np.unique(np.round(vals, 3), return_counts=True)\n",
    "    mode_d   = uniq[cnt.argmax()]\n",
    "\n",
    "    print(f\"[{metric}] mean={mean_d:.3f}, median={median_d:.3f}, \"\n",
    "          f\"mode≈{mode_d:.3f}, std={std_d:.3f}\")\n",
    "    \n",
    "    \n",
    "# [MAG] mean=2.404, median=1.884, mode≈1.146, std=1.967\n",
    "# [LSD] mean=0.724, median=0.683, mode≈0.601, std=0.216\n",
    "# [SC] mean=1.489, median=0.926, mode≈0.915, std=1.629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8834832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from retriever import (\n",
    "    FurnishedRoomSTFTDataset,\n",
    "    compute_audio_distance,\n",
    "    RIRRetrievalMLP,\n",
    ")\n",
    "\n",
    "# ─── configs ─────────────────────────────────────────────────────\n",
    "root         = '../data/RAF/FurnishedRoom'\n",
    "ckpt_path   = './outputs/20250806_194739/rir_retrieval_model.ckpt'\n",
    "feats_map_p = \"./features.pt\"   # dict: sample_id → Tensor[grid_feat_dim]\n",
    "grid_vec_p  = \"/path/to/grid_vec.pt\"    # Tensor[grid_feat_dim]\n",
    "use_global_grid = True                 # set True if you trained with a single global grid\n",
    "metric      = \"SPL\"                     # ground‐truth audio metric\n",
    "Ks          = [1, 2, 3]                 # for map@K and top‐K acc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model checkpoint and global grid\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "grid_vec = torch.load(\"./features.pt\").to(device)  # Tensor[1024], used for all samples\n",
    "\n",
    "# Reconstruct model from checkpoint config\n",
    "model = RIRRetrievalMLP(**ckpt[\"model_config\"]).to(device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# ─── prepare data ─────────────────────────────────────────────────\n",
    "q_ds = FurnishedRoomSTFTDataset(root, split=\"validation\",\n",
    "                                return_wav=True, mode=\"reference\")\n",
    "g_ds = FurnishedRoomSTFTDataset(root, split=\"reference\",\n",
    "                                return_wav=True, mode=\"reference\")\n",
    "q_loader = DataLoader(q_ds, batch_size=len(q_ds), shuffle=False)\n",
    "g_loader = DataLoader(g_ds, batch_size=len(g_ds), shuffle=False)\n",
    "batch_q = next(iter(q_loader))\n",
    "batch_g = next(iter(g_loader))\n",
    "\n",
    "# ─── build grid inputs ────────────────────────────────────────────\n",
    "Gq = grid_vec.unsqueeze(0).expand(len(q_ds), -1).to(device)\n",
    "Gg = grid_vec.unsqueeze(0).expand(len(g_ds), -1).to(device)\n",
    "\n",
    "# ─── forward to get embeddings ────────────────────────────────────\n",
    "with torch.no_grad():\n",
    "    Zq = model(\n",
    "        Gq,\n",
    "        batch_q[\"mic_pose\"].to(device),\n",
    "        batch_q[\"source_pose\"].to(device),\n",
    "        batch_q[\"rot\"].to(device),\n",
    "    )\n",
    "    Zg = model(\n",
    "        Gg,\n",
    "        batch_g[\"mic_pose\"].to(device),\n",
    "        batch_g[\"source_pose\"].to(device),\n",
    "        batch_g[\"rot\"].to(device),\n",
    "    )\n",
    "\n",
    "# ─── prepare audio‐distance for ground truth ───────────────────────\n",
    "flat_q, flat_g = (\n",
    "    batch_q[\"stft\"].to(device),\n",
    "    batch_g[\"stft\"].to(device),\n",
    ")\n",
    "wav_q, wav_g = batch_q[\"wav\"].to(device), batch_g[\"wav\"].to(device)\n",
    "all_flats = torch.cat([flat_q, flat_g], dim=0)\n",
    "all_wavs  = torch.cat([wav_q, wav_g], dim=0)\n",
    "D_full    = compute_audio_distance(all_flats, all_wavs, metric=metric)\n",
    "D_qg      = D_full[: len(q_ds), len(q_ds) :]\n",
    "\n",
    "# ground‐truth nearest\n",
    "gt_idxs   = D_qg.argmin(dim=1)          # for each query, gallery‐index of true NN\n",
    "gt_dists  = D_qg[torch.arange(len(q_ds)), gt_idxs]\n",
    "\n",
    "# ─── prediction via cosine sim ────────────────────────────────────\n",
    "S_pred    = Zq @ Zg.t()\n",
    "pred_ranks= S_pred.argsort(dim=1, descending=True)  # [Q, G]\n",
    "\n",
    "# metrics\n",
    "ranks = torch.tensor([\n",
    "    (pred_ranks[i] == gt_idxs[i]).nonzero().item()\n",
    "    for i in range(len(q_ds))\n",
    "], dtype=torch.int)\n",
    "\n",
    "map_at_k = {\n",
    "    k: (ranks < k).float().mean().item()\n",
    "    for k in Ks\n",
    "}\n",
    "mean_rank = (ranks.float() + 1).mean().item()  # 1‐indexed\n",
    "topk_acc = {\n",
    "    k: (ranks < k).float().mean().item()  # same as map@k here\n",
    "    for k in Ks\n",
    "}\n",
    "\n",
    "# mean audio‐metric diff between GT and pred@1\n",
    "pred1_idxs = pred_ranks[:, 0]\n",
    "pred1_dists= D_qg[torch.arange(len(q_ds)), pred1_idxs]\n",
    "mean_diff  = (pred1_dists - gt_dists).abs().mean().item()\n",
    "\n",
    "# ─── report ────────────────────────────────────────────────────────\n",
    "print(f\"map@1: {map_at_k[1]:.3f}, map@2: {map_at_k[2]:.3f}, map@3: {map_at_k[3]:.3f}\")\n",
    "print(f\"mean_rank: {mean_rank:.3f}\")\n",
    "print(\"top‑K accuracies:\", {k: f\"{topk_acc[k]:.3f}\" for k in Ks})\n",
    "print(f\"mean |Δ{metric}| (pred@1 vs GT): {mean_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a23a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from retriever import FurnishedRoomSTFTDataset, RIRRetrievalMLP\n",
    "import json\n",
    "\n",
    "# ─── configs ─────────────────────────────────────────────────────\n",
    "root            = '../data/RAF/FurnishedRoom'\n",
    "ckpt_path       = './outputs/20250806_194739/rir_retrieval_model.ckpt'\n",
    "# if you trained with a global grid vector, point this to your .pt file\n",
    "grid_vec_p      = \"./features.pt\"\n",
    "use_global_grid = True\n",
    "topk            = 10  # retrieve top-5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── load model and global grid ──────────────────────────────────\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model = RIRRetrievalMLP(**ckpt[\"model_config\"]).to(device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "if use_global_grid:\n",
    "    grid_vec = torch.load(grid_vec_p).to(device)    # Tensor[grid_feat_dim]\n",
    "\n",
    "# ─── define splits to process ────────────────────────────────────\n",
    "# evaluation corresponds to your \"validation\" split in the original code\n",
    "splits = {\n",
    "    \"train\":      \"train\",\n",
    "    \"evaluation\": \"validation\",\n",
    "    \"test\":       \"test\",\n",
    "}\n",
    "\n",
    "references = {}\n",
    "\n",
    "for split_name, split_id in splits.items():\n",
    "    # prepare query & gallery datasets\n",
    "    q_ds = FurnishedRoomSTFTDataset(root, split=split_id,\n",
    "                                    return_wav=False, mode=\"reference\")\n",
    "    g_ds = FurnishedRoomSTFTDataset(root, split=\"reference\",\n",
    "                                    return_wav=False, mode=\"reference\")\n",
    "\n",
    "    # load entire sets in one batch\n",
    "    q_batch = next(iter(DataLoader(q_ds, batch_size=len(q_ds), shuffle=False)))\n",
    "    g_batch = next(iter(DataLoader(g_ds, batch_size=len(g_ds), shuffle=False)))\n",
    "\n",
    "    # build grid inputs\n",
    "    if use_global_grid:\n",
    "        Gq = grid_vec.unsqueeze(0).expand(len(q_ds), -1)\n",
    "        Gg = grid_vec.unsqueeze(0).expand(len(g_ds), -1)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Per-sample grid not implemented\")\n",
    "\n",
    "    # forward pass to get embeddings\n",
    "    with torch.no_grad():\n",
    "        Zq = model(\n",
    "            Gq.to(device),\n",
    "            q_batch[\"mic_pose\"].to(device),\n",
    "            q_batch[\"source_pose\"].to(device),\n",
    "            q_batch[\"rot\"].to(device),\n",
    "        )\n",
    "        Zg = model(\n",
    "            Gg.to(device),\n",
    "            g_batch[\"mic_pose\"].to(device),\n",
    "            g_batch[\"source_pose\"].to(device),\n",
    "            g_batch[\"rot\"].to(device),\n",
    "        )\n",
    "\n",
    "    # compute cosine‐similarity and get top-k indices\n",
    "    S = Zq @ Zg.t()  # [Q, G]\n",
    "    topk_idxs = S.argsort(dim=1, descending=True)[:, :topk]  # [Q, topk]\n",
    "\n",
    "    # map each query ID to its top-k gallery IDs\n",
    "    refs_for_split = {}\n",
    "    for i, qid in enumerate(q_ds.ids):\n",
    "        retrieved = [g_ds.ids[j] for j in topk_idxs[i].tolist()]\n",
    "        refs_for_split[qid] = retrieved\n",
    "\n",
    "    references[split_name] = refs_for_split\n",
    "\n",
    "# ─── save to JSON ─────────────────────────────────────────────────\n",
    "with open(\"references.json\", \"w\") as f:\n",
    "    json.dump(references, f, indent=2)\n",
    "\n",
    "print(\"Wrote references.json with top-5 retrievals for each split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from retriever import FurnishedRoomSTFTDataset, compute_audio_distance\n",
    "import json\n",
    "\n",
    "# ─── configs ─────────────────────────────────────────────────────\n",
    "root               = '../data/RAF/FurnishedRoom'\n",
    "metric             = 'SPL'       # your ground-truth audio‐distance\n",
    "topk               = 5          # retrieve top-10 per query\n",
    "query_batch_size   = 2048         # tune so (batch_size + G)^2 fits GPU\n",
    "device             = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "splits = {\n",
    "    \"train\":      \"train\",        # 40 000 queries\n",
    "    \"evaluation\": \"validation\",   #  3 000 queries\n",
    "    \"test\":       \"test\",         #  3 000 queries\n",
    "}\n",
    "\n",
    "# ─── preload gallery ONCE ────────────────────────────────────────\n",
    "g_ds     = FurnishedRoomSTFTDataset(root, split=\"reference\",\n",
    "                                    return_wav=True, mode=\"reference\")\n",
    "g_loader = DataLoader(g_ds, batch_size=len(g_ds), shuffle=False)\n",
    "batch_g  = next(iter(g_loader))\n",
    "G        = len(g_ds)\n",
    "\n",
    "# flatten + move gallery to GPU\n",
    "flat_g = batch_g[\"stft\"].view(G, -1).to(device)  # [G, D]\n",
    "wav_g  = batch_g[\"wav\"].to(device)               # [G, L]\n",
    "\n",
    "references = {}\n",
    "\n",
    "for split_name, split_id in splits.items():\n",
    "    # ─── prepare query loader ────────────────────────────────────\n",
    "    q_ds     = FurnishedRoomSTFTDataset(root, split=split_id,\n",
    "                                        return_wav=True, mode=\"reference\")\n",
    "    Q        = len(q_ds)\n",
    "    q_loader = DataLoader(q_ds, batch_size=query_batch_size,\n",
    "                          shuffle=False, drop_last=False)\n",
    "\n",
    "    refs_for_split = {}\n",
    "\n",
    "    # ─── process each query batch on-GPU ────────────────────────\n",
    "    for batch_idx, batch_q in enumerate(tqdm(q_loader, \n",
    "                                             desc=f\"{split_name} batches\",\n",
    "                                             total=(Q + query_batch_size - 1) // query_batch_size)):\n",
    "        bsize = batch_q[\"stft\"].size(0)\n",
    "        # compute which slice of q_ds.ids this batch corresponds to\n",
    "        start = batch_idx * query_batch_size\n",
    "        end   = start + bsize\n",
    "        batch_ids = q_ds.ids[start:end]\n",
    "\n",
    "        # move this batch to GPU\n",
    "        flat_q = batch_q[\"stft\"].view(bsize, -1).to(device)  # [bsize, D]\n",
    "        wav_q  = batch_q[\"wav\"].to(device)                   # [bsize, L]\n",
    "\n",
    "        # compute the (bsize + G)² distance matrix on GPU\n",
    "        with torch.no_grad():\n",
    "            all_flats = torch.cat([flat_q, flat_g], dim=0)  # [bsize+G, D]\n",
    "            all_wavs  = torch.cat([wav_q,  wav_g],  dim=0)  # [bsize+G, L]\n",
    "            D_full    = compute_audio_distance(all_flats, all_wavs, metric=metric)\n",
    "            # slice out only the [batch × G] block, move to CPU\n",
    "            D_qg = D_full[:bsize, bsize:].cpu()             # [bsize, G]\n",
    "\n",
    "        # pick top-k smallest distances per query\n",
    "        topk_idxs = D_qg.argsort(dim=1)[:, :topk]           # [bsize, topk]\n",
    "\n",
    "        # map each query ID → list of top-k gallery IDs\n",
    "        for i, qid in enumerate(batch_ids):\n",
    "            refs_for_split[qid] = [g_ds.ids[j] for j in topk_idxs[i].tolist()]\n",
    "\n",
    "        # free GPU memory before next batch\n",
    "        del flat_q, wav_q, all_flats, all_wavs, D_full, D_qg\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    references[split_name] = refs_for_split\n",
    "\n",
    "# ─── save to JSON ─────────────────────────────────────────────────\n",
    "with open(\"gt_references.json\", \"w\") as f:\n",
    "    json.dump(references, f, indent=2)\n",
    "\n",
    "print(f\"Wrote gt_references.json with top-{topk} ground-truth retrievals (metric={metric})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1: Build data, compute metrics, cache them (NO PLOTS) ---\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from retriever import compute_audio_distance, FurnishedRoomSTFTDataset\n",
    "from torchaudio.transforms import GriffinLim\n",
    "import sys\n",
    "sys.path.append('../NeRAF')\n",
    "from NeRAF_helper import compute_t60, evaluate_edt, evaluate_clarity\n",
    "\n",
    "# ---------------- Params ----------------\n",
    "eval_pattern = \"../eval_results/furnishedroom_2/renders/eval_*.npy\"\n",
    "max_files = 200\n",
    "root_dir = \"../data/RAF/FurnishedRoom\"\n",
    "refs_file = \"./references.json\"\n",
    "sample_rate = 48000\n",
    "CACHE_PATH = \"./records_cache_furnishedroom_2.pkl\"\n",
    "\n",
    "# Griffin-Lim ISTFT setup\n",
    "n_fft = (513 - 1) * 2\n",
    "win_length = 512\n",
    "hop_length = 256\n",
    "power = 1\n",
    "istft = GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=power)\n",
    "\n",
    "# ─── Helpers ──────────────────────────────────────────────────────────────────\n",
    "def _as_2d_numpy(wav):\n",
    "    if isinstance(wav, torch.Tensor):\n",
    "        arr = wav.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.asarray(wav)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[None, :]\n",
    "    return arr\n",
    "\n",
    "def room_metric_diffs(wav_gt, wav_x, fs):\n",
    "    gt = _as_2d_numpy(wav_gt)\n",
    "    xx = _as_2d_numpy(wav_x)\n",
    "    L = min(gt.shape[1], xx.shape[1])\n",
    "    gt = gt[:, :L]\n",
    "    xx = xx[:, :L]\n",
    "\n",
    "    # T60: mean % error, invalids -> 100%\n",
    "    t60_gt, t60_x = compute_t60(gt, xx, fs=fs, advanced=True)\n",
    "    t60_gt = np.atleast_1d(t60_gt).astype(float)\n",
    "    t60_x  = np.atleast_1d(t60_x).astype(float)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        t60_diff = np.abs(t60_x - t60_gt) / (np.abs(t60_gt) + 1e-12)\n",
    "    invalid_mask = (t60_gt < -0.5) | (t60_x < -0.5)\n",
    "    t60_diff[invalid_mask] = 1.0\n",
    "    t60_err_pct = float(np.mean(t60_diff) * 100.0)\n",
    "\n",
    "    # EDT & C50: mean absolute differences\n",
    "    edt_gt, edt_x = evaluate_edt(xx, gt, fs=fs)\n",
    "    edt_mae = float(np.mean(np.abs(edt_x - edt_gt)))\n",
    "\n",
    "    c50_gt, c50_x = evaluate_clarity(xx, gt, fs=fs)\n",
    "    c50_mae = float(np.mean(np.abs(c50_x - c50_gt)))\n",
    "\n",
    "    return {'EDT': edt_mae, 'C50': c50_mae, 'T60': t60_err_pct}\n",
    "\n",
    "# ─── Data Gathering ───────────────────────────────────────────────────────────\n",
    "with open(refs_file, 'r') as f:\n",
    "    references = json.load(f)[\"test\"]\n",
    "\n",
    "file_paths = sorted(\n",
    "    glob.glob(eval_pattern),\n",
    "    key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0])\n",
    ")\n",
    "if max_files is not None:\n",
    "    file_paths = file_paths[:max_files]\n",
    "\n",
    "# Use reference mode for both splits as you set earlier\n",
    "ds_test = FurnishedRoomSTFTDataset(\n",
    "    root_dir=root_dir, split=\"test\", sample_rate=sample_rate, return_wav=True, mode=\"reference\"\n",
    ")\n",
    "ds_ref = FurnishedRoomSTFTDataset(\n",
    "    root_dir=root_dir, split=\"reference\", sample_rate=sample_rate, return_wav=True, mode=\"reference\"\n",
    ")\n",
    "\n",
    "records = []\n",
    "for fp in file_paths:\n",
    "    data = np.load(fp, allow_pickle=True).item()\n",
    "    idx = int(data[\"audio_idx\"])\n",
    "\n",
    "    test_item = ds_test[idx]\n",
    "    wav_gt = test_item['wav'].squeeze()\n",
    "    stft_gt = test_item['stft'].squeeze(0)               # log-mag [F,T]\n",
    "\n",
    "    stft_pred = torch.from_numpy(data[\"pred_stft\"]).float().squeeze(0)  # log-mag\n",
    "    mag_pred = torch.exp(stft_pred) - 1e-3               # linear mag\n",
    "    wav_pred = istft(mag_pred.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    # align lengths for waveform-based metrics\n",
    "    L = min(wav_gt.shape[0], wav_pred.shape[0])\n",
    "    wav_gt, wav_pred = wav_gt[:L], wav_pred[:L]\n",
    "\n",
    "    test_id = test_item['id']\n",
    "    top3_ids = references[test_id][:3]\n",
    "\n",
    "    ref_samples = []\n",
    "    for rid in top3_ids:\n",
    "        r_idx = ds_ref.id2idx[rid]\n",
    "        ref_item = ds_ref[r_idx]\n",
    "        wav_r = ref_item['wav'].squeeze()\n",
    "        stft_r = ref_item['stft'].squeeze(0)            # log-mag\n",
    "        Lr = min(wav_gt.shape[0], wav_r.shape[0])\n",
    "        wav_r = wav_r[:Lr]\n",
    "        ref_samples.append({'id': rid, 'wav': wav_r, 'stft': stft_r})\n",
    "\n",
    "    records.append({\n",
    "        'idx': idx, 'id': test_id,\n",
    "        'wav_gt': wav_gt, 'stft_gt': stft_gt,\n",
    "        'wav_pred': wav_pred, 'stft_pred': stft_pred,\n",
    "        'refs': ref_samples\n",
    "    })\n",
    "\n",
    "# ─── Metric Calculation ───────────────────────────────────────────────────────\n",
    "for rec in records:\n",
    "    # prediction vs GT\n",
    "    pair_stft = torch.stack([rec['stft_gt'], rec['stft_pred']], dim=0)  # log-mag\n",
    "    pair_wav  = torch.stack([rec['wav_gt'],  rec['wav_pred']], dim=0)\n",
    "\n",
    "    rec['metrics'] = {\n",
    "        'MSE' : compute_audio_distance(pair_stft, wavs=pair_wav, metric='MSE')[0,1].item(),\n",
    "        'SPL' : compute_audio_distance(pair_stft, wavs=pair_wav, metric='SPL', fs=sample_rate)[0,1].item(),\n",
    "        'MAG' : compute_audio_distance(pair_stft, metric='MAG')[0,1].item(),\n",
    "        'MAG2': compute_audio_distance(pair_stft, metric='MAG2')[0,1].item(),\n",
    "    }\n",
    "    rec['metrics'].update(room_metric_diffs(rec['wav_gt'], rec['wav_pred'], fs=sample_rate))\n",
    "\n",
    "    # top-3 reference metrics (each vs GT)\n",
    "    ref_metrics = []\n",
    "    for ref in rec['refs']:\n",
    "        pair_r_stft = torch.stack([rec['stft_gt'], ref['stft']], dim=0)  # log-mag\n",
    "        L = min(rec['wav_gt'].shape[0], ref['wav'].shape[0])\n",
    "        pair_r_wav = torch.stack([rec['wav_gt'][:L], ref['wav'][:L]], dim=0)\n",
    "\n",
    "        m = {\n",
    "            'id'  : ref['id'],\n",
    "            'MSE' : compute_audio_distance(pair_r_stft, wavs=pair_r_wav, metric='MSE')[0,1].item(),\n",
    "            'SPL' : compute_audio_distance(pair_r_stft, wavs=pair_r_wav, metric='SPL', fs=sample_rate)[0,1].item(),\n",
    "            'MAG' : compute_audio_distance(pair_r_stft, metric='MAG')[0,1].item(),\n",
    "            'MAG2': compute_audio_distance(pair_r_stft, metric='MAG2')[0,1].item(),\n",
    "        }\n",
    "        m.update(room_metric_diffs(rec['wav_gt'], ref['wav'], fs=sample_rate))\n",
    "        ref_metrics.append(m)\n",
    "    rec['ref_metrics'] = ref_metrics\n",
    "\n",
    "# ─── Save cache ──────────────────────────────────────────────────────────────\n",
    "with open(CACHE_PATH, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"records\": records,\n",
    "        \"params\": {\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"n_fft\": n_fft, \"win_length\": win_length, \"hop_length\": hop_length, \"power\": power\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "print(f\"Cached {len(records)} records to {CACHE_PATH}. \"\n",
    "      f\"Available metrics per sample: {list(records[0]['metrics'].keys()) if records else '[]'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af696ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: Choose metric & N, then plot from cache (FAST) ---\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CACHE_PATH = \"./records_cache_furnishedroom_2.pkl\"\n",
    "\n",
    "# --- User-tunable: choose sort metric and number of worst to show ---\n",
    "SORT_METRIC = \"MSE\"   # one of: \"SPL\", \"MSE\", \"MAG\", \"MAG2\", \"T60\", \"C50\", \"EDT\"\n",
    "TOP_N       = 5      # how many worst to plot (after sorting by SORT_METRIC, desc)\n",
    "\n",
    "# --- Small helper for titles ---\n",
    "def three_line_block(id_str, m):\n",
    "    \"\"\"\n",
    "    Format:\n",
    "      1) ID\n",
    "      2) SPL, MSE, MAG, MAG2\n",
    "      3) T60%, C50Δ, EDTΔ\n",
    "    \"\"\"\n",
    "    line1 = f\"{id_str}\"\n",
    "    line2 = (f\"SPL: {m['SPL']:.4f}   \"\n",
    "             f\"MSE: {m['MSE']:.2e}   \"\n",
    "             f\"MAG: {m['MAG']:.2f}   \"\n",
    "             f\"MAG2: {m['MAG2']:.2e}\")\n",
    "    line3 = (f\"T60%: {m['T60']:.1f}%   \"\n",
    "             f\"C50Δ: {m['C50']:.2f} dB   \"\n",
    "             f\"EDTΔ: {m['EDT']:.3f} s\")\n",
    "    return f\"{line1}\\n{line2}\\n{line3}\"\n",
    "\n",
    "# --- Load cache ---\n",
    "with open(CACHE_PATH, \"rb\") as f:\n",
    "    cache = pickle.load(f)\n",
    "records = cache[\"records\"]\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError(\"No records in cache. Run Cell 1 first.\")\n",
    "\n",
    "if SORT_METRIC not in records[0][\"metrics\"]:\n",
    "    raise ValueError(f\"SORT_METRIC '{SORT_METRIC}' not in metrics. \"\n",
    "                     f\"Choose from {list(records[0]['metrics'].keys())}\")\n",
    "\n",
    "# ─── Sort & Select Worst by chosen metric ─────────────────────────────────────\n",
    "records_sorted = sorted(records, key=lambda r: r['metrics'][SORT_METRIC], reverse=True)\n",
    "worst = records_sorted[:min(TOP_N, len(records_sorted))]\n",
    "\n",
    "# ─── Plotting ─────────────────────────────────────────────────────────────────\n",
    "for rec in worst:\n",
    "    # Time-domain canvas: 2 rows (GT/Pred, then 3 refs)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    # top row: GT and Pred, keep titles clean\n",
    "    axes[0,0].plot(rec['wav_gt'].numpy())\n",
    "    axes[0,0].set_title('Ground Truth', fontsize=11, pad=8)\n",
    "    axes[0,0].set_xlabel('Sample'); axes[0,0].set_ylabel('Amplitude')\n",
    "\n",
    "    axes[0,1].plot(rec['wav_pred'].numpy())\n",
    "    axes[0,1].set_title('Prediction', fontsize=11, pad=8)\n",
    "    axes[0,1].set_xlabel('Sample'); axes[0,1].set_ylabel('Amplitude')\n",
    "\n",
    "    axes[0,2].axis('off')  # spacer\n",
    "\n",
    "    # bottom row: top-3 references with their own 3-line metric blocks\n",
    "    for i, rm in enumerate(rec['ref_metrics'][:3]):\n",
    "        wav_r = rec['refs'][i]['wav']\n",
    "        axes[1,i].plot(wav_r.numpy())\n",
    "        axes[1,i].set_xlabel('Sample'); axes[1,i].set_ylabel('Amplitude')\n",
    "        axes[1,i].set_title(\n",
    "            three_line_block(f\"Ref {rm['id']}\", rm),\n",
    "            fontsize=10, pad=10\n",
    "        )\n",
    "\n",
    "    # Figure title (3 lines) for the prediction\n",
    "    fig.suptitle(\n",
    "        three_line_block(f\"{rec['id']} (sorted by {SORT_METRIC})\", rec['metrics']),\n",
    "        fontsize=12, y=0.90\n",
    "    )\n",
    "    fig.subplots_adjust(top=0.83, hspace=0.45, wspace=0.25)\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.88])\n",
    "    plt.show()\n",
    "\n",
    "    # STFT canvas: same layout but with images\n",
    "    fig2, axes2 = plt.subplots(2, 3, figsize=(18, 9))\n",
    "    im = axes2[0,0].imshow(rec['stft_gt'].numpy(), aspect='auto', origin='lower')\n",
    "    axes2[0,0].set_title('GT STFT', fontsize=11, pad=8)\n",
    "    axes2[0,0].set_xlabel('Time frame'); axes2[0,0].set_ylabel('Frequency bin')\n",
    "    plt.colorbar(im, ax=axes2[0,0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    im2 = axes2[0,1].imshow(rec['stft_pred'].numpy(), aspect='auto', origin='lower')\n",
    "    axes2[0,1].set_title('Pred STFT', fontsize=11, pad=8)\n",
    "    axes2[0,1].set_xlabel('Time frame'); axes2[0,1].set_ylabel('Frequency bin')\n",
    "    plt.colorbar(im2, ax=axes2[0,1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    axes2[0,2].axis('off')  # spacer\n",
    "\n",
    "    for i, rm in enumerate(rec['ref_metrics'][:3]):\n",
    "        stft_r = rec['refs'][i]['stft']\n",
    "        imr = axes2[1,i].imshow(stft_r.numpy(), aspect='auto', origin='lower')\n",
    "        axes2[1,i].set_title(\n",
    "            three_line_block(f\"Ref {rm['id']}\", rm),\n",
    "            fontsize=10, pad=10\n",
    "        )\n",
    "        axes2[1,i].set_xlabel('Time frame'); axes2[1,i].set_ylabel('Frequency bin')\n",
    "        plt.colorbar(imr, ax=axes2[1,i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    fig2.suptitle(\n",
    "        three_line_block(f\"{rec['id']} (sorted by {SORT_METRIC})\", rec['metrics']),\n",
    "        fontsize=12, y=0.98\n",
    "    )\n",
    "    fig2.subplots_adjust(top=0.82, hspace=0.50, wspace=0.28)\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.87])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from retriever import FurnishedRoomSTFTDataset\n",
    "from NeRAF_helper import measure_rt60_advance, measure_edt, measure_clarity\n",
    "\n",
    "def compute_edc_db(wav: torch.Tensor, T_target: int = 60) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute Schroeder EDC in dB, downsample to T_target frames.\n",
    "    wav: [T] float32 tensor (mono)\n",
    "    \"\"\"\n",
    "    e = wav.float()**2\n",
    "    edc = torch.flip(torch.cumsum(torch.flip(e, dims=[0]), dim=0), dims=[0])\n",
    "    edc = edc / (edc[0] + 1e-12)\n",
    "    edc_db = 10.0 * torch.log10(edc + 1e-12)\n",
    "    idx = torch.linspace(0, edc_db.numel() - 1, T_target).long()\n",
    "    return edc_db[idx]\n",
    "\n",
    "def compute_dr(wav: torch.Tensor, fs: int = 48000, direct_ms: float = 5.0) -> float:\n",
    "    \"\"\"\n",
    "    Compute Direct-to-Reverberant ratio (dB) for a single IR.\n",
    "    direct_ms: time window (ms) after direct-path arrival for 'direct' energy\n",
    "    \"\"\"\n",
    "    # Find direct-path arrival index\n",
    "    idx_direct = torch.argmax(torch.abs(wav)).item()\n",
    "    win_samples = int(direct_ms * fs / 1000.0)\n",
    "\n",
    "    start = max(idx_direct - win_samples // 2, 0)\n",
    "    end   = min(idx_direct + win_samples // 2, wav.numel())\n",
    "\n",
    "    direct_energy = torch.sum(wav[start:end] ** 2)\n",
    "    reverb_energy = torch.sum(wav[end:] ** 2)\n",
    "\n",
    "    dr_db = 10.0 * torch.log10((direct_energy + 1e-12) / (reverb_energy + 1e-12))\n",
    "    return dr_db.item()  # return as Python float\n",
    "\n",
    "# ─── Load dataset with all samples ────────────────────────────────\n",
    "root_dir = \"../data/RAF/EmptyRoom\"\n",
    "ds = FurnishedRoomSTFTDataset(\n",
    "    root_dir=root_dir,\n",
    "    split='all',\n",
    "    sample_rate=48000,\n",
    "    return_wav=True\n",
    ")\n",
    "\n",
    "# ─── Loop through dataset and compute features ────────────────────\n",
    "features_dict = {}\n",
    "for i in range(len(ds)):\n",
    "    item = ds[i]\n",
    "    sid = item['id']\n",
    "    wav = item['wav'].float()  # [T] mono\n",
    "\n",
    "    # EDC\n",
    "    edc_curve = compute_edc_db(wav, T_target=60)\n",
    "\n",
    "    # Decay features (single IR)\n",
    "    t60 = measure_rt60_advance(wav.numpy(), sr=48000)         # seconds\n",
    "    edt = measure_edt(wav.numpy(), fs=48000)                  # seconds\n",
    "    c50 = measure_clarity(wav.numpy(), time=50, fs=48000)     # dB\n",
    "    dr  = compute_dr(wav, fs=48000)                           # dB\n",
    "\n",
    "    decay_feats = torch.tensor([t60, c50, edt, dr], dtype=torch.float32)\n",
    "\n",
    "    features_dict[sid] = {\n",
    "        'edc': edc_curve,\n",
    "        'decay_feats': decay_feats\n",
    "    }\n",
    "\n",
    "# ─── Save to one file ─────────────────────────────────────────────\n",
    "save_path = os.path.join(root_dir, \"edc_decay_features.pt\")\n",
    "torch.save(features_dict, save_path)\n",
    "print(f\"Saved EDC + decay features (T60, C50, EDT, DR) for {len(features_dict)} samples to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from retriever import FurnishedRoomSTFTDataset, compute_audio_distance\n",
    "\n",
    "import sys\n",
    "sys.path.append('../NeRAF')\n",
    "# pairwise “helper diffs”\n",
    "from NeRAF_helper import compute_t60, evaluate_edt, evaluate_clarity\n",
    "# single-sample features (fallback if dataset misses precomputed)\n",
    "from NeRAF_helper import measure_rt60_advance, measure_edt, measure_clarity\n",
    "\n",
    "# ─── Config ────────────────────────────────────────────────────────\n",
    "root_dir    = \"../data/RAF/FurnishedRoom\"\n",
    "sample_rate = 48000\n",
    "i, j        = 0, 1   # change as you like\n",
    "EDC_T       = 60     # frames to downsample EDC to (must match across the pair)\n",
    "\n",
    "# ─── Tiny helpers ──────────────────────────────────────────────────\n",
    "def compute_edc_db(wav_1d: torch.Tensor, T_target: int = 60) -> torch.Tensor:\n",
    "    e = wav_1d.float()**2\n",
    "    edc = torch.flip(torch.cumsum(torch.flip(e, dims=[0]), dim=0), dims=[0])\n",
    "    edc = edc / (edc[0] + 1e-12)\n",
    "    edc_db = 10.0 * torch.log10(edc + 1e-12)\n",
    "    idx = torch.linspace(0, edc_db.numel() - 1, T_target).long()\n",
    "    return edc_db[idx]\n",
    "\n",
    "def get_or_compute_features(item, fs, edc_T=60):\n",
    "    \"\"\"Return (wav, stft, edc[T], decay_feats[3]) for a dataset item, computing if missing.\"\"\"\n",
    "    wav  = item['wav'].squeeze()\n",
    "    stft = item['stft'].squeeze(0)  # log-mag\n",
    "    # EDC\n",
    "    edc  = item.get('edc', None)\n",
    "    if edc is None:\n",
    "        edc = compute_edc_db(wav, T_target=edc_T)\n",
    "    # Decay feats: [T60, C50, EDT]\n",
    "    decay = item.get('decay_feats', None)\n",
    "    if decay is None:\n",
    "        # single-sample measurements\n",
    "        t60 = float(measure_rt60_advance(wav.detach().cpu().numpy(), sr=fs))\n",
    "        edt = float(measure_edt(wav.detach().cpu().numpy(), fs=fs))\n",
    "        c50 = float(measure_clarity(wav.detach().cpu().numpy(), time=50, fs=fs))\n",
    "        decay = torch.tensor([t60, c50, edt], dtype=torch.float32)\n",
    "    return wav, stft, edc, decay\n",
    "\n",
    "def helper_decay_diffs(wav_a: torch.Tensor, wav_b: torch.Tensor, fs: int):\n",
    "    \"\"\"EDTΔ (s), C50Δ (dB), T60% error using the helper functions (pairwise).\"\"\"\n",
    "    L = min(wav_a.numel(), wav_b.numel())\n",
    "    A = wav_a[:L].detach().cpu().numpy()[None, :]\n",
    "    B = wav_b[:L].detach().cpu().numpy()[None, :]\n",
    "\n",
    "    # T60: % error (invalid -> 100%)\n",
    "    t60_gt, t60_x = compute_t60(A, B, fs=fs, advanced=True)\n",
    "    t60_gt = np.atleast_1d(t60_gt).astype(float)\n",
    "    t60_x  = np.atleast_1d(t60_x).astype(float)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        t60_diff = np.abs(t60_x - t60_gt) / (np.abs(t60_gt) + 1e-12)\n",
    "    invalid = (t60_gt < -0.5) | (t60_x < -0.5)\n",
    "    t60_diff[invalid] = 1.0\n",
    "    t60_pct = float(np.mean(t60_diff) * 100.0)\n",
    "\n",
    "    # EDT & C50: absolute differences\n",
    "    edt_gt, edt_x = evaluate_edt(B, A, fs=fs)\n",
    "    edt_mae = float(np.mean(np.abs(edt_x - edt_gt)))\n",
    "\n",
    "    c50_gt, c50_x = evaluate_clarity(B, A, fs=fs)\n",
    "    c50_mae = float(np.mean(np.abs(c50_x - c50_gt)))\n",
    "\n",
    "    return dict(T60_h=t60_pct, C50_h=c50_mae, EDT_h=edt_mae)\n",
    "\n",
    "# ─── Dataset ───────────────────────────────────────────────────────\n",
    "ds = FurnishedRoomSTFTDataset(\n",
    "    root_dir=root_dir, split=\"reference\",\n",
    "    sample_rate=sample_rate, return_wav=True, mode=\"reference\"\n",
    ")\n",
    "\n",
    "a = ds[i]\n",
    "b = ds[j]\n",
    "\n",
    "wav_a, stft_a, edc_a, decay_a = get_or_compute_features(a, sample_rate, EDC_T)\n",
    "wav_b, stft_b, edc_b, decay_b = get_or_compute_features(b, sample_rate, EDC_T)\n",
    "\n",
    "# ─── Helper (pairwise) ─────────────────────────────────────────────\n",
    "helper = helper_decay_diffs(wav_a, wav_b, sample_rate)\n",
    "\n",
    "# ─── compute_audio_distance versions ───────────────────────────────\n",
    "pair_stft  = torch.stack([stft_a, stft_b], dim=0)                   # [2,F,T] (only for API shape/device)\n",
    "pair_decay = torch.stack([decay_a, decay_b], dim=0).float()         # [2,3] = [T60,C50,EDT]\n",
    "pair_edc   = torch.stack([edc_a, edc_b], dim=0).float()             # [2,T_edc]\n",
    "\n",
    "t60_cd = compute_audio_distance(pair_stft, decay_feats=pair_decay, metric='T60PCT')[0,1].item()\n",
    "c50_cd = compute_audio_distance(pair_stft, decay_feats=pair_decay, metric='C50')[0,1].item()\n",
    "edt_cd = compute_audio_distance(pair_stft, decay_feats=pair_decay, metric='EDT')[0,1].item()\n",
    "edc_d  = compute_audio_distance(pair_stft, edc_curves=pair_edc,   metric='EDC')[0,1].item()\n",
    "\n",
    "# ─── Print ─────────────────────────────────────────────────────────\n",
    "print(f\"A = {a['id']}  |  B = {b['id']}\")\n",
    "print(\"Helper (diffs):\")\n",
    "print(f\"  T60%: {helper['T60_h']:.1f}%   C50Δ: {helper['C50_h']:.3f} dB   EDTΔ: {helper['EDT_h']:.3f} s\")\n",
    "print(\"compute_audio_distance (unitless distances):\")\n",
    "print(f\"  T60_cd: {t60_cd:.6f}   C50_cd: {c50_cd:.6f}   EDT_cd: {edt_cd:.6f}   EDC_D: {edc_d:.6f}\")\n",
    "\n",
    "# ─── Quick plots: waveform + EDC curves ────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(wav_a.cpu().numpy(), label=f\"A {a['id']}\", alpha=0.8)\n",
    "axes[0].plot(wav_b.cpu().numpy(), label=f\"B {b['id']}\", alpha=0.8)\n",
    "axes[0].set_title(\"Waveforms\")\n",
    "axes[0].set_xlabel(\"Sample\"); axes[0].set_ylabel(\"Amplitude\")\n",
    "axes[0].legend(loc=\"upper right\", fontsize=9)\n",
    "\n",
    "axes[1].plot(edc_a.cpu().numpy(), label=f\"A {a['id']}\", alpha=0.9)\n",
    "axes[1].plot(edc_b.cpu().numpy(), label=f\"B {b['id']}\", alpha=0.9)\n",
    "axes[1].set_title(f\"EDC (downsampled to {EDC_T} frames)\")\n",
    "axes[1].set_xlabel(\"Frame\"); axes[1].set_ylabel(\"EDC (dB)\")\n",
    "axes[1].legend(loc=\"upper right\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1 (GPU + BATCHED + EDC): Build data, retrieve top-3 refs, compute metrics (incl. EDC), cache ---\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.transforms import GriffinLim\n",
    "from tqdm.auto import tqdm\n",
    "from evaluator import compute_audio_distance, compute_edc_db\n",
    "from retriever import FurnishedRoomSTFTDataset, RIRRetrievalMLP \n",
    "  # :contentReference[oaicite:0]{index=0}\n",
    "import sys\n",
    "sys.path.append('../NeRAF')\n",
    "from NeRAF_helper import compute_t60, evaluate_edt, evaluate_clarity\n",
    "\n",
    "# ---------------- Params ----------------\n",
    "EVAL_PATTERN         = \"../eval_results/emptyroom/emptyroom/renders/eval_*.npy\"\n",
    "MAX_EVAL_FILES       = 20\n",
    "ROOT_DIR             = \"../data/RAF/EmptyRoom\"\n",
    "CKPT_PATH            = './outputs/20250906_184700/rir_retrieval_model.ckpt'   #20250906_184700 20250812_204815\n",
    "GRID_VEC_PATH        = \"./features.pt\"  # use_global_grid is always True\n",
    "SAMPLE_RATE          = 48000\n",
    "CACHE_PATH           = \"./records_cache_autorefs.pkl\"\n",
    "\n",
    "RETRIEVAL_BACKEND    = \"METRIC\"      # \"METRIC\" (default) or \"EMBEDDING\"\n",
    "RETRIEVAL_METRIC     = \"MIXED\"       # or \"MIXED\" or \"FEATS\"\n",
    "MIXED_WEIGHTS        = ['EDC', 0.6, 'SPL', 0.4]\n",
    "TOPK                 = 3\n",
    "CHUNK_SIZE_G         = 2048\n",
    "# ---------------- New knobs for \"feature-vector\" retrieval ----------------\n",
    "FEATS_INCLUDE_EDC    = True      # include the (shape-normalized) EDC curve\n",
    "FEATS_USE_DECAYS     = ['T60','C50','EDT','DR']   # choose any subset of these four\n",
    "FEATS_CHUNK_SIZE_G   = 2048      # chunking like before\n",
    "FEATS_EPS            = 1e-6\n",
    "FEATS_EQUALIZE_GROUP_ENERGY = False\n",
    "FEATS_EDC_WEIGHT  = 0.4   # if set, overrides auto alpha\n",
    "FEATS_DECAY_WEIGHT = 0.6\n",
    "\n",
    "# ---------------- Local EDC normalizer (same logic as in retriever.py) ----\n",
    "def _normalize_edc_local(edc_db: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    edc_db: [B, T_edc] in dB. Make 0 dB at t=0, then z-score over time (per sample).\n",
    "    \"\"\"\n",
    "    edc0 = edc_db[:, :1]                   # [B,1]\n",
    "    edc_rel = edc_db - edc0\n",
    "    std = edc_rel.std(dim=1, keepdim=True).clamp_min(eps)\n",
    "    return edc_rel / std                   # [B, T_edc]\n",
    "\n",
    "def _select_decay_columns(decay_blk: torch.Tensor, names: Sequence[str]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    decay_blk: [B, D] where D can be 3 or 4 depending on whether DR was stored.\n",
    "    names: subset of ['T60','C50','EDT','DR'] in any order.\n",
    "    Maps to columns: T60->0, C50->1, EDT->2, DR->3 (if present).\n",
    "    \"\"\"\n",
    "    colmap = {'T60':0, 'C50':1, 'EDT':2, 'DR':3}\n",
    "    idxs = [colmap[n] for n in names if (n in colmap)]\n",
    "    idxs = [i for i in idxs if i < decay_blk.shape[1]]\n",
    "    return decay_blk[:, idxs] if len(idxs) else None\n",
    "\n",
    "# ---------------- Build per-chunk feature block (query + gallery) ---------\n",
    "def _build_feature_blocks_for_chunk(q_item, g_edc_chunk, g_decay_chunk, T_edc_gallery, \n",
    "                                    include_edc=True, decay_names=('T60','C50','EDT','DR')):\n",
    "    \"\"\"\n",
    "    Returns Xz: [B, D_total] on device, where B = 1 + G_chunk (query first).\n",
    "    EDC part is per-sample normalized; then all dims are z-scored per block.\n",
    "    Optionally re-weight EDC group vs decay group to equalize energy.\n",
    "    \"\"\"\n",
    "    # --- EDC block (optional) ---\n",
    "    edc_blk = None\n",
    "    if include_edc:\n",
    "        q_edc = q_item.get('edc')\n",
    "        if (q_edc is None) or (q_edc.numel() == 0):\n",
    "            wav_q = q_item['wav'].to(device)\n",
    "            q_edc = compute_edc_db(wav_q.float(), T_target=T_edc_gallery).to(device)\n",
    "        else:\n",
    "            q_edc = q_edc.to(device)\n",
    "            if q_edc.shape[0] != T_edc_gallery:\n",
    "                q_edc = (q_edc[:T_edc_gallery] if q_edc.shape[0] > T_edc_gallery\n",
    "                         else torch.nn.functional.pad(q_edc, (0, T_edc_gallery - q_edc.shape[0])))\n",
    "        edc_blk = torch.cat([q_edc.unsqueeze(0), g_edc_chunk], dim=0) if g_edc_chunk is not None else q_edc.unsqueeze(0)\n",
    "        edc_blk = _normalize_edc_local(edc_blk)   # [B, T_edc]\n",
    "\n",
    "    # --- Decay features block (optional subset) ---\n",
    "    decay_blk = None\n",
    "    if g_decay_chunk is not None and (len(decay_names) > 0):\n",
    "        q_decay = q_item.get('decay_feats')\n",
    "        if q_decay is None:\n",
    "            wav_q_np = _as_2d_numpy_cpu(q_item['wav'])\n",
    "            t60_q, _ = compute_t60(wav_q_np, wav_q_np, fs=SAMPLE_RATE, advanced=True)\n",
    "            edt_q, _ = evaluate_edt(wav_q_np, wav_q_np, fs=SAMPLE_RATE)\n",
    "            c50_q, _ = evaluate_clarity(wav_q_np, wav_q_np, fs=SAMPLE_RATE)\n",
    "            q_decay_np = np.array([\n",
    "                np.atleast_1d(t60_q).astype(float)[0],\n",
    "                np.atleast_1d(c50_q).astype(float)[0],\n",
    "                np.atleast_1d(edt_q).astype(float)[0]\n",
    "            ], dtype=np.float32)\n",
    "            q_decay = torch.from_numpy(q_decay_np).to(device)\n",
    "        else:\n",
    "            q_decay = q_decay.to(device)\n",
    "\n",
    "        # If gallery has DR and query doesn’t, add it\n",
    "        if g_decay_chunk.shape[1] == 4 and q_decay.numel() == 3:\n",
    "            from retriever import compute_dr\n",
    "            q_dr = compute_dr(q_item['wav'].detach().cpu().numpy(), fs=SAMPLE_RATE)\n",
    "            q_decay = torch.cat([q_decay, torch.tensor([q_dr], device=device, dtype=q_decay.dtype)], dim=0)\n",
    "\n",
    "        decay_blk = torch.cat([q_decay.unsqueeze(0), g_decay_chunk], dim=0)  # [B, D]\n",
    "        decay_blk = _select_decay_columns(decay_blk, list(decay_names))       # [B, D_sel] or None\n",
    "\n",
    "    # --- Concatenate parts along feature dim ---\n",
    "    pieces = []\n",
    "    d_edc = 0\n",
    "    d_decay = 0\n",
    "    if edc_blk is not None:\n",
    "        pieces.append(edc_blk); d_edc = edc_blk.shape[1]\n",
    "    if decay_blk is not None:\n",
    "        pieces.append(decay_blk); d_decay = decay_blk.shape[1]\n",
    "    if not pieces:\n",
    "        raise ValueError(\"No features selected: both EDC and decay feature set are empty.\")\n",
    "    X = torch.cat(pieces, dim=1)  # [B, D_total]\n",
    "\n",
    "    # --- z-normalize per block (query+gallery) ---\n",
    "    mu = _nanmean(X, dim=0, keepdim=True)\n",
    "    sd = _nanstd(X, dim=0, keepdim=True, eps=FEATS_EPS)\n",
    "    Xz = (X - mu) / sd  # [B, D_total]\n",
    "\n",
    "    # --- group energy equalization (post z-score) ---\n",
    "    if FEATS_EQUALIZE_GROUP_ENERGY and (d_edc > 0) and (d_decay > 0):\n",
    "        alpha = (d_decay / max(d_edc, 1)) ** 0.5 if (FEATS_EDC_WEIGHT is None) else float(FEATS_EDC_WEIGHT)\n",
    "        start = 0\n",
    "        # EDC slice\n",
    "        Xz[:, start:start+d_edc] *= alpha\n",
    "        start += d_edc\n",
    "        # DECAY slice\n",
    "        Xz[:, start:start+d_decay] *= float(FEATS_DECAY_WEIGHT)\n",
    "\n",
    "    return Xz\n",
    "\n",
    "# -------------- New batched cosine distance vectorizer --------------------\n",
    "def _featurevec_vector_batched(q_item, g_edc, g_decay, T_edc_gallery,\n",
    "                               include_edc=True, decay_names=('T60','C50','EDT','DR'),\n",
    "                               chunk_size=2048):\n",
    "    \"\"\"\n",
    "    Returns CPU distance vector [G] of cosine distances between query and all gallery items.\n",
    "    \"\"\"\n",
    "    G = (g_edc.shape[0] if g_edc is not None else g_decay.shape[0])\n",
    "    out_parts = []\n",
    "    for s in tqdm(range(0, G, chunk_size), desc=\"FEATS chunks\", unit=\"chunk\", leave=False):\n",
    "        e = min(s + chunk_size, G)\n",
    "        edc_c   = g_edc[s:e].to(device, non_blocking=True)   if g_edc   is not None else None\n",
    "        dec_c   = g_decay[s:e].to(device, non_blocking=True) if g_decay is not None else None\n",
    "\n",
    "        # Build [1+chunk, D] feature matrix (query first)\n",
    "        Xz = _build_feature_blocks_for_chunk(q_item, edc_c, dec_c, T_edc_gallery,\n",
    "                                             include_edc=include_edc, decay_names=decay_names)  # [B, D]\n",
    "        # Cosine distance = 1 - cosine similarity\n",
    "        Xn = torch.nn.functional.normalize(Xz, p=2, dim=1)   # unit rows\n",
    "        qv = Xn[0:1]                                         # [1, D]\n",
    "        sims = (qv @ Xn[1:].T).squeeze(0)                    # [chunk]\n",
    "        dvec = (1.0 - sims).detach().cpu()\n",
    "        out_parts.append(dvec)\n",
    "\n",
    "        del edc_c, dec_c, Xz, Xn, qv, sims, dvec\n",
    "        torch.cuda.empty_cache()\n",
    "    return torch.cat(out_parts, dim=0)  # [G] on CPU\n",
    "\n",
    "# ---------------- GPU setup ----------------\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "n_fft = (513 - 1) * 2; win_length = 512; hop_length = 256; power = 1\n",
    "istft = GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=power).to(device)\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _as_2d_numpy_cpu(wav_t):\n",
    "    arr = wav_t.detach().cpu().numpy()\n",
    "    return arr[None, :] if arr.ndim == 1 else arr\n",
    "\n",
    "def room_metric_diffs_gpu(wav_gt_t, wav_x_t, fs):\n",
    "    L = min(wav_gt_t.shape[0], wav_x_t.shape[0])\n",
    "    gt = _as_2d_numpy_cpu(wav_gt_t[:L]); xx = _as_2d_numpy_cpu(wav_x_t[:L])\n",
    "    t60_gt, t60_x = compute_t60(gt, xx, fs=fs, advanced=True)\n",
    "    t60_gt = np.atleast_1d(t60_gt).astype(float); t60_x  = np.atleast_1d(t60_x).astype(float)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        t60_diff = np.abs(t60_x - t60_gt) / (np.abs(t60_gt) + 1e-12)\n",
    "    invalid_mask = (t60_gt < -0.5) | (t60_x < -0.5)\n",
    "    t60_diff[invalid_mask] = 1.0\n",
    "    t60_err_pct = float(np.mean(t60_diff) * 100.0)\n",
    "    edt_gt, edt_x = evaluate_edt(xx, gt, fs=fs); edt_mae = float(np.mean(np.abs(edt_x - edt_gt)))\n",
    "    c50_gt, c50_x = evaluate_clarity(xx, gt, fs=fs); c50_mae = float(np.mean(np.abs(c50_x - c50_gt)))\n",
    "    return {'EDT': edt_mae, 'C50': c50_mae, 'T60': t60_err_pct}\n",
    "\n",
    "def _pair_metrics_gpu_with_edc(stft_a, wav_a, stft_b, wav_b, edc_a, edc_b):\n",
    "    \"\"\"Compute MSE/SPL/MAG/MAG2 + EDC (via compute_audio_distance) + room metrics.\"\"\"\n",
    "    pair_stft = torch.stack([stft_a, stft_b], dim=0)\n",
    "    L = min(wav_a.shape[0], wav_b.shape[0])\n",
    "    pair_wav  = torch.stack([wav_a[:L], wav_b[:L]], dim=0)\n",
    "\n",
    "    # EDC distance (B=2) — use same T_edc for both curves\n",
    "    T_edc = edc_a.shape[0]\n",
    "    if edc_b.shape[0] != T_edc:\n",
    "        # adjust pred/ref EDC length to GT's length if needed\n",
    "        if edc_b.shape[0] > T_edc: edc_b = edc_b[:T_edc]\n",
    "        else: edc_b = torch.nn.functional.pad(edc_b, (0, T_edc - edc_b.shape[0]))\n",
    "    pair_edc = torch.stack([edc_a, edc_b], dim=0)  # [2, T_edc]\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp), torch.no_grad():\n",
    "        mse  = compute_audio_distance(pair_stft, wavs=pair_wav, metric='MSE')[0,1].item()\n",
    "        spl  = compute_audio_distance(pair_stft, wavs=pair_wav, metric='SPL', fs=SAMPLE_RATE)[0,1].item()\n",
    "        mag  = compute_audio_distance(pair_stft, metric='MAG')[0,1].item()\n",
    "        mag2 = compute_audio_distance(pair_stft, metric='MAG2')[0,1].item()\n",
    "        edcD = compute_audio_distance(pair_stft, wavs=None, edc_curves=pair_edc, metric='EDC')[0,1].item()  # :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "    out = {'MSE': mse, 'SPL': spl, 'MAG': mag, 'MAG2': mag2, 'EDC': edcD}\n",
    "    out.update(room_metric_diffs_gpu(wav_a[:L], wav_b[:L], fs=SAMPLE_RATE))\n",
    "    return out\n",
    "\n",
    "def _build_query_blocks_for_chunk(q_stft, q_wav, q_item, g_stfts_chunk, g_wavs_chunk, g_edc_chunk, g_decay_chunk):\n",
    "    stft_blk = torch.cat([q_stft.unsqueeze(0), g_stfts_chunk], dim=0)\n",
    "    min_len = min(q_wav.shape[0], g_wavs_chunk.shape[1])\n",
    "    wav_blk = torch.cat([q_wav[:min_len].unsqueeze(0), g_wavs_chunk[:, :min_len]], dim=0)\n",
    "    edc_blk = None\n",
    "    if g_edc_chunk is not None:\n",
    "        q_edc = q_item.get('edc')\n",
    "        if q_edc is None:\n",
    "            T_edc = g_edc_chunk.shape[1]\n",
    "            q_edc = compute_edc_db(wav_blk[0].float(), T_target=T_edc).to(device)  # :contentReference[oaicite:2]{index=2}\n",
    "        else:\n",
    "            q_edc = q_edc.to(device)\n",
    "        edc_blk = torch.cat([q_edc.unsqueeze(0), g_edc_chunk], dim=0)\n",
    "    decay_blk = None\n",
    "    if (q_item.get('decay_feats') is not None) and (g_decay_chunk is not None):\n",
    "        decay_blk = torch.cat([q_item['decay_feats'].unsqueeze(0).to(device), g_decay_chunk], dim=0)\n",
    "    return stft_blk, wav_blk, edc_blk, decay_blk\n",
    "\n",
    "def _metric_vector_batched(q_stft, q_wav, q_item, g_stfts, g_wavs, g_edc, g_decay, metric, chunk_size=2048):\n",
    "    G = g_stfts.shape[0]; parts = []\n",
    "    for s in tqdm(range(0, G, chunk_size), desc=f\"{metric} chunks\", unit=\"chunk\", leave=False):\n",
    "        e = min(s + chunk_size, G)\n",
    "        stfts_c = g_stfts[s:e].to(device, non_blocking=True)\n",
    "        wavs_c  = g_wavs[s:e].to(device, non_blocking=True)\n",
    "        edc_c   = g_edc[s:e].to(device, non_blocking=True)   if g_edc   is not None else None\n",
    "        dec_c   = g_decay[s:e].to(device, non_blocking=True) if g_decay is not None else None\n",
    "        stft_blk, wav_blk, edc_blk, decay_blk = _build_query_blocks_for_chunk(q_stft, q_wav, q_item, stfts_c, wavs_c, edc_c, dec_c)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp), torch.no_grad():\n",
    "            D = compute_audio_distance(stft=stft_blk, wavs=wav_blk, edc_curves=edc_blk, decay_feats=decay_blk, metric=metric, fs=SAMPLE_RATE)\n",
    "        parts.append(D[0, 1:].detach().cpu())\n",
    "        del stfts_c, wavs_c, edc_c, dec_c, stft_blk, wav_blk, edc_blk, decay_blk, D\n",
    "        torch.cuda.empty_cache()\n",
    "    return torch.cat(parts, dim=0)  # CPU [G]\n",
    "\n",
    "def _mixed_vector_batched(q_stft, q_wav, q_item, g_stfts, g_wavs, g_edc, g_decay, weights, chunk_size=2048):\n",
    "    acc = None\n",
    "    for m, w in tqdm(list(zip(weights[0::2], weights[1::2])), desc=\"MIXED sub-metrics\", unit=\"metric\", leave=False):\n",
    "        d = _metric_vector_batched(q_stft, q_wav, q_item, g_stfts, g_wavs, g_edc, g_decay, metric=m, chunk_size=chunk_size)\n",
    "        mask = torch.isfinite(d); mu = d[mask].mean(); sd = d[mask].std().clamp_min(1e-6)\n",
    "        z = (d - mu) / sd\n",
    "        acc = z * float(w) if acc is None else acc + z * float(w)\n",
    "    return acc  # CPU [G]\n",
    "\n",
    "# --- add near imports ---\n",
    "def _nanmean(x: torch.Tensor, dim: int, keepdim: bool = False) -> torch.Tensor:\n",
    "    mask = ~torch.isnan(x)\n",
    "    x0 = torch.where(mask, x, torch.zeros_like(x))\n",
    "    cnt = mask.sum(dim=dim, keepdim=keepdim).clamp_min(1)\n",
    "    return x0.sum(dim=dim, keepdim=keepdim) / cnt\n",
    "\n",
    "def _nanstd(x: torch.Tensor, dim: int, keepdim: bool = False, eps: float = 1e-6) -> torch.Tensor:\n",
    "    m = _nanmean(x, dim=dim, keepdim=True)\n",
    "    v = _nanmean((x - m) ** 2, dim=dim, keepdim=keepdim)\n",
    "    return v.clamp_min(0).sqrt().clamp_min(eps)\n",
    "\n",
    "# ---------------- Load eval files ----------------\n",
    "file_paths = sorted(glob.glob(EVAL_PATTERN), key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))\n",
    "if MAX_EVAL_FILES is not None: file_paths = file_paths[:MAX_EVAL_FILES]\n",
    "\n",
    "ds_test = FurnishedRoomSTFTDataset(root_dir=ROOT_DIR, split=\"test\", sample_rate=SAMPLE_RATE, return_wav=True, mode=\"reference\")\n",
    "ds_ref  = FurnishedRoomSTFTDataset(root_dir=ROOT_DIR, split=\"reference\", sample_rate=SAMPLE_RATE, return_wav=True, mode=\"reference\")\n",
    "\n",
    "# Preload gallery to CPU\n",
    "ref_loader = DataLoader(ds_ref, batch_size=len(ds_ref), shuffle=False)\n",
    "batch_ref = next(iter(ref_loader))\n",
    "ref_ids       = batch_ref['id']\n",
    "ref_stfts_cpu = batch_ref['stft']\n",
    "ref_wavs_cpu  = batch_ref['wav']\n",
    "ref_edc_cpu   = batch_ref.get('edc')\n",
    "ref_decay_cpu = batch_ref.get('decay_feats')\n",
    "ref_mic_pose  = batch_ref['mic_pose']     # ← needed by the retriever\n",
    "ref_src_pose  = batch_ref['source_pose']  # ← needed by the retriever\n",
    "ref_rot       = batch_ref['rot']          # ← needed by the retriever\n",
    "G = ref_stfts_cpu.size(0)\n",
    "T_edc_gallery = (ref_edc_cpu.shape[1] if ref_edc_cpu is not None else 60)\n",
    "\n",
    "# --- NEW: Embedding backend setup ---\n",
    "Zg = None  # gallery embeddings (GPU) if we use the model\n",
    "if RETRIEVAL_BACKEND.upper() == \"EMBEDDING\":\n",
    "    ckpt    = torch.load(CKPT_PATH, map_location=device)\n",
    "    model   = RIRRetrievalMLP(**ckpt[\"model_config\"]).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    grid_vec = torch.load(GRID_VEC_PATH, map_location=device).to(device)   # use_global_grid=True\n",
    "    with torch.no_grad():\n",
    "        Gg    = grid_vec.unsqueeze(0).expand(G, -1).to(device)\n",
    "        mic_g = ref_mic_pose.to(device)\n",
    "        src_g = ref_src_pose.to(device)\n",
    "        rot_g = ref_rot.to(device)\n",
    "        Zg    = model(Gg, mic_g, src_g, rot_g)  # [G, D] embeddings on device\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "# ---------------- Per-file loop ----------------\n",
    "for fp in tqdm(file_paths, desc=f\"Processing eval files (batched on {device.type.upper()})\", unit=\"file\"):\n",
    "    data = np.load(fp, allow_pickle=True).item()\n",
    "    idx = int(data[\"audio_idx\"])\n",
    "    test_item = ds_test[idx]\n",
    "\n",
    "    # Move query to GPU\n",
    "    wav_gt   = test_item['wav'].squeeze().to(device, non_blocking=True)\n",
    "    stft_gt  = test_item['stft'].squeeze(0).to(device, non_blocking=True)\n",
    "\n",
    "    stft_pred = torch.from_numpy(data[\"pred_stft\"]).float().squeeze(0).to(device)\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        mag_pred  = torch.exp(stft_pred) - 1e-3\n",
    "        wav_pred  = istft(mag_pred.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    # align for waveform metrics\n",
    "    L = min(wav_gt.shape[0], wav_pred.shape[0]); wav_gt, wav_pred = wav_gt[:L], wav_pred[:L]\n",
    "\n",
    "    # ---- Retrieval distances (batched) ----\n",
    "    # ---- Retrieval (two backends) ----\n",
    "    if RETRIEVAL_BACKEND.upper() == \"EMBEDDING\":\n",
    "        # Build query embedding with the same model + global grid features\n",
    "        test_item = ds_test[idx]  # already set above; keeping for clarity\n",
    "        with torch.no_grad():\n",
    "            Gq    = grid_vec.unsqueeze(0).to(device)\n",
    "            mic_q = test_item['mic_pose'].unsqueeze(0).to(device)\n",
    "            src_q = test_item['source_pose'].unsqueeze(0).to(device)\n",
    "            rot_q = test_item['rot'].unsqueeze(0).to(device)\n",
    "            Zq    = model(Gq, mic_q, src_q, rot_q)     # [1, D]\n",
    "\n",
    "            S = (Zq @ Zg.T).squeeze(0)                 # [G]\n",
    "            top3_idx = torch.topk(S, k=TOPK, largest=True).indices.tolist()\n",
    "            top3_ids = [ref_ids[i] for i in top3_idx]\n",
    "\n",
    "        # for uniform downstream, synthesize a \"distance\" vector if you need it later\n",
    "        # (not strictly required here)\n",
    "        d_vec_cpu = None\n",
    "\n",
    "    else:\n",
    "        # ---- Retrieval distances (batched metrics; existing behavior) ----\n",
    "        if RETRIEVAL_METRIC == \"FEATS\":\n",
    "            d_vec_cpu = _featurevec_vector_batched(\n",
    "                q_item=test_item,\n",
    "                g_edc=ref_edc_cpu, \n",
    "                g_decay=ref_decay_cpu,\n",
    "                T_edc_gallery=T_edc_gallery,\n",
    "                include_edc=FEATS_INCLUDE_EDC,\n",
    "                decay_names=tuple(FEATS_USE_DECAYS),\n",
    "                chunk_size=FEATS_CHUNK_SIZE_G\n",
    "            )\n",
    "        elif RETRIEVAL_METRIC != \"MIXED\":\n",
    "            d_vec_cpu = _metric_vector_batched(\n",
    "                stft_gt, wav_gt, test_item,\n",
    "                ref_stfts_cpu, ref_wavs_cpu, ref_edc_cpu, ref_decay_cpu,\n",
    "                metric=RETRIEVAL_METRIC, chunk_size=CHUNK_SIZE_G\n",
    "            )\n",
    "        else:\n",
    "            d_vec_cpu = _mixed_vector_batched(\n",
    "                stft_gt, wav_gt, test_item,\n",
    "                ref_stfts_cpu, ref_wavs_cpu, ref_edc_cpu, ref_decay_cpu,\n",
    "                weights=MIXED_WEIGHTS, chunk_size=CHUNK_SIZE_G\n",
    "            )\n",
    "\n",
    "        top3_idx = torch.argsort(d_vec_cpu)[:TOPK].tolist()\n",
    "        top3_ids = [ref_ids[i] for i in top3_idx]\n",
    "\n",
    "\n",
    "    # ---- Build EDC curves (GT / Pred / Refs) for plotting & distances ----\n",
    "    # Consistent target length based on gallery (fallback 60 if missing)\n",
    "    T_edc_gallery = (ref_edc_cpu.shape[1] if ref_edc_cpu is not None else 60)\n",
    "\n",
    "    # GT EDC: prefer dataset's; else compute to gallery length\n",
    "    q_edc = test_item.get('edc')\n",
    "    if q_edc is None or q_edc.numel() == 0:\n",
    "        q_edc = compute_edc_db(wav_gt.float(), T_target=T_edc_gallery).to(device)\n",
    "    else:\n",
    "        # ensure correct length\n",
    "        q_edc = q_edc.to(device)\n",
    "        if q_edc.shape[0] != T_edc_gallery:\n",
    "            if q_edc.shape[0] > T_edc_gallery:\n",
    "                q_edc = q_edc[:T_edc_gallery]\n",
    "            else:\n",
    "                q_edc = torch.nn.functional.pad(q_edc, (0, T_edc_gallery - q_edc.shape[0]))\n",
    "\n",
    "    # Pred EDC: always compute from wav_pred to same length\n",
    "    p_edc = compute_edc_db(wav_pred.float(), T_target=T_edc_gallery).to(device)\n",
    "\n",
    "    # (optional sanity) avoid NaNs\n",
    "    q_edc = torch.nan_to_num(q_edc, nan=0.0)\n",
    "    p_edc = torch.nan_to_num(p_edc, nan=0.0)\n",
    "\n",
    "\n",
    "    # ---- Metrics: pred vs GT (with EDC) ----\n",
    "    pred_metrics = _pair_metrics_gpu_with_edc(stft_gt, wav_gt, stft_pred, wav_pred, q_edc, p_edc)\n",
    "\n",
    "    # ---- Metrics: top-3 refs vs GT (with EDC) + store ref EDC curves for plotting ----\n",
    "    edc_refs_curves = []\n",
    "    refs_out = []\n",
    "    for i in tqdm(top3_idx, desc=\"Top-K metrics\", unit=\"ref\", leave=False):\n",
    "        stft_r = ref_stfts_cpu[i].to(device, non_blocking=True)\n",
    "        wav_r  = ref_wavs_cpu[i].to(device, non_blocking=True)\n",
    "        # ref EDC curve (already precomputed in dataset)\n",
    "        if ref_edc_cpu is not None:\n",
    "            r_edc = ref_edc_cpu[i].to(device, non_blocking=True)\n",
    "        else:\n",
    "            # fallback if not present\n",
    "            r_edc = compute_edc_db(wav_r.float(), T_target=T_edc_gallery).to(device)\n",
    "        m = _pair_metrics_gpu_with_edc(stft_gt, wav_gt, stft_r, wav_r, q_edc, r_edc)\n",
    "        refs_out.append({'id': ref_ids[i], **m})\n",
    "        edc_refs_curves.append(r_edc.detach().cpu())\n",
    "\n",
    "    records.append({\n",
    "        'idx': idx,\n",
    "        'id': test_item['id'],\n",
    "        'wav_gt':   wav_gt.detach().cpu(),   'stft_gt':  stft_gt.detach().cpu(),\n",
    "        'wav_pred': wav_pred.detach().cpu(), 'stft_pred': stft_pred.detach().cpu(),\n",
    "        'edc_gt':   q_edc.detach().cpu(),    'edc_pred': p_edc.detach().cpu(),       # <-- store EDC curves\n",
    "        'edc_refs': edc_refs_curves,                                                     # list of 3 curves\n",
    "        'retrieval': {\n",
    "            'backend': RETRIEVAL_BACKEND,                 # ← new\n",
    "            'metric': RETRIEVAL_METRIC if RETRIEVAL_BACKEND.upper()==\"METRIC\" else None,\n",
    "            'weights': MIXED_WEIGHTS if (RETRIEVAL_BACKEND.upper()==\"METRIC\" and RETRIEVAL_METRIC==\"MIXED\") else None,\n",
    "            'normalize_mode': \"per_query\",\n",
    "            'top3_ids': top3_ids,\n",
    "            'ckpt_path': CKPT_PATH if RETRIEVAL_BACKEND.upper()==\"EMBEDDING\" else None,\n",
    "        },\n",
    "        'metrics_pred': pred_metrics,\n",
    "        'metrics_refs': refs_out,   # list of 3 dicts (each has 'EDC' now)\n",
    "    })\n",
    "\n",
    "    del wav_gt, stft_gt, stft_pred, wav_pred, q_edc, p_edc\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ---------------- Save cache ----------------\n",
    "with open(CACHE_PATH, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"records\": records,\n",
    "        \"params\": {\n",
    "            \"sample_rate\": SAMPLE_RATE,\n",
    "            \"n_fft\": n_fft, \"win_length\": win_length, \"hop_length\": hop_length, \"power\": power,\n",
    "            \"retrieval_metric\": RETRIEVAL_METRIC,\n",
    "            \"mixed_weights\": MIXED_WEIGHTS,\n",
    "            \"normalize_mode\": \"per_query\",\n",
    "            \"topk\": TOPK,\n",
    "            \"root_dir\": ROOT_DIR,\n",
    "            \"chunk_size_g\": CHUNK_SIZE_G,\n",
    "            \"retrieval_backend\": RETRIEVAL_BACKEND,\n",
    "            \"ckpt_path\": CKPT_PATH if RETRIEVAL_BACKEND.upper()==\"EMBEDDING\" else None,\n",
    "            \"grid_vec_path\": GRID_VEC_PATH if RETRIEVAL_BACKEND.upper()==\"EMBEDDING\" else None,\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "print(f\"Cached {len(records)} records to {CACHE_PATH}. \"\n",
    "      f\"Example keys: {list(records[0].keys()) if records else '[]'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: Load cache, compute averages & deltas, plot by metric (now with EDC curves) ---\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from retriever import FurnishedRoomSTFTDataset\n",
    "\n",
    "CACHE_PATH = \"./records_cache_autorefs.pkl\"  # update if different\n",
    "\n",
    "# --- User knobs ---\n",
    "SORT_METRIC = \"SPL\"      # can now also be \"EDC\"\n",
    "TOP_N       = 20\n",
    "\n",
    "def three_line_block(id_str, m):\n",
    "    # 1) ID\n",
    "    # 2) SPL, MSE, MAG, MAG2\n",
    "    # 3) T60%, C50Δ, EDTΔ, EDC\n",
    "    line1 = f\"{id_str}\"\n",
    "    line2 = (f\"SPL: {m['SPL']:.4f}   \"\n",
    "             f\"MSE: {m['MSE']:.2e}   \"\n",
    "             f\"MAG: {m['MAG']:.2f}   \"\n",
    "             f\"MAG2: {m['MAG2']:.2e}\")\n",
    "    line3 = (f\"T60%: {m['T60']:.1f}%   \"\n",
    "             f\"C50Δ: {m['C50']:.2f} dB   \"\n",
    "             f\"EDTΔ: {m['EDT']:.3f} s   \"\n",
    "             f\"EDC: {m['EDC']:.3f}\")\n",
    "    return f\"{line1}\\n{line2}\\n{line3}\"\n",
    "\n",
    "def _align_edc(a: torch.Tensor, b: torch.Tensor):\n",
    "    La, Lb = a.shape[0], b.shape[0]\n",
    "    L = min(La, Lb)\n",
    "    return a[:L], b[:L], L\n",
    "\n",
    "with open(CACHE_PATH, \"rb\") as f:\n",
    "    cache = pickle.load(f)\n",
    "records = cache[\"records\"]; params = cache[\"params\"]\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError(\"No records in cache. Run Cell 1 first.\")\n",
    "\n",
    "# include EDC in comparisons\n",
    "sample_metrics = ['SPL','MSE','MAG','MAG2','EDC','T60','C50','EDT']\n",
    "for key in sample_metrics:\n",
    "    if key not in records[0]['metrics_pred']:\n",
    "        raise ValueError(f\"Missing metric '{key}' in cached data.\")\n",
    "\n",
    "def _accumulate_averages(rows):\n",
    "    K = 3\n",
    "    sums_pred = {m:0.0 for m in sample_metrics}\n",
    "    sums_ref  = [{m:0.0 for m in sample_metrics} for _ in range(K)]\n",
    "    n = len(rows)\n",
    "    for rec in rows:\n",
    "        for m in sample_metrics:\n",
    "            sums_pred[m] += float(rec['metrics_pred'][m])\n",
    "        for k in range(K):\n",
    "            for m in sample_metrics:\n",
    "                sums_ref[k][m] += float(rec['metrics_refs'][k][m])\n",
    "    avg_pred = {m: sums_pred[m]/n for m in sample_metrics}\n",
    "    avg_ref  = [{m: sums_ref[k][m]/n for m in sample_metrics} for k in range(K)]\n",
    "    deltas   = [{m: (avg_ref[k][m] - avg_pred[m]) for m in sample_metrics} for k in range(K)]\n",
    "    return avg_pred, avg_ref, deltas\n",
    "\n",
    "avg_pred_all, avg_ref_all, deltas_all = _accumulate_averages(records)\n",
    "\n",
    "print(\"=== Averages over ALL selected eval samples ===\")\n",
    "print(\"Prediction vs GT:\")\n",
    "for m in sample_metrics: print(f\"  {m:>4}: {avg_pred_all[m]:.8f}\")\n",
    "for k in range(3):\n",
    "    print(f\"\\nTop-{k+1} reference vs GT:\")\n",
    "    for m in sample_metrics: print(f\"  {m:>4}: {avg_ref_all[k][m]:.8f}\")\n",
    "    print(\"  Δ(ref - pred):\")\n",
    "    for m in sample_metrics: print(f\"    {m:>4}: {deltas_all[k][m]:+.8f}  ({'better' if deltas_all[k][m]<0 else 'worse'})\")\n",
    "\n",
    "records_sorted = sorted(records, key=lambda r: r['metrics_pred'][SORT_METRIC], reverse=True)\n",
    "worst = records_sorted[:min(TOP_N, len(records_sorted))]\n",
    "# worst = records_sorted[-TOP_N:]\n",
    "\n",
    "avg_pred_topN, avg_ref_topN, deltas_topN = _accumulate_averages(worst)\n",
    "print(f\"\\n=== Averages over Top-{len(worst)} WORST by {SORT_METRIC} ===\")\n",
    "print(\"Prediction vs GT:\")\n",
    "for m in sample_metrics: print(f\"  {m:>4}: {avg_pred_topN[m]:.8f}\")\n",
    "for k in range(3):\n",
    "    print(f\"\\nTop-{k+1} reference vs GT:\")\n",
    "    for m in sample_metrics: print(f\"  {m:>4}: {avg_ref_topN[k][m]:.8f}\")\n",
    "    print(\"  Δ(ref - pred):\")\n",
    "    for m in sample_metrics: print(f\"    {m:>4}: {deltas_topN[k][m]:+.8f}  ({'better' if deltas_topN[k][m]<0 else 'worse'})\")\n",
    "\n",
    "# --- Plotting (time, STFT, EDC) for Top_N worst by SORT_METRIC ---\n",
    "for rec in worst:\n",
    "    # 1) Time-domain canvas\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    axes[0,0].plot(rec['wav_gt'].numpy());   axes[0,0].set_title('Ground Truth', fontsize=11, pad=8)\n",
    "    axes[0,1].plot(rec['wav_pred'].numpy()); axes[0,1].set_title('Prediction', fontsize=11, pad=8)\n",
    "    axes[0,0].set_xlabel('Sample'); axes[0,0].set_ylabel('Amplitude')\n",
    "    axes[0,1].set_xlabel('Sample'); axes[0,1].set_ylabel('Amplitude')\n",
    "    axes[0,2].axis('off')\n",
    "    for i, rm in enumerate(rec['metrics_refs'][:3]):\n",
    "        rid = rec['retrieval']['top3_ids'][i]\n",
    "        ds_ref = 'dataset_ref' in globals() and dataset_ref or FurnishedRoomSTFTDataset(\n",
    "            root_dir=params.get(\"root_dir\", \"../data/RAF/EmptyRoom\"), split=\"reference\",\n",
    "            sample_rate=params[\"sample_rate\"], return_wav=True, mode=\"reference\"\n",
    "        )\n",
    "        if 'dataset_ref' not in globals(): dataset_ref = ds_ref\n",
    "        wav_r = dataset_ref[dataset_ref.id2idx[rid]]['wav']\n",
    "        axes[1,i].plot(wav_r.numpy())\n",
    "        axes[1,i].set_xlabel('Sample'); axes[1,i].set_ylabel('Amplitude')\n",
    "        axes[1,i].set_title(three_line_block(f\"Ref {rm['id']}\", rm), fontsize=10, pad=10)\n",
    "    fig.suptitle(three_line_block(f\"{rec['id']} (sorted by {SORT_METRIC})\", rec['metrics_pred']), fontsize=12, y=0.90)\n",
    "    fig.subplots_adjust(top=0.83, hspace=0.45, wspace=0.25)\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.88]); plt.show()\n",
    "\n",
    "    # 2) STFT canvas\n",
    "    fig2, axes2 = plt.subplots(2, 3, figsize=(18, 9))\n",
    "    im = axes2[0,0].imshow(rec['stft_gt'].numpy(), aspect='auto', origin='lower');  axes2[0,0].set_title('GT STFT', fontsize=11, pad=8)\n",
    "    im2= axes2[0,1].imshow(rec['stft_pred'].numpy(), aspect='auto', origin='lower'); axes2[0,1].set_title('Pred STFT', fontsize=11, pad=8)\n",
    "    axes2[0,0].set_xlabel('Time frame'); axes2[0,0].set_ylabel('Frequency bin')\n",
    "    axes2[0,1].set_xlabel('Time frame'); axes2[0,1].set_ylabel('Frequency bin')\n",
    "    plt.colorbar(im, ax=axes2[0,0], fraction=0.046, pad=0.04); plt.colorbar(im2, ax=axes2[0,1], fraction=0.046, pad=0.04)\n",
    "    axes2[0,2].axis('off')\n",
    "    for i, rm in enumerate(rec['metrics_refs'][:3]):\n",
    "        rid = rec['retrieval']['top3_ids'][i]\n",
    "        ds_ref = 'dataset_ref' in globals() and dataset_ref or FurnishedRoomSTFTDataset(\n",
    "            root_dir=params.get(\"root_dir\", \"../data/RAF/EmptyRoom\"), split=\"reference\",\n",
    "            sample_rate=params[\"sample_rate\"], return_wav=True, mode=\"reference\"\n",
    "        )\n",
    "        if 'dataset_ref' not in globals(): dataset_ref = ds_ref\n",
    "        stft_r = dataset_ref[dataset_ref.id2idx[rid]]['stft']\n",
    "        imr = axes2[1,i].imshow(stft_r.numpy(), aspect='auto', origin='lower')\n",
    "        axes2[1,i].set_title(three_line_block(f\"Ref {rm['id']}\", rm), fontsize=10, pad=10)\n",
    "        axes2[1,i].set_xlabel('Time frame'); axes2[1,i].set_ylabel('Frequency bin')\n",
    "        plt.colorbar(imr, ax=axes2[1,i], fraction=0.046, pad=0.04)\n",
    "    fig2.suptitle(three_line_block(f\"{rec['id']} (sorted by {SORT_METRIC})\", rec['metrics_pred']), fontsize=12, y=0.98)\n",
    "    fig2.subplots_adjust(top=0.82, hspace=0.50, wspace=0.28)\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.87]); plt.show()\n",
    "\n",
    "    # 3) EDC canvas (new/updated): line plots of EDC curves\n",
    "    fig3, axes3 = plt.subplots(2, 3, figsize=(18, 7))\n",
    "\n",
    "    # GT vs Pred\n",
    "    edc_gt = rec['edc_gt']\n",
    "    edc_pr = rec['edc_pred']\n",
    "    edc_gt_al, edc_pr_al, L = _align_edc(edc_gt, edc_pr)\n",
    "    t = np.arange(L)\n",
    "    axes3[0,0].plot(t, edc_gt_al.numpy(), label='GT')\n",
    "    axes3[0,0].plot(t, edc_pr_al.numpy(), label='Pred', alpha=0.9)\n",
    "    axes3[0,0].set_title('EDC: GT vs Pred', fontsize=11, pad=8)\n",
    "    axes3[0,0].set_xlabel('EDC frame'); axes3[0,0].set_ylabel('EDC (dB)')\n",
    "    axes3[0,0].legend(loc='best')\n",
    "\n",
    "    axes3[0,1].axis('off'); axes3[0,2].axis('off')\n",
    "\n",
    "    # three refs: GT vs each top-3 ref\n",
    "    for i, rm in enumerate(rec['metrics_refs'][:3]):\n",
    "        edc_r = rec['edc_refs'][i]\n",
    "        edc_gt_al, edc_r_al, Lr = _align_edc(rec['edc_gt'], edc_r)\n",
    "        tr = np.arange(Lr)\n",
    "        ax = axes3[1,i]\n",
    "        ax.plot(tr, edc_gt_al.numpy(), label='GT')\n",
    "        ax.plot(tr, edc_r_al.numpy(), label=f\"Ref {rm['id']}\", alpha=0.9)\n",
    "        ax.set_xlabel('EDC frame'); ax.set_ylabel('EDC (dB)')\n",
    "        ax.set_title(three_line_block(f\"Ref {rm['id']}\", rm), fontsize=10, pad=10)\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    fig3.suptitle(three_line_block(f\"{rec['id']} (sorted by {SORT_METRIC})\", rec['metrics_pred']), fontsize=12, y=0.96)\n",
    "    fig3.subplots_adjust(top=0.86, hspace=0.45, wspace=0.25)\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.90]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLEAN 5-COLUMN GALLERY (GT | Pred | Top-1 | Top-2 | Top-3) ===\n",
    "# Paper-ready: no ticks, bold headers row 1, metrics above plots.\n",
    "\n",
    "# ==== KNOBS ====\n",
    "CACHE_PATH         = \"./records_cache_autorefs.pkl\"\n",
    "ROOT_DIR           = \"../data/RAF/FurnishedRoom\"\n",
    "SAMPLE_RATE        = 48000\n",
    "\n",
    "NUM_ROWS           = 15\n",
    "SORT_BY_METRIC     = \"SPL\"   # \"SPL\",\"T60\",\"C50\",\"EDT\",\"EDC\"\n",
    "WORST_FIRST        = True    # True → worst→best, False → best→worst\n",
    "\n",
    "SHOW_WAVEFORMS     = True\n",
    "SHOW_STFTS         = True\n",
    "\n",
    "FIG_DPI            = 130\n",
    "FIG_W              = 14\n",
    "FIG_H_WAVE_PERROW  = 1.05\n",
    "FIG_H_STFT_PERROW  = 1.05\n",
    "\n",
    "COL_TITLE_SIZE     = 12\n",
    "METRIC_FONTSIZE    = 8\n",
    "BORDER_LINEWIDTH   = 0.6\n",
    "BORDER_ALPHA       = 0.85\n",
    "\n",
    "# ==== IMPORTS ====\n",
    "import pickle, numpy as np, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from retriever import FurnishedRoomSTFTDataset\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# ==== LOAD CACHE ====\n",
    "with open(CACHE_PATH, \"rb\") as f:\n",
    "    cache = pickle.load(f)\n",
    "records = cache[\"records\"]; params = cache[\"params\"]\n",
    "if not records:\n",
    "    raise RuntimeError(\"No records in cache.\")\n",
    "\n",
    "sample_metrics = ['SPL','T60','C50','EDT','EDC']\n",
    "for m in sample_metrics:\n",
    "    if m not in records[0]['metrics_pred']:\n",
    "        raise ValueError(f\"Missing metric '{m}' in cache.\")\n",
    "\n",
    "# ==== REF DATASETS ====\n",
    "dataset_ref_primary = FurnishedRoomSTFTDataset(\n",
    "    root_dir=params.get(\"root_dir\", ROOT_DIR),\n",
    "    split=\"reference\",\n",
    "    sample_rate=params.get(\"sample_rate\", SAMPLE_RATE),\n",
    "    return_wav=True, mode=\"reference\"\n",
    ")\n",
    "dataset_ref_fallback = FurnishedRoomSTFTDataset(\n",
    "    root_dir=params.get(\"root_dir\", ROOT_DIR),\n",
    "    split=\"train\",\n",
    "    sample_rate=params.get(\"sample_rate\", SAMPLE_RATE),\n",
    "    return_wav=True, mode=\"reference\"\n",
    ")\n",
    "\n",
    "def _find_ref_by_id(rid):\n",
    "    rid_str = str(rid)\n",
    "    idx = dataset_ref_primary.id2idx.get(rid_str, None)\n",
    "    if idx is not None:\n",
    "        return dataset_ref_primary[idx]\n",
    "    idx = dataset_ref_fallback.id2idx.get(rid_str, None)\n",
    "    if idx is not None:\n",
    "        return dataset_ref_fallback[idx]\n",
    "    return None\n",
    "\n",
    "# ==== HELPERS ====\n",
    "def _fmt_one_line(m):\n",
    "    return f\"ΔSTFT:{m['SPL']:.3f}  T60%:{m['T60']:.2f}  C50:{m['C50']:.3f}  EDT:{m['EDT']:.3f}\"\n",
    "\n",
    "def _put_metrics_above(ax, text, fontsize, y_offset=1.05):\n",
    "    ax.text(0.5, y_offset, text, transform=ax.transAxes,\n",
    "            ha=\"center\", va=\"bottom\", fontsize=fontsize,\n",
    "            fontweight=\"normal\", clip_on=False)\n",
    "\n",
    "def _collect_vmin_vmax(*arrs):\n",
    "    vmin = min(float(a.min()) for a in arrs)\n",
    "    vmax = max(float(a.max()) for a in arrs)\n",
    "    return vmin, vmax\n",
    "\n",
    "def _sort_key(rec, metric):\n",
    "    return float(rec['metrics_pred'][metric])\n",
    "\n",
    "# ==== PREPARE ROWS ====\n",
    "recs_sorted = sorted(records, key=lambda r: _sort_key(r, SORT_BY_METRIC), reverse=WORST_FIRST)\n",
    "rows = recs_sorted[:min(NUM_ROWS, len(recs_sorted))]\n",
    "\n",
    "col_keys   = [\"GT\", \"Pred\", \"Ref1\", \"Ref2\", \"Ref3\"]\n",
    "col_titles = [\"Ground Truth\", \"Prediction\", \"Top-1 Retrieved\", \"Top-2 Retrieved\", \"Top-3 Retrieved\"]\n",
    "\n",
    "gallery = []\n",
    "missing_count = 0\n",
    "for rec in rows:\n",
    "    top3_ids = rec['retrieval']['top3_ids'][:3]\n",
    "    ref_blocks = []\n",
    "    for i, rid in enumerate(top3_ids):\n",
    "        ref_item = _find_ref_by_id(rid)\n",
    "        if ref_item is None:\n",
    "            missing_count += 1\n",
    "            ref_blocks.append({\"wav\": None, \"stft\": None, \"m\": rec['metrics_refs'][i], \"missing\": True})\n",
    "        else:\n",
    "            ref_blocks.append({\n",
    "                \"wav\":  ref_item['wav'].squeeze().cpu(),\n",
    "                \"stft\": ref_item['stft'].squeeze(0).cpu(),\n",
    "                \"m\":    rec['metrics_refs'][i],\n",
    "                \"missing\": False\n",
    "            })\n",
    "    gallery.append({\n",
    "        \"id\": rec['id'],\n",
    "        \"GT\":   {\"wav\": rec['wav_gt'].cpu(),  \"stft\": rec['stft_gt'].cpu()},\n",
    "        \"Pred\": {\"wav\": rec['wav_pred'].cpu(), \"stft\": rec['stft_pred'].cpu(), \"m\": rec['metrics_pred']},\n",
    "        \"Ref1\": ref_blocks[0], \"Ref2\": ref_blocks[1], \"Ref3\": ref_blocks[2],\n",
    "    })\n",
    "\n",
    "if missing_count:\n",
    "    print(f\"[INFO] {missing_count} refs not found in either split.\")\n",
    "\n",
    "# ==== FIGURE A: WAVEFORMS ====\n",
    "if SHOW_WAVEFORMS:\n",
    "    figA, axsA = plt.subplots(len(gallery), 5,\n",
    "                              figsize=(FIG_W, FIG_H_WAVE_PERROW*len(gallery)), dpi=FIG_DPI)\n",
    "    if len(gallery) == 1: axsA = np.expand_dims(axsA, 0)\n",
    "\n",
    "    for r, row in enumerate(gallery):\n",
    "        for c, key in enumerate(col_keys):\n",
    "            ax = axsA[r, c]; blk = row[key]\n",
    "            if blk.get(\"missing\", False):\n",
    "                ax.axis(\"off\")\n",
    "                ax.text(0.5, 0.5, \"ref not found\", transform=ax.transAxes,\n",
    "                        ha=\"center\", va=\"center\", fontsize=8)\n",
    "            else:\n",
    "                w = blk[\"wav\"].float().numpy()\n",
    "                t = np.arange(len(w)) / SAMPLE_RATE\n",
    "                ax.plot(t, w, linewidth=0.8)\n",
    "                ax.set_xticks([]); ax.set_yticks([])\n",
    "                for sp in ax.spines.values():\n",
    "                    sp.set_linewidth(BORDER_LINEWIDTH); sp.set_alpha(BORDER_ALPHA)\n",
    "\n",
    "            if r == 0:\n",
    "                ax.set_title(col_titles[c], fontweight=\"bold\", fontsize=COL_TITLE_SIZE, pad=20)\n",
    "                if key != \"GT\" and not blk.get(\"missing\", False):\n",
    "                    _put_metrics_above(ax, _fmt_one_line(blk[\"m\"]), METRIC_FONTSIZE, y_offset=1.15)\n",
    "            else:\n",
    "                if key != \"GT\" and not blk.get(\"missing\", False):\n",
    "                    _put_metrics_above(ax, _fmt_one_line(blk[\"m\"]), METRIC_FONTSIZE, y_offset=1.05)\n",
    "\n",
    "    figA.suptitle(f\"Waveforms — rows: samples (sorted by {SORT_BY_METRIC}, {'worst→best' if WORST_FIRST else 'best→worst'})\",\n",
    "                  y=0.995, fontsize=10)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "# ==== FIGURE B: STFTs ====\n",
    "if SHOW_STFTS:\n",
    "    figB, axsB = plt.subplots(len(gallery), 5,\n",
    "                              figsize=(FIG_W, FIG_H_STFT_PERROW*len(gallery)), dpi=FIG_DPI)\n",
    "    if len(gallery) == 1: axsB = np.expand_dims(axsB, 0)\n",
    "\n",
    "    for r, row in enumerate(gallery):\n",
    "        Sarr = []\n",
    "        for key in col_keys:\n",
    "            blk = row[key]\n",
    "            Sarr.append(None if blk.get(\"missing\", False) else blk[\"stft\"].float().numpy())\n",
    "        valid = [a for a in Sarr if a is not None]\n",
    "        vmin, vmax = _collect_vmin_vmax(*valid) if valid else (0, 1)\n",
    "\n",
    "        for c, key in enumerate(col_keys):\n",
    "            ax = axsB[r, c]; blk = row[key]\n",
    "            if blk.get(\"missing\", False):\n",
    "                ax.axis(\"off\")\n",
    "                ax.text(0.5, 0.5, \"ref not found\", transform=ax.transAxes,\n",
    "                        ha=\"center\", va=\"center\", fontsize=8)\n",
    "            else:\n",
    "                ax.imshow(blk[\"stft\"].float().numpy(), origin=\"lower\", aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "                ax.set_xticks([]); ax.set_yticks([])\n",
    "                for sp in ax.spines.values():\n",
    "                    sp.set_linewidth(BORDER_LINEWIDTH); sp.set_alpha(BORDER_ALPHA)\n",
    "\n",
    "            if r == 0:\n",
    "                ax.set_title(col_titles[c], fontweight=\"bold\", fontsize=COL_TITLE_SIZE, pad=20)\n",
    "                if key != \"GT\" and not blk.get(\"missing\", False):\n",
    "                    _put_metrics_above(ax, _fmt_one_line(blk[\"m\"]), METRIC_FONTSIZE, y_offset=1.15)\n",
    "            else:\n",
    "                if key != \"GT\" and not blk.get(\"missing\", False):\n",
    "                    _put_metrics_above(ax, _fmt_one_line(blk[\"m\"]), METRIC_FONTSIZE, y_offset=1.05)\n",
    "\n",
    "    figB.suptitle(f\"STFT log-mag — rows: samples (sorted by {SORT_BY_METRIC}, {'worst→best' if WORST_FIRST else 'best→worst'})\",\n",
    "                  y=0.995, fontsize=10)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbcf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieval for all splits: top-10 from reference using MIXED(EDC,SPL) or EMBEDDING ---\n",
    "import os, json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# use your retriever dataset + distance\n",
    "from retriever import FurnishedRoomSTFTDataset\n",
    "from evaluator import compute_audio_distance\n",
    "\n",
    "# Optional embedding backend (RIRRetrievalMLP)\n",
    "EMBEDDING_CKPT_PATH = './outputs/20250812_204815/rir_retrieval_model.ckpt'\n",
    "GRID_VEC_PATH       = './features.pt'   # use_global_grid=True\n",
    "\n",
    "# ─── configs ─────────────────────────────────────────────────────────────────────────────\n",
    "root                 = '../data/RAF/EmptyRoom'\n",
    "\n",
    "# Backend: \"MIXED\" (EDC/SPL) or \"EMBEDDING\"\n",
    "RETRIEVAL_BACKEND    = \"MIXED\"          # or \"EMBEDDING\"\n",
    "\n",
    "# For MIXED backend (default weights)\n",
    "MIXED_WEIGHTS        = [('EDC', 0.6), ('SPL', 0.4)]   # change here if desired\n",
    "\n",
    "# For both backends\n",
    "topk                 = 10\n",
    "query_batch_size     = 2048     # tune to your GPU (works with chunking, so flexible)\n",
    "gallery_chunk_size   = 2048    # for MIXED: process gallery in chunks to avoid OOM\n",
    "device               = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Splits mapping to your dataset\n",
    "splits = {\n",
    "    \"train\":      \"train\",\n",
    "    \"evaluation\": \"validation\",\n",
    "    \"test\":       \"test\",\n",
    "}\n",
    "\n",
    "# ─── helpers ─────────────────────────────────────────────────────────────────────────────\n",
    "def _z_per_query(d_qg: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Z-score per query row, ignoring inf/nan. d_qg: [B, G] on CPU.\n",
    "    Returns z-normalized distances (mean 0, std 1) per query.\n",
    "    \"\"\"\n",
    "    x = d_qg\n",
    "    mask = torch.isfinite(x)\n",
    "    # mean/std per row\n",
    "    row_cnt = mask.sum(dim=1, keepdim=True).clamp_min(1)\n",
    "    row_sum = torch.where(mask, x, torch.zeros_like(x)).sum(dim=1, keepdim=True)\n",
    "    mu = row_sum / row_cnt\n",
    "    var = torch.where(mask, (x - mu)**2, torch.zeros_like(x)).sum(dim=1, keepdim=True) / row_cnt\n",
    "    sd = var.clamp_min(0).sqrt().clamp_min(1e-6)\n",
    "    z = (x - mu) / sd\n",
    "    # keep inf/nan as large numbers so they rank last\n",
    "    z = torch.where(mask, z, torch.full_like(z, 1e6))\n",
    "    return z\n",
    "\n",
    "def _compute_block_distance(metric: str, q_batch, g_batch):\n",
    "    \"\"\"\n",
    "    Compute [B, G_chunk] distance block for a single metric, on GPU, then return CPU tensor.\n",
    "    Uses the (B+G_chunk)^2 trick with compute_audio_distance, then slices the [B x G_chunk] submatrix.\n",
    "    \"\"\"\n",
    "    # Flatten STFTs (whatever \"stft\" shape is, we keep consistent with your example)\n",
    "    flat_q = q_batch[\"stft\"].view(q_batch[\"stft\"].shape[0], -1).to(device, non_blocking=True)\n",
    "    flat_g = g_batch[\"stft\"].view(g_batch[\"stft\"].shape[0], -1).to(device, non_blocking=True)\n",
    "\n",
    "    wav_q  = q_batch[\"wav\"].to(device, non_blocking=True)\n",
    "    wav_g  = g_batch[\"wav\"].to(device, non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_flat = torch.cat([flat_q, flat_g], dim=0)     # [B+G, D]\n",
    "        # align waveforms to same length per batch pair\n",
    "        L = min(wav_q.shape[1], wav_g.shape[1]) if wav_q.ndim == 2 else min(wav_q.shape[-1], wav_g.shape[-1])\n",
    "        all_wav  = torch.cat([wav_q[:, :L],  wav_g[:, :L]], dim=0)  # [B+G, L]\n",
    "        D_full   = compute_audio_distance(all_flat, all_wav, metric=metric)  # [B+G, B+G]\n",
    "\n",
    "        B = flat_q.shape[0]\n",
    "        D_qg = D_full[:B, B:].detach().cpu()             # [B, G_chunk]\n",
    "    # cleanup\n",
    "    del flat_q, flat_g, wav_q, wav_g, all_flat, all_wav, D_full\n",
    "    torch.cuda.empty_cache()\n",
    "    return D_qg\n",
    "\n",
    "def _mixed_distance_matrix(q_loader, g_ds, g_batch):\n",
    "    \"\"\"\n",
    "    Compute MIXED distance matrix per query batch against full gallery in chunks.\n",
    "    Returns a list of (batch_ids, D_mix_cpu) where D_mix_cpu is [B, G] CPU.\n",
    "    \"\"\"\n",
    "    G = len(g_ds)\n",
    "    g_ids = g_ds.ids\n",
    "    results = []\n",
    "\n",
    "    # Pre-split the gallery into chunks of size gallery_chunk_size\n",
    "    g_slices = []\n",
    "    start = 0\n",
    "    while start < G:\n",
    "        end = min(start + gallery_chunk_size, G)\n",
    "        g_slices.append((start, end))\n",
    "        start = end\n",
    "\n",
    "    for batch_idx, q_batch in enumerate(tqdm(q_loader, desc=\"Query batches (MIXED)\", leave=False)):\n",
    "        B = q_batch[\"stft\"].shape[0]\n",
    "        # Collect z-normalized weighted sum across gallery chunks, per metric\n",
    "        mix_acc = None\n",
    "\n",
    "        for metric, w in MIXED_WEIGHTS:\n",
    "            rows = []\n",
    "            for (gs, ge) in g_slices:\n",
    "                g_part = {k: v[gs:ge] for k, v in g_batch.items()}  # slice this gallery chunk\n",
    "                D = _compute_block_distance(metric, q_batch, g_part)  # [B, ge-gs]\n",
    "                rows.append(D)\n",
    "\n",
    "            D_full = torch.cat(rows, dim=1)    # [B, G] CPU\n",
    "            z = _z_per_query(D_full) * float(w)\n",
    "            mix_acc = z if mix_acc is None else (mix_acc + z)\n",
    "\n",
    "        results.append((q_loader.dataset.ids[batch_idx*query_batch_size : batch_idx*query_batch_size + B], mix_acc))\n",
    "    return results\n",
    "\n",
    "def _embedding_topk(q_loader, g_ds, Zg, model, grid_vec):\n",
    "    \"\"\"\n",
    "    EMBEDDING backend: cosine similarity top-k using pose + global grid.\n",
    "    Returns a dict mapping q_id -> list[g_ids].\n",
    "    \"\"\"\n",
    "    refs_for_split = {}\n",
    "    g_ids = g_ds.ids\n",
    "    Zg_n = torch.nn.functional.normalize(Zg, p=2, dim=1)  # [G, D]\n",
    "\n",
    "    for batch_idx, q_batch in enumerate(tq_loader := tqdm(q_loader, desc=\"Query batches (EMBEDDING)\", leave=False)):\n",
    "        bsize = q_batch[\"stft\"].size(0)\n",
    "        start = batch_idx * query_batch_size\n",
    "        end   = start + bsize\n",
    "        batch_ids = q_loader.dataset.ids[start:end]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Gq    = grid_vec.unsqueeze(0).expand(bsize, -1).to(device)\n",
    "            mic_q = q_batch['mic_pose'].to(device)\n",
    "            src_q = q_batch['source_pose'].to(device)\n",
    "            rot_q = q_batch['rot'].to(device)\n",
    "            Zq    = model(Gq, mic_q, src_q, rot_q)               # [B, D]\n",
    "            Zq_n  = torch.nn.functional.normalize(Zq, p=2, dim=1)\n",
    "\n",
    "            sims  = Zq_n @ Zg_n.T                                 # [B, G]\n",
    "            # For each query row, sort sims descending (largest = closest)\n",
    "            top_idxs = torch.argsort(sims, dim=1, descending=True)  # [B, G]\n",
    "\n",
    "        # map each query ID -> list of top-k gallery IDs (w/ self-exclusion)\n",
    "        for i, qid in enumerate(batch_ids):\n",
    "            order = top_idxs[i].tolist()\n",
    "            # self-exclusion if query also exists in reference\n",
    "            # drop the first occurrence where g_id == qid\n",
    "            filtered = [g_ids[j] for j in order if g_ids[j] != qid]\n",
    "            refs_for_split[qid] = filtered[:topk]\n",
    "\n",
    "        del Gq, mic_q, src_q, rot_q, Zq, Zq_n, sims, top_idxs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return refs_for_split\n",
    "\n",
    "# ─── preload gallery ONCE ────────────────────────────────────────────────────────────────\n",
    "g_ds     = FurnishedRoomSTFTDataset(root, split=\"reference\", return_wav=True, mode=\"reference\")\n",
    "g_loader = DataLoader(g_ds, batch_size=len(g_ds), shuffle=False)\n",
    "batch_g  = next(iter(g_loader))   # dict of tensors on CPU\n",
    "G        = len(g_ds)\n",
    "\n",
    "# For EMBEDDING, prep model + gallery embeddings\n",
    "if RETRIEVAL_BACKEND.upper() == \"EMBEDDING\":\n",
    "    from retriever import RIRRetrievalMLP\n",
    "    ckpt  = torch.load(EMBEDDING_CKPT_PATH, map_location=device)\n",
    "    model = RIRRetrievalMLP(**ckpt[\"model_config\"]).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    grid_vec = torch.load(GRID_VEC_PATH, map_location=device).to(device)\n",
    "    with torch.no_grad():\n",
    "        Gg    = grid_vec.unsqueeze(0).expand(G, -1).to(device)\n",
    "        mic_g = batch_g['mic_pose'].to(device)\n",
    "        src_g = batch_g['source_pose'].to(device)\n",
    "        rot_g = batch_g['rot'].to(device)\n",
    "        Zg    = model(Gg, mic_g, src_g, rot_g)  # [G, D] embeddings\n",
    "\n",
    "    # free some refs (keep batch_g for MIXED if user flips)\n",
    "    del Gg, mic_g, src_g, rot_g\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "references = {}\n",
    "\n",
    "# ─── process each split ─────────────────────────────────────────────────────────────────\n",
    "for split_name, split_id in splits.items():\n",
    "    q_ds     = FurnishedRoomSTFTDataset(root, split=split_id, return_wav=True, mode=\"reference\")\n",
    "    q_loader = DataLoader(q_ds, batch_size=query_batch_size, shuffle=False, drop_last=False)\n",
    "    refs_for_split = {}\n",
    "\n",
    "    if RETRIEVAL_BACKEND.upper() == \"EMBEDDING\":\n",
    "        refs_for_split = _embedding_topk(q_loader, g_ds, Zg, model, grid_vec)\n",
    "\n",
    "    else:\n",
    "        # MIXED backend\n",
    "        # compute per-metric distances in gallery chunks; z-norm per query; weight & sum\n",
    "        mix_batches = _mixed_distance_matrix(q_loader, g_ds, batch_g)  # list of (batch_ids, [B, G])\n",
    "\n",
    "        g_ids = g_ds.ids\n",
    "        for (batch_ids, D_mix) in tqdm(mix_batches, desc=f\"Assembling top-{topk} ({split_name})\", leave=False):\n",
    "            # For each query, argsort ascending (smaller distance = closer)\n",
    "            order = torch.argsort(D_mix, dim=1)  # [B, G]\n",
    "\n",
    "            for i, qid in enumerate(batch_ids):\n",
    "                idxs = order[i].tolist()\n",
    "                # self-exclusion: drop first occurrence where gallery id == qid\n",
    "                filtered = [g_ids[j] for j in idxs if g_ids[j] != qid]\n",
    "                refs_for_split[qid] = filtered[:topk]\n",
    "\n",
    "    references[split_name] = refs_for_split\n",
    "\n",
    "# ─── save to JSON (same shape as your example) ──────────────────────────────────────────\n",
    "out_path = \"references.json\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(references, f, indent=2)\n",
    "\n",
    "print(f\"Wrote {out_path} with top-{topk} retrievals \"\n",
    "      f\"(backend={RETRIEVAL_BACKEND}, weights={MIXED_WEIGHTS if RETRIEVAL_BACKEND.upper()=='MIXED' else 'cosine embedding'})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c070a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torchaudio.transforms import GriffinLim\n",
    "from evaluator import compute_audio_distance, compute_edc_db\n",
    "\n",
    "# ---------------- Knobs ----------------\n",
    "EVAL_PATTERN = \"../eval_results/refine_best/renders/eval_*.npy\"   # refine_best, baseline\n",
    "\n",
    "TOPK = 1000                 # number of worst samples to plot; set None or 0 for ALL\n",
    "METRIC = \"C50\"            # one of: \"MSE\",\"MAG\",\"MAG2\",\"SPL\",\"EDC\"\n",
    "PLOT_BOTH = True          # True => 2 rows (mic + source). False => only one row controlled by POSE_MODE\n",
    "POSE_MODE = \"mic\"         # used only if PLOT_BOTH=False\n",
    "\n",
    "SAMPLE_RATE = 48000\n",
    "# ISTFT params that match your STFT (513 bins -> n_fft=1024)\n",
    "n_fft = (513 - 1) * 2\n",
    "win_length = 512\n",
    "hop_length = 256\n",
    "power = 1.0\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def ensure_2d_stft(x):\n",
    "    \"\"\"Squeeze and ensure shape (F,T) as torch.float32.\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x)\n",
    "    x = x.squeeze()\n",
    "    if x.ndim != 2:\n",
    "        raise ValueError(f\"STFT must be 2D (F,T); got shape {tuple(x.shape)}\")\n",
    "    return x.float()\n",
    "\n",
    "def ensure_1d_wav(x):\n",
    "    \"\"\"Squeeze and ensure 1D waveform as torch.float32.\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x)\n",
    "    x = x.squeeze()\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(f\"Waveform must be 1D; got shape {tuple(x.shape)}\")\n",
    "    return x.float()\n",
    "\n",
    "def stft_pair(stft_gt_t: torch.Tensor, stft_pred_t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Stack two STFTs into (2, F, T) after aligning T.\"\"\"\n",
    "    T = min(stft_gt_t.shape[1], stft_pred_t.shape[1])\n",
    "    return torch.stack([stft_gt_t[:, :T], stft_pred_t[:, :T]], dim=0)\n",
    "\n",
    "def reconstruct_pred_waveform_from_logmag(stft_pred_t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Your eval files typically store log-magnitude STFT for pred.\n",
    "    Following your reference: mag = exp(logmag) - 1e-3, then Griffin-Lim.\n",
    "    Returns torch.FloatTensor 1D.\n",
    "    \"\"\"\n",
    "    mag = torch.exp(stft_pred_t) - 1e-3\n",
    "    istft = GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=power)\n",
    "    wav_pred_t = istft(mag)  # (samples,)\n",
    "    return wav_pred_t.float()\n",
    "\n",
    "def compute_score(sample_dict, metric: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute scalar distance between prediction and GT using evaluator.\n",
    "    - STFT-only metrics: MSE, MAG, MAG2\n",
    "    - Waveform/EDC metrics: SPL, EDC (needs wav_pred)\n",
    "    \"\"\"\n",
    "    stft_gt = ensure_2d_stft(sample_dict[\"data\"])\n",
    "    stft_pred = ensure_2d_stft(sample_dict[\"pred_stft\"])\n",
    "\n",
    "    pair_stft = stft_pair(stft_gt, stft_pred)\n",
    "\n",
    "    wavs = None\n",
    "    edc_curves = None\n",
    "    mu = metric.upper()\n",
    "\n",
    "    if mu in (\"SPL\", \"EDC\"):\n",
    "        wav_gt = ensure_1d_wav(sample_dict[\"waveform\"])\n",
    "        wav_pred = reconstruct_pred_waveform_from_logmag(stft_pred)\n",
    "\n",
    "        # length-align\n",
    "        L = min(wav_gt.shape[0], wav_pred.shape[0])\n",
    "        wavs = torch.stack([wav_gt[:L], wav_pred[:L]], dim=0)  # (2, L)\n",
    "\n",
    "        if mu == \"EDC\":\n",
    "            T_edc = 60  # typical target points; adjust if you prefer\n",
    "            edc_gt = compute_edc_db(wavs[0], T_target=T_edc)\n",
    "            edc_pr = compute_edc_db(wavs[1], T_target=T_edc)\n",
    "            edc_curves = torch.stack([edc_gt, edc_pr], dim=0)  # (2, T_edc)\n",
    "\n",
    "    # evaluator expects tensors\n",
    "    D = compute_audio_distance(\n",
    "        stft=pair_stft,\n",
    "        wavs=wavs,\n",
    "        edc_curves=edc_curves,\n",
    "        metric=mu,\n",
    "        fs=SAMPLE_RATE\n",
    "    )\n",
    "    return float(D[0, 1].item())\n",
    "\n",
    "def collect_records(pattern, metric):\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    recs = []\n",
    "    for f in tqdm(files, desc=\"Scoring files\", unit=\"file\"):\n",
    "        d = np.load(f, allow_pickle=True).item()\n",
    "        score = compute_score(d, metric)\n",
    "        mic = np.asarray(d[\"mic_pose\"], dtype=float).reshape(3)\n",
    "        src = np.asarray(d[\"source_pose\"], dtype=float).reshape(3)\n",
    "        recs.append({\"score\": score, \"mic\": mic, \"src\": src, \"file\": f})\n",
    "    return recs\n",
    "\n",
    "def pick_indices_by_topk(scores, topk):\n",
    "    if (not topk) or (topk >= len(scores)):\n",
    "        return np.arange(len(scores))\n",
    "    # worst = largest distance\n",
    "    return np.argsort(scores)[-topk:]\n",
    "\n",
    "def _scatter_planes(axs_row, pts3, scores, title_prefix, vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    axs_row: 3 axes (XY, XZ, YZ)\n",
    "    pts3: (N, 3)\n",
    "    scores: (N,)\n",
    "    \"\"\"\n",
    "    ax_xy, ax_xz, ax_yz = axs_row\n",
    "    sc1 = ax_xy.scatter(pts3[:,0], pts3[:,1], c=scores, vmin=vmin, vmax=vmax)\n",
    "    ax_xy.set_title(f\"{title_prefix} XY\"); ax_xy.set_xlabel(\"X\"); ax_xy.set_ylabel(\"Y\")\n",
    "\n",
    "    sc2 = ax_xz.scatter(pts3[:,0], pts3[:,2], c=scores, vmin=vmin, vmax=vmax)\n",
    "    ax_xz.set_title(f\"{title_prefix} XZ\"); ax_xz.set_xlabel(\"X\"); ax_xz.set_ylabel(\"Z\")\n",
    "\n",
    "    sc3 = ax_yz.scatter(pts3[:,1], pts3[:,2], c=scores, vmin=vmin, vmax=vmax)\n",
    "    ax_yz.set_title(f\"{title_prefix} YZ\"); ax_yz.set_xlabel(\"Y\"); ax_yz.set_ylabel(\"Z\")\n",
    "    return sc1  # colorbar handle\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "records = collect_records(EVAL_PATTERN, METRIC)\n",
    "scores = np.array([r[\"score\"] for r in records], dtype=float)\n",
    "\n",
    "idx = pick_indices_by_topk(scores, TOPK)\n",
    "scores_sel = scores[idx]\n",
    "mic_pts = np.stack([records[i][\"mic\"] for i in idx], axis=0)\n",
    "src_pts = np.stack([records[i][\"src\"] for i in idx], axis=0)\n",
    "\n",
    "# Shared color scale so rows are comparable\n",
    "vmin, vmax = float(scores_sel.min()), float(scores_sel.max())\n",
    "\n",
    "if PLOT_BOTH:\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 10), constrained_layout=True)\n",
    "    sc_mic = _scatter_planes(axs[0], mic_pts, scores_sel, f\"MIC (Top-{TOPK or 'All'})\", vmin=vmin, vmax=vmax)\n",
    "    sc_src = _scatter_planes(axs[1], src_pts, scores_sel, f\"SRC (Top-{TOPK or 'All'})\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "    cbar = fig.colorbar(sc_mic, ax=axs, orientation=\"horizontal\", fraction=0.04, pad=0.08)\n",
    "    cbar.set_label(f\"{METRIC} distance (higher = worse)\")\n",
    "    fig.suptitle(f\"Quality by Pose — metric={METRIC}, top-k={TOPK or 'All'}\", y=0.995)\n",
    "    plt.show()\n",
    "else:\n",
    "    which = \"mic\" if POSE_MODE.lower().startswith(\"m\") else \"src\"\n",
    "    pts = mic_pts if which == \"mic\" else src_pts\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 4), constrained_layout=True)\n",
    "    sc = _scatter_planes(axs, pts, scores_sel, f\"{which.upper()} (Top-{TOPK or 'All'})\", vmin=vmin, vmax=vmax)\n",
    "    cbar = fig.colorbar(sc, ax=axs, orientation=\"horizontal\", fraction=0.04, pad=0.08)\n",
    "    cbar.set_label(f\"{METRIC} distance (higher = worse)\")\n",
    "    fig.suptitle(f\"{which.upper()} poses — metric={METRIC}, top-k={TOPK or 'All'}\", y=0.995)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paper-style table: NeRAF (Baseline) vs Ours vs Gain (EDT, T60, C50, ΔSTFT) ===\n",
    "# - 3 rows × 4 cols, plus a thin colorbar column aligned per row\n",
    "# - Row colorbars: Baseline (viridis swatch), Ours (viridis swatch), Gain (blue→grey→red swatch)\n",
    "# - Gain: per-column normalization/band so no “all-grey” surprises\n",
    "# - No per-axis ticks; X/Y shown once (outer labels)\n",
    "# - Plots X vs Z but labels vertical axis as “Y”\n",
    "# - GPU-batched GLIM; uses your compute_audio_distance\n",
    "\n",
    "import os, glob, numpy as np, torch, matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "from tqdm.auto import tqdm\n",
    "from torchaudio.transforms import GriffinLim\n",
    "\n",
    "from evaluator import compute_audio_distance\n",
    "\n",
    "# ------------------- Knobs -------------------\n",
    "BASELINE_PATTERN = \"../eval_results/baseline/renders/eval_*.npy\"\n",
    "REFINE_PATTERN   = \"../eval_results/refine_best/renders/eval_*.npy\"\n",
    "\n",
    "MAX_FILES   = None          # e.g., 200 for quick tests; None/0 => all\n",
    "POSE_MODE   = \"mic\"         # \"mic\" or \"src\" (we plot X vs Z but call it Y)\n",
    "COLOR_MODE  = \"spectrum\"    # \"spectrum\" or \"discrete\" for Gain\n",
    "NEUTRAL_BAND = \"auto\"       # \"auto\" => 5% of each column's max |Δ|; or float\n",
    "\n",
    "SAMPLE_RATE = 48000\n",
    "n_fft       = (513 - 1) * 2\n",
    "win_length  = 512\n",
    "hop_length  = 256\n",
    "power       = 1.0\n",
    "BATCH_GLIM  = 128\n",
    "\n",
    "COL_METRICS = [\"EDT\", \"T60\", \"C50\", \"SPL\"]   # SPL will be labeled as ΔSTFT\n",
    "COL_TITLES  = {\"EDT\": \"EDT\", \"T60\": \"T60\", \"C50\": \"C50\", \"SPL\": \"ΔSTFT\"}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"DejaVu Serif\",\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlesize\": 12,\n",
    "})\n",
    "\n",
    "COLORS_3 = ListedColormap([\"#e41a1c\", \"#9e9e9e\", \"#377eb8\"])  # red/grey/blue\n",
    "SPECTRUM_COLORS = [\"#e41a1c\", \"#9e9e9e\", \"#377eb8\"]\n",
    "\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _load_aligned(bpat, rpat, max_files=None):\n",
    "    bfiles = sorted(glob.glob(bpat))\n",
    "    rfiles = sorted(glob.glob(rpat))\n",
    "    bmap = {os.path.basename(f): f for f in bfiles}\n",
    "    rmap = {os.path.basename(f): f for f in rfiles}\n",
    "    keys = sorted(set(bmap) & set(rmap))\n",
    "    if not keys:\n",
    "        raise RuntimeError(\"No overlapping eval_*.npy files.\")\n",
    "    if max_files and max_files > 0:\n",
    "        keys = keys[:max_files]\n",
    "    return keys, bmap, rmap\n",
    "\n",
    "def _ensure_2d_stft(x):\n",
    "    t = torch.from_numpy(x).float().squeeze()\n",
    "    if t.ndim != 2: raise ValueError(f\"STFT must be 2D (F,T); got {tuple(t.shape)}\")\n",
    "    return t\n",
    "\n",
    "def _ensure_1d_wav(x):\n",
    "    t = torch.from_numpy(x).float().squeeze()\n",
    "    if t.ndim != 1: raise ValueError(f\"Waveform must be 1D; got {tuple(t.shape)}\")\n",
    "    return t\n",
    "\n",
    "def _batch_griffin_lim(mag_batch, istft):\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp), torch.no_grad():\n",
    "        wav = istft(mag_batch)\n",
    "    return wav\n",
    "\n",
    "def _batched_logmag_to_wav(stft_list, istft, batch=BATCH_GLIM):\n",
    "    out = [None]*len(stft_list)\n",
    "    for s in tqdm(range(0, len(stft_list), batch), desc=\"GLIM batches\", unit=\"batch\"):\n",
    "        e = min(s+batch, len(stft_list))\n",
    "        Ts = [p.shape[1] for p in stft_list[s:e]]\n",
    "        Tm = min(Ts)\n",
    "        mags = torch.stack([torch.exp(p[:, :Tm]) - 1e-3 for p in stft_list[s:e]], dim=0).to(device, non_blocking=True)\n",
    "        wav_b = _batch_griffin_lim(mags, istft)\n",
    "        for i, w in enumerate(wav_b.detach().cpu()):\n",
    "            out[s+i] = w.contiguous()\n",
    "        del mags, wav_b\n",
    "        torch.cuda.empty_cache()\n",
    "    return out\n",
    "\n",
    "def _pair_score(metric, stft_gt, wav_gt, stft_pred, wav_pred):\n",
    "    mu = metric.upper()\n",
    "    L = min(len(wav_gt), len(wav_pred))\n",
    "    wavs = torch.stack([wav_gt[:L], wav_pred[:L]], dim=0)\n",
    "    T = min(stft_gt.shape[1], stft_pred.shape[1])\n",
    "    stft_pair = torch.stack([stft_gt[:, :T], stft_pred[:, :T]], dim=0)\n",
    "    with torch.no_grad():\n",
    "        D = compute_audio_distance(stft_pair, wavs=wavs, metric=mu, fs=SAMPLE_RATE)\n",
    "    return float(D[0,1].item())\n",
    "\n",
    "def _auto_band(delta_vals):\n",
    "    m = float(np.nanmax(np.abs(delta_vals))) if len(delta_vals) else 1.0\n",
    "    return max(1e-9, 0.05 * m) if NEUTRAL_BAND == \"auto\" else float(NEUTRAL_BAND)\n",
    "\n",
    "def _norm_discrete(delta_vals):\n",
    "    band = _auto_band(delta_vals)\n",
    "    bounds = [-np.inf, -band, band, np.inf]\n",
    "    norm = BoundaryNorm(bounds, COLORS_3.N)\n",
    "    return COLORS_3, norm, band\n",
    "\n",
    "def _norm_spectrum(delta_vals):\n",
    "    m = float(np.nanmax(np.abs(delta_vals))) if len(delta_vals) else 1.0\n",
    "    band = _auto_band(delta_vals)\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"red_grey_blue\", [\n",
    "        (0.0,  SPECTRUM_COLORS[0]),\n",
    "        (0.5,  SPECTRUM_COLORS[1]),\n",
    "        (1.0,  SPECTRUM_COLORS[2]),\n",
    "    ])\n",
    "    class NeutralTwoSlopeNorm(mcolors.Normalize):\n",
    "        def __init__(self, vmin, vcenter, vmax, band):\n",
    "            super().__init__(vmin, vmax); self.vcenter = vcenter; self.band = band\n",
    "        def __call__(self, value, clip=None):\n",
    "            x = np.ma.asarray(value); res = np.empty_like(x, dtype=float); res.fill(np.nan)\n",
    "            neg = (x <= -self.band); res[neg] = 0.5 * (x[neg] - self.vmin) / (-self.band - self.vmin)\n",
    "            neut = (np.abs(x) < self.band); res[neut] = 0.5\n",
    "            pos = (x >= self.band); res[pos] = 0.5 + 0.5*(x[pos] - self.band)/(self.vmax - self.band)\n",
    "            if clip: res = np.clip(res, 0.0, 1.0)\n",
    "            return res\n",
    "    norm = NeutralTwoSlopeNorm(vmin=-m, vcenter=0.0, vmax=m, band=band)\n",
    "    return cmap, norm, band\n",
    "\n",
    "def _layered_scatter(ax, x, y, vals, cmap, norm, band, s_grey=9, s_color=11):\n",
    "    vals = np.asarray(vals)\n",
    "    mg = np.abs(vals) < band\n",
    "    mr = vals >= band\n",
    "    mb = vals <= -band\n",
    "    if mg.any():\n",
    "        ax.scatter(x[mg], y[mg], c=vals[mg], cmap=cmap, norm=norm,\n",
    "                   s=s_grey, linewidths=0, alpha=0.9, zorder=1)\n",
    "    if mr.any():\n",
    "        ax.scatter(x[mr], y[mr], c=vals[mr], cmap=cmap, norm=norm,\n",
    "                   s=s_color, edgecolors='none', alpha=1.0, zorder=3)\n",
    "    if mb.any():\n",
    "        ax.scatter(x[mb], y[mb], c=vals[mb], cmap=cmap, norm=norm,\n",
    "                   s=s_color, edgecolors='none', alpha=1.0, zorder=3)\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "def main():\n",
    "    keys, bmap, rmap = _load_aligned(BASELINE_PATTERN, REFINE_PATTERN, max_files=MAX_FILES)\n",
    "\n",
    "    stft_gt_list, wav_gt_list = [], []\n",
    "    stft_b_list,  stft_r_list = [], []\n",
    "    poses = np.zeros((len(keys), 3), dtype=np.float32)\n",
    "    for i, k in enumerate(tqdm(keys, desc=\"Loading eval dicts\", unit=\"file\")):\n",
    "        d_b = np.load(bmap[k], allow_pickle=True).item()\n",
    "        d_r = np.load(rmap[k], allow_pickle=True).item()\n",
    "        stft_gt_list.append(_ensure_2d_stft(d_b[\"data\"]))\n",
    "        wav_gt_list.append(_ensure_1d_wav(d_b[\"waveform\"]))\n",
    "        stft_b_list.append(_ensure_2d_stft(d_b[\"pred_stft\"]))\n",
    "        stft_r_list.append(_ensure_2d_stft(d_r[\"pred_stft\"]))\n",
    "        poses[i] = np.asarray(d_b[\"source_pose\" if POSE_MODE.lower().startswith(\"s\") else \"mic_pose\"],\n",
    "                              dtype=float).reshape(3)\n",
    "\n",
    "    istft = GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=power).to(device)\n",
    "    wav_b_list = _batched_logmag_to_wav(stft_b_list, istft, batch=BATCH_GLIM)\n",
    "    wav_r_list = _batched_logmag_to_wav(stft_r_list, istft, batch=BATCH_GLIM)\n",
    "\n",
    "    base = {m: np.zeros(len(keys), dtype=np.float32) for m in COL_METRICS}\n",
    "    ours = {m: np.zeros(len(keys), dtype=np.float32) for m in COL_METRICS}\n",
    "    for m in COL_METRICS:\n",
    "        for i in tqdm(range(len(keys)), desc=f\"Scoring {m}\", unit=\"sample\", leave=False):\n",
    "            base[m][i] = _pair_score(m, stft_gt_list[i], wav_gt_list[i], stft_b_list[i], wav_b_list[i])\n",
    "            ours[m][i] = _pair_score(m, stft_gt_list[i], wav_gt_list[i], stft_r_list[i], wav_r_list[i])\n",
    "\n",
    "    # === Figure ===\n",
    "    # 4 data columns + 1 thin colorbar column; thin CB aligned per row\n",
    "    fig = plt.figure(figsize=(12, 7.6))\n",
    "    gs = gridspec.GridSpec(\n",
    "        nrows=3, ncols=5, figure=fig,\n",
    "        left=0.07, right=0.96, top=0.92, bottom=0.10,\n",
    "        wspace=0.18, hspace=0.38,\n",
    "        width_ratios=[1, 1, 1, 1, 0.035]\n",
    "    )\n",
    "\n",
    "    # axes grid (3x4) + row colorbar axes in last column\n",
    "    axes = np.array([[fig.add_subplot(gs[r, c]) for c in range(4)] for r in range(3)])\n",
    "    cbar_axes = [fig.add_subplot(gs[r, 4]) for r in range(3)]  # row-aligned, thin\n",
    "\n",
    "    # column headers once\n",
    "    for c, m in enumerate(COL_METRICS):\n",
    "        axes[0, c].set_title(COL_TITLES.get(m, m), pad=6)\n",
    "\n",
    "    # row labels as ylabels (keeps left margin tight)\n",
    "    axes[0, 0].set_ylabel(\"Baseline (NeRAF)\", rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "    axes[1, 0].set_ylabel(\"Ours\",             rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "    axes[2, 0].set_ylabel(\"Gain\",             rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "\n",
    "    # outer labels only\n",
    "    fig.supxlabel(\"X\", y=0.06, fontsize=12)\n",
    "    fig.supylabel(\"Y\", x=0.055, fontsize=12)\n",
    "\n",
    "    X = poses[:, 0]\n",
    "    Y = poses[:, 2]  # plotted from Z, labeled as Y\n",
    "\n",
    "    viridis_cmap = plt.cm.get_cmap(\"viridis\")\n",
    "\n",
    "    # ----- Baseline & Ours rows -----\n",
    "    # per-column vmin/vmax (units differ); qualitative row colorbars (no ticks)\n",
    "    for c, m in enumerate(COL_METRICS):\n",
    "        vmin = float(min(base[m].min(), ours[m].min()))\n",
    "        vmax = float(max(base[m].max(), ours[m].max()))\n",
    "        axes[0, c].scatter(X, Y, c=base[m], cmap=viridis_cmap, vmin=vmin, vmax=vmax, s=8, linewidths=0)\n",
    "        axes[1, c].scatter(X, Y, c=ours[m], cmap=viridis_cmap, vmin=vmin, vmax=vmax, s=8, linewidths=0)\n",
    "\n",
    "    # Baseline row colorbar (thin, aligned)\n",
    "    sm_row = plt.cm.ScalarMappable(norm=mcolors.Normalize(0, 1), cmap=viridis_cmap)\n",
    "    cb1 = fig.colorbar(sm_row, cax=cbar_axes[0])\n",
    "    # cb1.set_label(\"lower  →  higher\", labelpad=4)\n",
    "\n",
    "    # Ours row colorbar (thin, aligned)\n",
    "    cb2 = fig.colorbar(sm_row, cax=cbar_axes[1])\n",
    "    cb2.set_label(\"lower  is better\", labelpad=4)\n",
    "\n",
    "    # ----- Gain row -----\n",
    "    # Per-column normalization/band so one metric with large Δ doesn't neutralize others.\n",
    "    # We still show a single qualitative swatch for the row (no ticks).\n",
    "    # Build a “representative” swatch mapper from the first column’s band/range.\n",
    "    if COLOR_MODE.lower().startswith(\"disc\"):\n",
    "        get_norm = _norm_discrete\n",
    "    else:\n",
    "        get_norm = _norm_spectrum\n",
    "\n",
    "    first_sm = None\n",
    "    for c, m in enumerate(COL_METRICS):\n",
    "        delta = ours[m] - base[m]\n",
    "        cmap_c, norm_c, band_c = get_norm(delta)\n",
    "        _layered_scatter(axes[2, c], X, Y, delta, cmap_c, norm_c, band_c, s_grey=9, s_color=11)\n",
    "        if first_sm is None:\n",
    "            first_sm = plt.cm.ScalarMappable(norm=norm_c, cmap=cmap_c)\n",
    "\n",
    "    cb3 = fig.colorbar(first_sm, cax=cbar_axes[2])\n",
    "    # cb3.set_label(\"Δ (negative = Retrieval Augmented better)\", labelpad=4)\n",
    "\n",
    "    # Clean panels: no ticks, subtle frames\n",
    "    for r in range(3):\n",
    "        for c in range(4):\n",
    "            ax = axes[r, c]\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            for sp in ax.spines.values():\n",
    "                sp.set_linewidth(0.6); sp.set_alpha(0.6)\n",
    "\n",
    "    fig.suptitle(f\"Columns: EDT, T60, C50, ΔSTFT   |   Pose = {POSE_MODE.upper()}\",\n",
    "                 y=0.975, fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paper-style table: Baseline vs Ours (two designs) vs two Gain rows ===\n",
    "# Rows (top→bottom):\n",
    "#   1) Baseline (NeRAF)\n",
    "#   2) Ours (Design 1: feature fusion)        <-- REFINE_PATTERN\n",
    "#   3) Ours (Design 2: output modification)   <-- STFT_PATTERN\n",
    "#   4) Gain (Design 1 - Baseline)\n",
    "#   5) Gain (Design 2 - Baseline)\n",
    "#\n",
    "# Notes:\n",
    "# - 4 metric columns (EDT, T60, C50, ΔSTFT) + 1 thin colorbar column per row\n",
    "# - Per-column scaling for each data row; gain rows use blue↔grey↔red with neutral band\n",
    "# - No per-axis ticks; outer X/Y only; X vs Z plotted, Y label says “Y” (because traditions)\n",
    "# - GPU-batched GLIM; uses compute_audio_distance\n",
    "\n",
    "import os, glob, numpy as np, torch, matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib import gridspec\n",
    "from tqdm.auto import tqdm\n",
    "from torchaudio.transforms import GriffinLim\n",
    "\n",
    "from evaluator import compute_audio_distance\n",
    "\n",
    "# ------------------- Knobs -------------------\n",
    "BASELINE_PATTERN = \"../eval_results/baseline/renders/eval_*.npy\"\n",
    "REFINE_PATTERN   = \"../eval_results/refine_best/renders/eval_*.npy\"  # Design 1 (feature-fusion)\n",
    "STFT_PATTERN     = \"../eval_results/stft_no_edc/renders/eval_*.npy\"    # Design 2 (output-mod)\n",
    "\n",
    "MAX_FILES   = None          # e.g., 200 for quick tests; None/0 => all\n",
    "POSE_MODE   = \"mic\"         # \"mic\" or \"src\" (we plot X vs Z but call it Y)\n",
    "COLOR_MODE  = \"spectrum\"    # \"spectrum\" or \"discrete\" for Gain rows\n",
    "NEUTRAL_BAND = \"auto\"       # \"auto\" => 5% of each column's max |Δ|; or float\n",
    "\n",
    "SAMPLE_RATE = 48000\n",
    "n_fft       = (513 - 1) * 2\n",
    "win_length  = 512\n",
    "hop_length  = 256\n",
    "power       = 1.0\n",
    "BATCH_GLIM  = 128\n",
    "\n",
    "COL_METRICS = [\"EDT\", \"T60\", \"C50\", \"SPL\"]   # SPL will be labeled as ΔSTFT\n",
    "COL_TITLES  = {\"EDT\": \"EDT\", \"T60\": \"T60\", \"C50\": \"C50\", \"SPL\": \"ΔSTFT\"}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"DejaVu Serif\",\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlesize\": 12,\n",
    "})\n",
    "\n",
    "COLORS_3 = ListedColormap([\"#e41a1c\", \"#9e9e9e\", \"#377eb8\"])  # red/grey/blue\n",
    "SPECTRUM_COLORS = [\"#e41a1c\", \"#9e9e9e\", \"#377eb8\"]\n",
    "\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _load_aligned_three(bpat, r1pat, r2pat, max_files=None):\n",
    "    bfiles = sorted(glob.glob(bpat))\n",
    "    r1files = sorted(glob.glob(r1pat))\n",
    "    r2files = sorted(glob.glob(r2pat))\n",
    "    bmap  = {os.path.basename(f): f for f in bfiles}\n",
    "    r1map = {os.path.basename(f): f for f in r1files}\n",
    "    r2map = {os.path.basename(f): f for f in r2files}\n",
    "    keys = sorted(set(bmap) & set(r1map) & set(r2map))\n",
    "    if not keys:\n",
    "        raise RuntimeError(\"No overlapping eval_*.npy files across all three patterns.\")\n",
    "    if max_files and max_files > 0:\n",
    "        keys = keys[:max_files]\n",
    "    return keys, bmap, r1map, r2map\n",
    "\n",
    "def _ensure_2d_stft(x):\n",
    "    t = torch.from_numpy(x).float().squeeze()\n",
    "    if t.ndim != 2: raise ValueError(f\"STFT must be 2D (F,T); got {tuple(t.shape)}\")\n",
    "    return t\n",
    "\n",
    "def _ensure_1d_wav(x):\n",
    "    t = torch.from_numpy(x).float().squeeze()\n",
    "    if t.ndim != 1: raise ValueError(f\"Waveform must be 1D; got {tuple(t.shape)}\")\n",
    "    return t\n",
    "\n",
    "def _batch_griffin_lim(mag_batch, istft):\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp), torch.no_grad():\n",
    "        wav = istft(mag_batch)\n",
    "    return wav\n",
    "\n",
    "def _batched_logmag_to_wav(stft_list, istft, batch=BATCH_GLIM):\n",
    "    out = [None]*len(stft_list)\n",
    "    for s in tqdm(range(0, len(stft_list), batch), desc=\"GLIM batches\", unit=\"batch\"):\n",
    "        e = min(s+batch, len(stft_list))\n",
    "        Ts = [p.shape[1] for p in stft_list[s:e]]\n",
    "        Tm = min(Ts)\n",
    "        mags = torch.stack([torch.exp(p[:, :Tm]) - 1e-3 for p in stft_list[s:e]], dim=0).to(device, non_blocking=True)\n",
    "        wav_b = _batch_griffin_lim(mags, istft)\n",
    "        for i, w in enumerate(wav_b.detach().cpu()):\n",
    "            out[s+i] = w.contiguous()\n",
    "        del mags, wav_b\n",
    "        torch.cuda.empty_cache()\n",
    "    return out\n",
    "\n",
    "def _pair_score(metric, stft_gt, wav_gt, stft_pred, wav_pred):\n",
    "    mu = metric.upper()\n",
    "    L = min(len(wav_gt), len(wav_pred))\n",
    "    wavs = torch.stack([wav_gt[:L], wav_pred[:L]], dim=0)\n",
    "    T = min(stft_gt.shape[1], stft_pred.shape[1])\n",
    "    stft_pair = torch.stack([stft_gt[:, :T], stft_pred[:, :T]], dim=0)\n",
    "    with torch.no_grad():\n",
    "        D = compute_audio_distance(stft_pair, wavs=wavs, metric=mu, fs=SAMPLE_RATE)\n",
    "    return float(D[0,1].item())\n",
    "\n",
    "def _auto_band(delta_vals):\n",
    "    m = float(np.nanmax(np.abs(delta_vals))) if len(delta_vals) else 1.0\n",
    "    return max(1e-9, 0.05 * m) if NEUTRAL_BAND == \"auto\" else float(NEUTRAL_BAND)\n",
    "\n",
    "def _norm_discrete(delta_vals):\n",
    "    band = _auto_band(delta_vals)\n",
    "    bounds = [-np.inf, -band, band, np.inf]\n",
    "    norm = BoundaryNorm(bounds, COLORS_3.N)\n",
    "    return COLORS_3, norm, band\n",
    "\n",
    "def _norm_spectrum(delta_vals):\n",
    "    m = float(np.nanmax(np.abs(delta_vals))) if len(delta_vals) else 1.0\n",
    "    band = _auto_band(delta_vals)\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"red_grey_blue\", [\n",
    "        (0.0,  SPECTRUM_COLORS[0]),\n",
    "        (0.5,  SPECTRUM_COLORS[1]),\n",
    "        (1.0,  SPECTRUM_COLORS[2]),\n",
    "    ])\n",
    "    class NeutralTwoSlopeNorm(mcolors.Normalize):\n",
    "        def __init__(self, vmin, vcenter, vmax, band):\n",
    "            super().__init__(vmin, vmax); self.vcenter = vcenter; self.band = band\n",
    "        def __call__(self, value, clip=None):\n",
    "            x = np.ma.asarray(value); res = np.empty_like(x, dtype=float); res.fill(np.nan)\n",
    "            neg = (x <= -self.band); res[neg] = 0.5 * (x[neg] - self.vmin) / (-self.band - self.vmin)\n",
    "            neut = (np.abs(x) < self.band); res[neut] = 0.5\n",
    "            pos = (x >= self.band); res[pos] = 0.5 + 0.5*(x[pos] - self.band)/(self.vmax - self.band)\n",
    "            if clip: res = np.clip(res, 0.0, 1.0)\n",
    "            return res\n",
    "    norm = NeutralTwoSlopeNorm(vmin=-m, vcenter=0.0, vmax=m, band=band)\n",
    "    return cmap, norm, band\n",
    "\n",
    "def _layered_scatter(ax, x, y, vals, cmap, norm, band, s_grey=9, s_color=11):\n",
    "    vals = np.asarray(vals)\n",
    "    mg = np.abs(vals) < band\n",
    "    mr = vals >= band\n",
    "    mb = vals <= -band\n",
    "    if mg.any():\n",
    "        ax.scatter(x[mg], y[mg], c=vals[mg], cmap=cmap, norm=norm,\n",
    "                   s=s_grey, linewidths=0, alpha=0.9, zorder=1)\n",
    "    if mr.any():\n",
    "        ax.scatter(x[mr], y[mr], c=vals[mr], cmap=cmap, norm=norm,\n",
    "                   s=s_color, edgecolors='none', alpha=1.0, zorder=3)\n",
    "    if mb.any():\n",
    "        ax.scatter(x[mb], y[mb], c=vals[mb], cmap=cmap, norm=norm,\n",
    "                   s=s_color, edgecolors='none', alpha=1.0, zorder=3)\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "def main():\n",
    "    keys, bmap, r1map, r2map = _load_aligned_three(BASELINE_PATTERN, REFINE_PATTERN, STFT_PATTERN, max_files=MAX_FILES)\n",
    "\n",
    "    stft_gt_list, wav_gt_list = [], []\n",
    "    stft_b_list,  stft_r1_list, stft_r2_list = [], [], []\n",
    "    poses = np.zeros((len(keys), 3), dtype=np.float32)\n",
    "\n",
    "    for i, k in enumerate(tqdm(keys, desc=\"Loading eval dicts\", unit=\"file\")):\n",
    "        d_b  = np.load(bmap[k],  allow_pickle=True).item()\n",
    "        d_r1 = np.load(r1map[k], allow_pickle=True).item()\n",
    "        d_r2 = np.load(r2map[k], allow_pickle=True).item()\n",
    "\n",
    "        stft_gt_list.append(_ensure_2d_stft(d_b[\"data\"]))\n",
    "        wav_gt_list.append(_ensure_1d_wav(d_b[\"waveform\"]))\n",
    "        stft_b_list.append(_ensure_2d_stft(d_b[\"pred_stft\"]))\n",
    "        stft_r1_list.append(_ensure_2d_stft(d_r1[\"pred_stft\"]))  # Design 1\n",
    "        stft_r2_list.append(_ensure_2d_stft(d_r2[\"pred_stft\"]))  # Design 2\n",
    "\n",
    "        poses[i] = np.asarray(d_b[\"source_pose\" if POSE_MODE.lower().startswith(\"s\") else \"mic_pose\"],\n",
    "                              dtype=float).reshape(3)\n",
    "\n",
    "    istft = GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=power).to(device)\n",
    "\n",
    "    wav_b_list  = _batched_logmag_to_wav(stft_b_list,  istft, batch=BATCH_GLIM)\n",
    "    wav_r1_list = _batched_logmag_to_wav(stft_r1_list, istft, batch=BATCH_GLIM)\n",
    "    wav_r2_list = _batched_logmag_to_wav(stft_r2_list, istft, batch=BATCH_GLIM)\n",
    "\n",
    "    base = {m: np.zeros(len(keys), dtype=np.float32) for m in COL_METRICS}\n",
    "    ours1 = {m: np.zeros(len(keys), dtype=np.float32) for m in COL_METRICS}\n",
    "    ours2 = {m: np.zeros(len(keys), dtype=np.float32) for m in COL_METRICS}\n",
    "\n",
    "    for m in COL_METRICS:\n",
    "        for i in tqdm(range(len(keys)), desc=f\"Scoring {m}\", unit=\"sample\", leave=False):\n",
    "            base[m][i]  = _pair_score(m, stft_gt_list[i], wav_gt_list[i], stft_b_list[i],  wav_b_list[i])\n",
    "            ours1[m][i] = _pair_score(m, stft_gt_list[i], wav_gt_list[i], stft_r1_list[i], wav_r1_list[i])\n",
    "            ours2[m][i] = _pair_score(m, stft_gt_list[i], wav_gt_list[i], stft_r2_list[i], wav_r2_list[i])\n",
    "\n",
    "    # === Figure ===\n",
    "    # 4 data columns + 1 thin colorbar column; thin CB aligned per row\n",
    "    NROWS = 5\n",
    "    fig = plt.figure(figsize=(12, 10.4))\n",
    "    gs = gridspec.GridSpec(\n",
    "        nrows=NROWS, ncols=5, figure=fig,\n",
    "        left=0.07, right=0.96, top=0.94, bottom=0.08,\n",
    "        wspace=0.18, hspace=0.35,\n",
    "        width_ratios=[1, 1, 1, 1, 0.035]\n",
    "    )\n",
    "\n",
    "    # axes grid + row colorbar axes in last column\n",
    "    axes = np.array([[fig.add_subplot(gs[r, c]) for c in range(4)] for r in range(NROWS)])\n",
    "    cbar_axes = [fig.add_subplot(gs[r, 4]) for r in range(NROWS)]  # row-aligned, thin\n",
    "\n",
    "    # column headers once\n",
    "    for c, m in enumerate(COL_METRICS):\n",
    "        axes[0, c].set_title(COL_TITLES.get(m, m), pad=6)\n",
    "\n",
    "    # row labels (bold)\n",
    "    axes[0, 0].set_ylabel(\"Baseline (NeRAF)\", rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "    axes[1, 0].set_ylabel(\"Ours (Design 1:\\nfeature fusion)\", rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "    axes[2, 0].set_ylabel(\"Ours (Design 2:\\noutput modification)\", rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "    axes[3, 0].set_ylabel(\"Gain (D1 − Base)\", rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "    axes[4, 0].set_ylabel(\"Gain (D2 − Base)\", rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "\n",
    "    # outer labels only\n",
    "    fig.supxlabel(\"X\", y=0.05, fontsize=12)\n",
    "    fig.supylabel(\"Y\", x=0.055, fontsize=12)\n",
    "\n",
    "    X = poses[:, 0]\n",
    "    Y = poses[:, 2]  # plotted from Z, labeled as Y\n",
    "\n",
    "    viridis_cmap = plt.cm.get_cmap(\"viridis\")\n",
    "\n",
    "    # ----- Baseline & Ours rows -----\n",
    "    # Per-column vmin/vmax (units differ); qualitative row colorbars (no ticks)\n",
    "    for c, m in enumerate(COL_METRICS):\n",
    "        vmin = float(min(base[m].min(), ours1[m].min(), ours2[m].min()))\n",
    "        vmax = float(max(base[m].max(), ours1[m].max(), ours2[m].max()))\n",
    "        axes[0, c].scatter(X, Y, c=base[m],  cmap=viridis_cmap, vmin=vmin, vmax=vmax, s=8, linewidths=0)\n",
    "        axes[1, c].scatter(X, Y, c=ours1[m], cmap=viridis_cmap, vmin=vmin, vmax=vmax, s=8, linewidths=0)\n",
    "        axes[2, c].scatter(X, Y, c=ours2[m], cmap=viridis_cmap, vmin=vmin, vmax=vmax, s=8, linewidths=0)\n",
    "\n",
    "    sm_row = plt.cm.ScalarMappable(norm=mcolors.Normalize(0, 1), cmap=viridis_cmap)\n",
    "    fig.colorbar(sm_row, cax=cbar_axes[0])\n",
    "    fig.colorbar(sm_row, cax=cbar_axes[1])\n",
    "    cb_ours2 = fig.colorbar(sm_row, cax=cbar_axes[2])\n",
    "    cb_ours2.set_label(\"lower is better\", labelpad=4)\n",
    "\n",
    "    # ----- Gain rows -----\n",
    "    if COLOR_MODE.lower().startswith(\"disc\"):\n",
    "        get_norm = _norm_discrete\n",
    "    else:\n",
    "        get_norm = _norm_spectrum\n",
    "\n",
    "    # Row 4: Gain D1 (ours1 − base)\n",
    "    first_sm_d1 = None\n",
    "    for c, m in enumerate(COL_METRICS):\n",
    "        delta1 = ours1[m] - base[m]\n",
    "        cmap_c, norm_c, band_c = get_norm(delta1)\n",
    "        _layered_scatter(axes[3, c], X, Y, delta1, cmap_c, norm_c, band_c, s_grey=9, s_color=11)\n",
    "        if first_sm_d1 is None:\n",
    "            first_sm_d1 = plt.cm.ScalarMappable(norm=norm_c, cmap=cmap_c)\n",
    "    fig.colorbar(first_sm_d1, cax=cbar_axes[3])\n",
    "\n",
    "    # Row 5: Gain D2 (ours2 − base)\n",
    "    first_sm_d2 = None\n",
    "    for c, m in enumerate(COL_METRICS):\n",
    "        delta2 = ours2[m] - base[m]\n",
    "        cmap_c, norm_c, band_c = get_norm(delta2)\n",
    "        _layered_scatter(axes[4, c], X, Y, delta2, cmap_c, norm_c, band_c, s_grey=9, s_color=11)\n",
    "        if first_sm_d2 is None:\n",
    "            first_sm_d2 = plt.cm.ScalarMappable(norm=norm_c, cmap=cmap_c)\n",
    "    fig.colorbar(first_sm_d2, cax=cbar_axes[4])\n",
    "\n",
    "    # Clean panels: no ticks, subtle frames\n",
    "    for r in range(NROWS):\n",
    "        for c in range(4):\n",
    "            ax = axes[r, c]\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            for sp in ax.spines.values():\n",
    "                sp.set_linewidth(0.6); sp.set_alpha(0.6)\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"Columns: EDT, T60, C50, ΔSTFT   |   Pose = \"\n",
    "        f\"{POSE_MODE.upper()}   |   Δ rows: negative = better than Baseline\",\n",
    "        y=0.985, fontsize=12\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Baseline-only pose cartography across views (XY, YZ, XZ) ===\n",
    "# Rows: ΔSTFT, EDC, T60, C50, EDT   (5)\n",
    "# Cols: views (XY, YZ, XZ) + 1 thin colorbar column per row   (3 + 1)\n",
    "# - Per-row normalization (so all 3 views of a metric share the same scale)\n",
    "# - Viridis for all rows (baseline values); thin row-aligned colorbars\n",
    "# - No ticks; subtle frames; compact, paper-ready\n",
    "# - Built-in print-label swap: xy→xz, xz→xy, yz→zy (labels only; coordinates are correct)\n",
    "\n",
    "import os, glob, numpy as np, torch, matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from tqdm.auto import tqdm\n",
    "from torchaudio.transforms import GriffinLim\n",
    "\n",
    "from evaluator import compute_audio_distance\n",
    "\n",
    "# ------------------- Knobs -------------------\n",
    "BASELINE_PATTERN = \"../eval_results/emptyroom/emptyroom/renders/eval_*.npy\"\n",
    "MAX_FILES    = None\n",
    "POSE_MODE    = \"mic\"          # \"mic\" or \"src\"\n",
    "PRINT_LABELS = True           # swap labels for printing: xy→xz, xz→xy, yz→zy\n",
    "SAMPLE_RATE  = 48000\n",
    "n_fft        = (513 - 1) * 2\n",
    "win_length   = 512\n",
    "hop_length   = 256\n",
    "power        = 1.0\n",
    "BATCH_GLIM   = 128\n",
    "\n",
    "ROW_METRICS  = [\"SPL\", \"EDC\", \"T60\", \"C50\", \"EDT\"]   # SPL ≡ ΔSTFT\n",
    "ROW_TITLES   = {\"SPL\":\"ΔSTFT\", \"EDC\":\"EDC\", \"T60\":\"T60\", \"C50\":\"C50\", \"EDT\":\"EDT\"}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"DejaVu Serif\",\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlesize\": 12,\n",
    "})\n",
    "\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _load_baseline(bpat, max_files=None):\n",
    "    bfiles = sorted(glob.glob(bpat))\n",
    "    if not bfiles:\n",
    "        raise RuntimeError(\"No eval_*.npy files found for baseline.\")\n",
    "    if max_files and max_files > 0:\n",
    "        bfiles = bfiles[:max_files]\n",
    "    return bfiles\n",
    "\n",
    "def _ensure_2d_stft(x):\n",
    "    t = torch.from_numpy(x).float().squeeze()\n",
    "    if t.ndim != 2: raise ValueError(f\"STFT must be 2D (F,T); got {tuple(t.shape)}\")\n",
    "    return t\n",
    "\n",
    "def _ensure_1d_wav(x):\n",
    "    t = torch.from_numpy(x).float().squeeze()\n",
    "    if t.ndim != 1: raise ValueError(f\"Waveform must be 1D; got {tuple(t.shape)}\")\n",
    "    return t\n",
    "\n",
    "def _batch_griffin_lim(mag_batch, istft):\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp), torch.no_grad():\n",
    "        wav = istft(mag_batch)\n",
    "    return wav\n",
    "\n",
    "def _batched_logmag_to_wav(stft_list, istft, batch=BATCH_GLIM):\n",
    "    out = [None]*len(stft_list)\n",
    "    for s in tqdm(range(0, len(stft_list), batch), desc=\"GLIM batches\", unit=\"batch\"):\n",
    "        e = min(s+batch, len(stft_list))\n",
    "        Ts = [p.shape[1] for p in stft_list[s:e]]\n",
    "        Tm = min(Ts)\n",
    "        mags = torch.stack([torch.exp(p[:, :Tm]) - 1e-3 for p in stft_list[s:e]], dim=0).to(device, non_blocking=True)\n",
    "        wav_b = _batch_griffin_lim(mags, istft)\n",
    "        for i, w in enumerate(wav_b.detach().cpu()):\n",
    "            out[s+i] = w.contiguous()\n",
    "        del mags, wav_b\n",
    "        torch.cuda.empty_cache()\n",
    "    return out\n",
    "\n",
    "def _pair_score(metric, stft_gt, wav_gt, stft_pred, wav_pred):\n",
    "    mu = metric.upper()\n",
    "    L = min(len(wav_gt), len(wav_pred))\n",
    "    wavs = torch.stack([wav_gt[:L], wav_pred[:L]], dim=0)\n",
    "    T = min(stft_gt.shape[1], stft_pred.shape[1])\n",
    "    stft_pair = torch.stack([stft_gt[:, :T], stft_pred[:, :T]], dim=0)\n",
    "    with torch.no_grad():\n",
    "        D = compute_audio_distance(stft_pair, wavs=wavs, metric=mu, fs=SAMPLE_RATE)\n",
    "    return float(D[0,1].item())\n",
    "\n",
    "def _view_coords(poses):\n",
    "    X = poses[:, 0]; Y = poses[:, 1]; Z = poses[:, 2]\n",
    "    return {\n",
    "        \"xy\": (X, Y),\n",
    "        \"yz\": (Y, Z),\n",
    "        \"xz\": (X, Z),\n",
    "    }\n",
    "\n",
    "def _print_name(name):\n",
    "    if not PRINT_LABELS: return name.upper()\n",
    "    swap = {\"xy\":\"XZ\", \"xz\":\"XY\", \"yz\":\"ZY\"}  # label swap only\n",
    "    return swap.get(name.lower(), name.upper())\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "def main():\n",
    "    bfiles = _load_baseline(BASELINE_PATTERN, max_files=MAX_FILES)\n",
    "\n",
    "    stft_gt_list, wav_gt_list = [], []\n",
    "    stft_b_list = []\n",
    "    poses = np.zeros((len(bfiles), 3), dtype=np.float32)\n",
    "\n",
    "    for i, f in enumerate(tqdm(bfiles, desc=\"Loading eval dicts\", unit=\"file\")):\n",
    "        d = np.load(f, allow_pickle=True).item()\n",
    "        stft_gt_list.append(_ensure_2d_stft(d[\"data\"]))\n",
    "        wav_gt_list.append(_ensure_1d_wav(d[\"waveform\"]))\n",
    "        stft_b_list.append(_ensure_2d_stft(d[\"pred_stft\"]))\n",
    "        poses[i] = np.asarray(d[\"source_pose\" if POSE_MODE.lower().startswith(\"s\") else \"mic_pose\"],\n",
    "                              dtype=float).reshape(3)\n",
    "\n",
    "    istft = GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=power).to(device)\n",
    "    wav_b_list = _batched_logmag_to_wav(stft_b_list, istft, batch=BATCH_GLIM)\n",
    "\n",
    "    # Score baseline for all metrics\n",
    "    base = {m: np.zeros(len(bfiles), dtype=np.float32) for m in ROW_METRICS}\n",
    "    for m in ROW_METRICS:\n",
    "        for i in tqdm(range(len(bfiles)), desc=f\"Scoring {m}\", unit=\"sample\", leave=False):\n",
    "            base[m][i] = _pair_score(m, stft_gt_list[i], wav_gt_list[i], stft_b_list[i], wav_b_list[i])\n",
    "\n",
    "    # === Figure: 5 rows × (3 views + 1 colorbar) ===\n",
    "    VIEWS = [\"xy\", \"yz\", \"xz\"]\n",
    "    coords = _view_coords(poses)\n",
    "\n",
    "    NROWS = len(ROW_METRICS)\n",
    "    NCOLS = 4  # 3 views + colorbar\n",
    "    fig = plt.figure(figsize=(10.8, 11.6))\n",
    "    gs = gridspec.GridSpec(\n",
    "        nrows=NROWS, ncols=NCOLS, figure=fig,\n",
    "        left=0.07, right=0.96, top=0.94, bottom=0.06,\n",
    "        wspace=0.16, hspace=0.35,\n",
    "        width_ratios=[1, 1, 1, 0.035]\n",
    "    )\n",
    "\n",
    "    axes = np.array([[fig.add_subplot(gs[r, c]) for c in range(3)] for r in range(NROWS)])\n",
    "    cbar_axes = [fig.add_subplot(gs[r, 3]) for r in range(NROWS)]\n",
    "\n",
    "    # Column headers (view names, with print-swap applied)\n",
    "    for j, v in enumerate(VIEWS):\n",
    "        axes[0, j].set_title(_print_name(v), pad=6)\n",
    "\n",
    "    viridis = plt.cm.get_cmap(\"viridis\")\n",
    "\n",
    "    # Rows per metric: consistent vmin/vmax across the 3 views\n",
    "    for r, m in enumerate(ROW_METRICS):\n",
    "        vmin, vmax = float(base[m].min()), float(base[m].max())\n",
    "        for j, v in enumerate(VIEWS):\n",
    "            x, y = coords[v]\n",
    "            axes[r, j].scatter(x, y, c=base[m], cmap=viridis, vmin=vmin, vmax=vmax,\n",
    "                               s=8, linewidths=0)\n",
    "        sm = plt.cm.ScalarMappable(norm=mcolors.Normalize(vmin=vmin, vmax=vmax), cmap=viridis)\n",
    "        cb = fig.colorbar(sm, cax=cbar_axes[r])\n",
    "        if r == 0:\n",
    "            cb.set_label(\"lower is better\", labelpad=4)\n",
    "\n",
    "        # Row labels (left-most panel)\n",
    "        axes[r, 0].set_ylabel(ROW_TITLES.get(m, m), rotation=90, labelpad=28, fontsize=12, weight=\"bold\")\n",
    "\n",
    "    # Clean panels\n",
    "    for r in range(NROWS):\n",
    "        for j in range(3):\n",
    "            ax = axes[r, j]\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            for sp in ax.spines.values():\n",
    "                sp.set_linewidth(0.6); sp.set_alpha(0.6)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Baseline pose cartography — Views: {', '.join(_print_name(v) for v in VIEWS)}   |   \"\n",
    "        f\"Pose = {POSE_MODE.upper()}\",\n",
    "        y=0.985, fontsize=12\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b816c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ONE-CELL DASHBOARD (GPU + tqdm): Compare Retrieval vs Baseline, rank by improvement, plot GT/R/B ---\n",
    "\n",
    "# ==== KNOBS ====\n",
    "EVAL_PATTERN_RET   = \"../eval_results/refine_best/render/eval_*.npy\"\n",
    "EVAL_PATTERN_BASE  = \"../eval_results/baseline/renders/eval_*.npy\"\n",
    "ROOT_DIR           = \"../data/RAF/FurnishedRoom\"\n",
    "SAMPLE_RATE        = 48000\n",
    "MAX_EVAL_FILES     = None          # e.g., 300 or None for all overlaps\n",
    "TOP_N              = 30            # how many best-improved samples to visualize\n",
    "RANK_METRIC        = \"C50\"         # one of: \"SPL\", \"EDC\", \"EDT\", \"T60\", \"C50\"\n",
    "# plotting\n",
    "FIGSIZE_PER_SAMPLE = (14, 6)       # width, height for each sample (two rows x three columns)\n",
    "DPI                = 120\n",
    "\n",
    "# ==== IMPORTS ====\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.transforms import GriffinLim\n",
    "\n",
    "# Your project utilities (as in your snippet)\n",
    "from evaluator import compute_audio_distance, compute_edc_db\n",
    "from retriever import FurnishedRoomSTFTDataset  # RIRRetrievalMLP not needed here\n",
    "\n",
    "import sys\n",
    "sys.path.append('../NeRAF')\n",
    "from NeRAF_helper import compute_t60, evaluate_edt, evaluate_clarity\n",
    "\n",
    "# ==== DEVICE / ISTFT SETUP (GPU) ====\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "n_fft = (513 - 1) * 2; win_length = 512; hop_length = 256; power = 1\n",
    "istft = GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=power).to(device)\n",
    "\n",
    "# ==== HELPERS (GPU-friendly; only move to CPU for plotting) ====\n",
    "def _as_2d_numpy_cpu(wav_t):\n",
    "    arr = wav_t.detach().cpu().numpy()\n",
    "    return arr[None, :] if arr.ndim == 1 else arr\n",
    "\n",
    "def _pair_metrics_with_edc(stft_a, wav_a, stft_b, wav_b, edc_a, edc_b, fs=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    GPU: compute SPL, MAG/MAG2, MSE + EDC distance via compute_audio_distance\n",
    "    and room metrics EDT/C50 MAE and T60 %err. Returns dict of floats.\n",
    "    \"\"\"\n",
    "    # align waveforms for fair metric computation (on GPU)\n",
    "    L = min(wav_a.shape[0], wav_b.shape[0])\n",
    "    wav_a = wav_a[:L]; wav_b = wav_b[:L]\n",
    "\n",
    "    # EDC equal length (use GT length), still on GPU\n",
    "    T_edc = edc_a.shape[0]\n",
    "    if edc_b.shape[0] != T_edc:\n",
    "        if edc_b.shape[0] > T_edc: edc_b = edc_b[:T_edc]\n",
    "        else: edc_b = torch.nn.functional.pad(edc_b, (0, T_edc - edc_b.shape[0]))\n",
    "\n",
    "    pair_stft = torch.stack([stft_a, stft_b], dim=0)\n",
    "    pair_wav  = torch.stack([wav_a, wav_b], dim=0)\n",
    "    pair_edc  = torch.stack([edc_a, edc_b], dim=0)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        mse  = compute_audio_distance(pair_stft, wavs=pair_wav, metric='MSE')[0,1].item()\n",
    "        spl  = compute_audio_distance(pair_stft, wavs=pair_wav, metric='SPL', fs=fs)[0,1].item()\n",
    "        mag  = compute_audio_distance(pair_stft, metric='MAG')[0,1].item()\n",
    "        mag2 = compute_audio_distance(pair_stft, metric='MAG2')[0,1].item()\n",
    "        edcD = compute_audio_distance(pair_stft, wavs=None, edc_curves=pair_edc, metric='EDC')[0,1].item()\n",
    "\n",
    "    # Room-style metrics: need numpy; ship minimal data to CPU\n",
    "    gt = _as_2d_numpy_cpu(wav_a); xx = _as_2d_numpy_cpu(wav_b)\n",
    "    t60_gt, t60_x = compute_t60(gt, xx, fs=fs, advanced=True)\n",
    "    t60_gt = np.atleast_1d(t60_gt).astype(float); t60_x = np.atleast_1d(t60_x).astype(float)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        t60_diff = np.abs(t60_x - t60_gt) / (np.abs(t60_gt) + 1e-12)\n",
    "    invalid_mask = (t60_gt < -0.5) | (t60_x < -0.5)\n",
    "    t60_diff[invalid_mask] = 1.0\n",
    "    t60_err_pct = float(np.mean(t60_diff) * 100.0)\n",
    "\n",
    "    edt_gt, edt_x = evaluate_edt(xx, gt, fs=fs); edt_mae = float(np.mean(np.abs(edt_x - edt_gt)))\n",
    "    c50_gt, c50_x = evaluate_clarity(xx, gt, fs=fs); c50_mae = float(np.mean(np.abs(c50_x - c50_gt)))\n",
    "\n",
    "    return {'MSE': mse, 'SPL': spl, 'MAG': mag, 'MAG2': mag2, 'EDC': edcD,\n",
    "            'EDT': edt_mae, 'C50': c50_mae, 'T60': t60_err_pct}\n",
    "\n",
    "def _load_results(pattern, desc=\"scan\"):\n",
    "    paths_all = glob.glob(pattern)\n",
    "    # tqdm with numeric idx parse can be slow; wrap in progress bar\n",
    "    out = {}\n",
    "    for p in tqdm(paths_all, desc=f\"Loading {desc}\", unit=\"file\", leave=False):\n",
    "        try:\n",
    "            d = np.load(p, allow_pickle=True).item()\n",
    "            idx = int(d[\"audio_idx\"])\n",
    "            out[idx] = {\"path\": p, \"pred_stft\": d[\"pred_stft\"]}\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {p}: {e}\")\n",
    "    # sort by numeric audio_idx\n",
    "    out = dict(sorted(out.items(), key=lambda kv: kv[0]))\n",
    "    return out\n",
    "\n",
    "def _ensure_logmag_to_wav(stft_logmag_t):\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        mag = torch.exp(stft_logmag_t) - 1e-3\n",
    "        wav = istft(mag.unsqueeze(0)).squeeze(0)\n",
    "    return wav\n",
    "\n",
    "def _make_edc(wav_t, T_target=60):\n",
    "    return torch.nan_to_num(compute_edc_db(wav_t.float(), T_target=T_target), nan=0.0)\n",
    "\n",
    "def _collect_vmin_vmax(*arrays):\n",
    "    vmin = min([a.min() for a in arrays])\n",
    "    vmax = max([a.max() for a in arrays])\n",
    "    return float(vmin), float(vmax)\n",
    "\n",
    "# ==== LOAD DATASETS / RESULTS (with tqdm) ====\n",
    "ret = _load_results(EVAL_PATTERN_RET, desc=\"retrieval\")\n",
    "bas = _load_results(EVAL_PATTERN_BASE, desc=\"baseline\")\n",
    "\n",
    "# intersect on audio_idx to ensure fair comparison\n",
    "common_idx = sorted(set(ret.keys()) & set(bas.keys()))\n",
    "if MAX_EVAL_FILES is not None:\n",
    "    common_idx = common_idx[:MAX_EVAL_FILES]\n",
    "\n",
    "# dataset for GT\n",
    "ds_test = FurnishedRoomSTFTDataset(root_dir=ROOT_DIR, split=\"test\",\n",
    "                                   sample_rate=SAMPLE_RATE, return_wav=True, mode=\"normal\")\n",
    "\n",
    "# ==== PASS 1: compute metrics and stash plotting payloads (tqdm + GPU) ====\n",
    "records = []\n",
    "for idx in tqdm(common_idx, desc=\"Scoring (GPU)\", unit=\"file\"):\n",
    "    item = ds_test[idx]\n",
    "\n",
    "    # --- Ground truth (to GPU) ---\n",
    "    stft_gt  = item['stft'].squeeze(0).to(device, non_blocking=True)   # [F,T]\n",
    "    wav_gt   = item['wav'].squeeze().to(device, non_blocking=True)\n",
    "\n",
    "    # EDC target length (prefer dataset if present)\n",
    "    if item.get('edc') is not None and item['edc'].numel() > 0:\n",
    "        T_edc = int(item['edc'].shape[0])\n",
    "    else:\n",
    "        T_edc = 60\n",
    "\n",
    "    # Ensure GT EDC curve (length T_edc)\n",
    "    if item.get('edc') is not None and item['edc'].numel() > 0:\n",
    "        edc_gt = item['edc'].to(device, non_blocking=True)\n",
    "        if edc_gt.shape[0] != T_edc:\n",
    "            edc_gt = edc_gt[:T_edc] if edc_gt.shape[0] > T_edc else torch.nn.functional.pad(edc_gt, (0, T_edc - edc_gt.shape[0]))\n",
    "        edc_gt = torch.nan_to_num(edc_gt, nan=0.0)\n",
    "    else:\n",
    "        edc_gt = _make_edc(wav_gt, T_target=T_edc).to(device, non_blocking=True)\n",
    "\n",
    "    # --- Retrieval (stft_best) ---\n",
    "    pred_stft_R = torch.from_numpy(ret[idx][\"pred_stft\"]).float().squeeze(0).to(device, non_blocking=True)\n",
    "    wav_R       = _ensure_logmag_to_wav(pred_stft_R)\n",
    "    L_R         = min(wav_gt.shape[0], wav_R.shape[0])\n",
    "    wav_R       = wav_R[:L_R]; wav_gt_R = wav_gt[:L_R]  # aligned for R metrics\n",
    "    edc_R       = _make_edc(wav_R, T_target=T_edc).to(device, non_blocking=True)\n",
    "\n",
    "    # --- Baseline ---\n",
    "    pred_stft_B = torch.from_numpy(bas[idx][\"pred_stft\"]).float().squeeze(0).to(device, non_blocking=True)\n",
    "    wav_B       = _ensure_logmag_to_wav(pred_stft_B)\n",
    "    L_B         = min(wav_gt.shape[0], wav_B.shape[0])\n",
    "    wav_B       = wav_B[:L_B]; wav_gt_B = wav_gt[:L_B]  # aligned for B metrics\n",
    "    edc_B       = _make_edc(wav_B, T_target=T_edc).to(device, non_blocking=True)\n",
    "\n",
    "    # --- Metrics (vs GT) on GPU (with brief CPU hops inside helper) ---\n",
    "    metrics_R = _pair_metrics_with_edc(stft_gt, wav_gt_R, pred_stft_R, wav_R, edc_gt, edc_R, fs=SAMPLE_RATE)\n",
    "    metrics_B = _pair_metrics_with_edc(stft_gt, wav_gt_B, pred_stft_B, wav_B, edc_gt, edc_B, fs=SAMPLE_RATE)\n",
    "\n",
    "    # Ranking score: improvement (baseline_error - retrieval_error); bigger is better\n",
    "    if RANK_METRIC not in metrics_R:\n",
    "        raise ValueError(f\"RANK_METRIC '{RANK_METRIC}' not in computed metrics: {list(metrics_R.keys())}\")\n",
    "    improve = float(metrics_B[RANK_METRIC] - metrics_R[RANK_METRIC])\n",
    "\n",
    "    # Store plotting payloads (keep tensors on GPU for now; move to CPU right before plotting)\n",
    "    records.append({\n",
    "        \"idx\": idx,\n",
    "        \"plot_gpu\": {\n",
    "            \"wav\": {\"gt\": wav_gt, \"ret\": wav_R, \"bas\": wav_B},\n",
    "            \"stft\": {\"gt\": stft_gt, \"ret\": pred_stft_R, \"bas\": pred_stft_B},\n",
    "        },\n",
    "        \"metrics_R\": metrics_R,\n",
    "        \"metrics_B\": metrics_B,\n",
    "        \"improve\": improve\n",
    "    })\n",
    "\n",
    "    # keep VRAM tidy\n",
    "    del stft_gt, wav_gt, pred_stft_R, wav_R, pred_stft_B, wav_B, edc_gt, edc_R, edc_B, wav_gt_R, wav_gt_B\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ==== RANK & SELECT TOP-N ====\n",
    "records_sorted = sorted(records, key=lambda r: r[\"improve\"], reverse=True)\n",
    "top = records_sorted[:TOP_N]\n",
    "\n",
    "# ==== PLOTTING (with tqdm; tensors -> CPU only here) ====\n",
    "def _title_from_metrics(name, m):\n",
    "    # Only show the 5 requested: SPL, EDC, EDT, T60, C50\n",
    "    return (f\"{name}\\n\"\n",
    "            f\"ΔSTFT:{m['SPL']:.3f}  EDC:{m['EDC']:.3f}  \"\n",
    "            f\"EDT:{m['EDT']:.3f}  T60%:{m['T60']:.2f}  C50:{m['C50']:.3f}\")\n",
    "\n",
    "for rec in tqdm(top, desc=\"Plotting\", unit=\"sample\"):\n",
    "    idx = rec[\"idx\"]\n",
    "    wav_gt = rec[\"plot_gpu\"][\"wav\"][\"gt\"].detach().cpu().float().numpy()\n",
    "    wav_R  = rec[\"plot_gpu\"][\"wav\"][\"ret\"].detach().cpu().float().numpy()\n",
    "    wav_B  = rec[\"plot_gpu\"][\"wav\"][\"bas\"].detach().cpu().float().numpy()\n",
    "\n",
    "    S_gt   = rec[\"plot_gpu\"][\"stft\"][\"gt\"].detach().cpu().float().numpy()\n",
    "    S_R    = rec[\"plot_gpu\"][\"stft\"][\"ret\"].detach().cpu().float().numpy()\n",
    "    S_B    = rec[\"plot_gpu\"][\"stft\"][\"bas\"].detach().cpu().float().numpy()\n",
    "\n",
    "    # Time axis\n",
    "    t_gt = np.arange(len(wav_gt)) / SAMPLE_RATE\n",
    "    t_R  = np.arange(len(wav_R))  / SAMPLE_RATE\n",
    "    t_B  = np.arange(len(wav_B))  / SAMPLE_RATE\n",
    "\n",
    "    # Consistent spectrogram scaling\n",
    "    vmin, vmax = _collect_vmin_vmax(S_gt, S_R, S_B)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=FIGSIZE_PER_SAMPLE, dpi=DPI,\n",
    "                             gridspec_kw={'height_ratios':[1, 1]})\n",
    "    fig.suptitle(f\"audio_idx={idx}\", y=1.02)\n",
    "\n",
    "    # --- Row 1: time-domain ---\n",
    "    axes[0,0].plot(t_R, wav_R, linewidth=0.75)\n",
    "    axes[0,0].set_title(_title_from_metrics(\"Retrieval (waveform)\", rec[\"metrics_R\"]))\n",
    "    axes[0,0].set_xlabel(\"Time [s]\"); axes[0,0].set_ylabel(\"Amp\")\n",
    "\n",
    "    axes[0,1].plot(t_gt, wav_gt, linewidth=0.75)\n",
    "    axes[0,1].set_title(\"GT (waveform)\")\n",
    "    axes[0,1].set_xlabel(\"Time [s]\"); axes[0,1].set_ylabel(\"Amp\")\n",
    "\n",
    "    axes[0,2].plot(t_B, wav_B, linewidth=0.75)\n",
    "    axes[0,2].set_title(_title_from_metrics(\"Baseline (waveform)\", rec[\"metrics_B\"]))\n",
    "    axes[0,2].set_xlabel(\"Time [s]\"); axes[0,2].set_ylabel(\"Amp\")\n",
    "\n",
    "    # --- Row 2: STFTs ---\n",
    "    axes[1,0].imshow(S_R, origin='lower', aspect='auto', vmin=vmin, vmax=vmax)\n",
    "    axes[1,0].set_title(_title_from_metrics(\"Retrieval (STFT log-mag)\", rec[\"metrics_R\"]))\n",
    "    axes[1,0].set_xlabel(\"Frames\"); axes[1,0].set_ylabel(\"Bins\")\n",
    "\n",
    "    axes[1,1].imshow(S_gt, origin='lower', aspect='auto', vmin=vmin, vmax=vmax)\n",
    "    axes[1,1].set_title(\"GT (STFT log-mag)\")\n",
    "    axes[1,1].set_xlabel(\"Frames\"); axes[1,1].set_ylabel(\"Bins\")\n",
    "\n",
    "    axes[1,2].imshow(S_B, origin='lower', aspect='auto', vmin=vmin, vmax=vmax)\n",
    "    axes[1,2].set_title(_title_from_metrics(\"Baseline (STFT log-mag)\", rec[\"metrics_B\"]))\n",
    "    axes[1,2].set_xlabel(\"Frames\"); axes[1,2].set_ylabel(\"Bins\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(f\"Plotted top {len(top)} samples ranked by improvement in {RANK_METRIC} on {device.type.upper()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763da6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLEAN, SORTED, GPU GALLERY (rows=samples; cols=GT | Baseline | D1(stft_best) | D2(refine_best)) ---\n",
    "# Makes TWO figures: Waveforms and STFT log-mags.\n",
    "\n",
    "# ==== KNOBS ====\n",
    "EVAL_PATTERN_BASE   = \"../eval_results/baseline/renders/eval_*.npy\"      # Baseline (NeRAF)\n",
    "EVAL_PATTERN_D1     = \"../eval_results/stft_no_edc/renders/eval_*.npy\"   # Design 1: Feature Fusion\n",
    "EVAL_PATTERN_D2     = \"../eval_results/refine_best/renders/eval_*.npy\"   # Design 2: Output Modification\n",
    "ROOT_DIR            = \"../data/RAF/FurnishedRoom\"\n",
    "SAMPLE_RATE         = 48000\n",
    "\n",
    "MAX_EVAL_FILES      = None     # cap overlaps before sorting (None → all)\n",
    "NUM_SAMPLES         = 10     # up to 10 rows\n",
    "SORT_BY_METRIC      = \"SPL\"  # \"SPL\",\"T60\",\"C50\",\"EDT\",\"EDC\"\n",
    "\n",
    "FIG_DPI             = 130\n",
    "FIG_W               = 14\n",
    "FIG_H_WAVE          = 1.1\n",
    "FIG_H_STFT          = 1.1\n",
    "COL_TITLE_SIZE      = 12     # bold column names (top row)\n",
    "METRIC_FONTSIZE     = 8      # single-line metrics (non-bold)\n",
    "\n",
    "# ==== IMPORTS ====\n",
    "import os, glob, numpy as np, torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.transforms import GriffinLim\n",
    "\n",
    "from evaluator import compute_audio_distance, compute_edc_db\n",
    "from retriever import FurnishedRoomSTFTDataset\n",
    "\n",
    "import sys\n",
    "sys.path.append('../NeRAF')\n",
    "from NeRAF_helper import compute_t60, evaluate_edt, evaluate_clarity\n",
    "\n",
    "# ==== DEVICE / ISTFT ====\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = (device.type == \"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "n_fft = (513 - 1) * 2; win_length = 512; hop_length = 256; power = 1\n",
    "istft = GriffinLim(n_fft=n_fft, win_length=win_length, hop_length=hop_length, power=power).to(device)\n",
    "\n",
    "# ==== HELPERS ====\n",
    "def _load_results(pattern, desc):\n",
    "    out = {}\n",
    "    for p in tqdm(glob.glob(pattern), desc=f\"Loading {desc}\", unit=\"file\", leave=False):\n",
    "        try:\n",
    "            d = np.load(p, allow_pickle=True).item()\n",
    "            out[int(d[\"audio_idx\"])] = {\"pred_stft\": d[\"pred_stft\"]}\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] skip {p}: {e}\")\n",
    "    return dict(sorted(out.items(), key=lambda kv: kv[0]))\n",
    "\n",
    "def _ensure_logmag_to_wav(stft_logmag_t):\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        mag = torch.exp(stft_logmag_t) - 1e-3\n",
    "        wav = istft(mag.unsqueeze(0)).squeeze(0)\n",
    "    return wav\n",
    "\n",
    "def _make_edc(wav_t, T_target=60):\n",
    "    return torch.nan_to_num(compute_edc_db(wav_t.float(), T_target=T_target), nan=0.0)\n",
    "\n",
    "def _as_2d_numpy_cpu(x):\n",
    "    a = x.detach().cpu().numpy()\n",
    "    return a[None, :] if a.ndim == 1 else a\n",
    "\n",
    "def _metrics_vs_gt(stft_gt, wav_gt, stft_pred, wav_pred, edc_gt, fs=SAMPLE_RATE):\n",
    "    # align\n",
    "    L = min(wav_gt.shape[0], wav_pred.shape[0])\n",
    "    wav_gt = wav_gt[:L]; wav_pred = wav_pred[:L]\n",
    "    pair_stft = torch.stack([stft_gt, stft_pred], dim=0)\n",
    "    pair_wav  = torch.stack([wav_gt, wav_pred], dim=0)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        spl = compute_audio_distance(pair_stft, wavs=pair_wav, metric='SPL', fs=fs)[0,1].item()\n",
    "        # EDT/C50/T60 via numpy APIs\n",
    "        edt_gt, edt_x = evaluate_edt(_as_2d_numpy_cpu(wav_pred), _as_2d_numpy_cpu(wav_gt), fs=fs)\n",
    "        c50_gt, c50_x = evaluate_clarity(_as_2d_numpy_cpu(wav_pred), _as_2d_numpy_cpu(wav_gt), fs=fs)\n",
    "        t60_gt, t60_x = compute_t60(_as_2d_numpy_cpu(wav_gt), _as_2d_numpy_cpu(wav_pred), fs=fs, advanced=True)\n",
    "        t60_gt = np.atleast_1d(t60_gt).astype(float); t60_x = np.atleast_1d(t60_x).astype(float)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            t60_diff = np.abs(t60_x - t60_gt) / (np.abs(t60_gt) + 1e-12)\n",
    "        t60_diff[(t60_gt < -0.5) | (t60_x < -0.5)] = 1.0\n",
    "        t60_err = float(np.mean(t60_diff) * 100.0)\n",
    "        edt_mae = float(np.mean(np.abs(edt_x - edt_gt)))\n",
    "        c50_mae = float(np.mean(np.abs(c50_x - c50_gt)))\n",
    "    return {'SPL': float(spl), 'T60': t60_err, 'C50': c50_mae, 'EDT': edt_mae}\n",
    "\n",
    "def _fmt_one_line(m):  # single line metrics\n",
    "    return f\"ΔSTFT:{m['SPL']:.3f}  T60%:{m['T60']:.2f}  C50:{m['C50']:.3f}  EDT:{m['EDT']:.3f}\"\n",
    "\n",
    "def _collect_vmin_vmax(*arrs):\n",
    "    vmin = min(a.min() for a in arrs); vmax = max(a.max() for a in arrs)\n",
    "    return float(vmin), float(vmax)\n",
    "\n",
    "# NEW: Use CAD for sorting when available (SPL/EDC). Fallback to precomputed m for others.\n",
    "_DIRECT_BY_CAD = {\"SPL\", \"EDC\"}\n",
    "\n",
    "def _score_for_sort(row, metric, fs=SAMPLE_RATE):\n",
    "    if metric in _DIRECT_BY_CAD:\n",
    "        # Compute CAD on-the-fly between Baseline and GT using requested metric\n",
    "        stft_gt = row[\"GT\"][\"stft\"].to(device, non_blocking=True)\n",
    "        wav_gt  = row[\"GT\"][\"wav\"].to(device, non_blocking=True)\n",
    "        stft_b  = row[\"B\"][\"stft\"].to(device, non_blocking=True)\n",
    "        wav_b   = row[\"B\"][\"wav\"].to(device, non_blocking=True)\n",
    "        # Align\n",
    "        L = min(wav_gt.shape[0], wav_b.shape[0])\n",
    "        wav_gt = wav_gt[:L]; wav_b = wav_b[:L]\n",
    "        pair_stft = torch.stack([stft_gt, stft_b], dim=0)\n",
    "        pair_wav  = torch.stack([wav_gt,  wav_b],  dim=0)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            val = compute_audio_distance(pair_stft, wavs=pair_wav, metric=metric, fs=fs)[0,1].item()\n",
    "        return float(val)\n",
    "    else:\n",
    "        # Use previously computed scalar error\n",
    "        return float(row[\"B\"][\"m\"][metric])\n",
    "\n",
    "# ==== LOAD ====\n",
    "B   = _load_results(EVAL_PATTERN_BASE, \"baseline\")\n",
    "D1  = _load_results(EVAL_PATTERN_D1,   \"design-1 (stft_best)\")\n",
    "D2  = _load_results(EVAL_PATTERN_D2,   \"design-2 (refine_best)\")\n",
    "\n",
    "common = sorted(set(B.keys()) & set(D1.keys()) & set(D2.keys()))\n",
    "if MAX_EVAL_FILES: common = common[:MAX_EVAL_FILES]\n",
    "\n",
    "ds = FurnishedRoomSTFTDataset(root_dir=ROOT_DIR, split=\"test\",\n",
    "                              sample_rate=SAMPLE_RATE, return_wav=True, mode=\"normal\")\n",
    "\n",
    "# ==== BUILD + SCORE (GPU) ====\n",
    "rows = []\n",
    "for idx in tqdm(common, desc=\"Preparing (GPU)\", unit=\"file\"):\n",
    "    item = ds[idx]\n",
    "    stft_gt = item['stft'].squeeze(0).to(device, non_blocking=True)\n",
    "    wav_gt  = item['wav'].squeeze().to(device, non_blocking=True)\n",
    "\n",
    "    stft_b  = torch.from_numpy(B[idx][\"pred_stft\"]).float().squeeze(0).to(device, non_blocking=True)\n",
    "    stft_d1 = torch.from_numpy(D1[idx][\"pred_stft\"]).float().squeeze(0).to(device, non_blocking=True)\n",
    "    stft_d2 = torch.from_numpy(D2[idx][\"pred_stft\"]).float().squeeze(0).to(device, non_blocking=True)\n",
    "\n",
    "    wav_b  = _ensure_logmag_to_wav(stft_b)\n",
    "    wav_d1 = _ensure_logmag_to_wav(stft_d1)\n",
    "    wav_d2 = _ensure_logmag_to_wav(stft_d2)\n",
    "\n",
    "    # metrics vs GT\n",
    "    mB  = _metrics_vs_gt(stft_gt, wav_gt, stft_b,  wav_b,  None)\n",
    "    mD1 = _metrics_vs_gt(stft_gt, wav_gt, stft_d1, wav_d1, None)\n",
    "    mD2 = _metrics_vs_gt(stft_gt, wav_gt, stft_d2, wav_d2, None)\n",
    "\n",
    "    rows.append({\n",
    "        \"idx\": idx,\n",
    "        \"GT\": {\"wav\": wav_gt, \"stft\": stft_gt},    # no metrics shown for GT\n",
    "        \"B\":  {\"wav\": wav_b,  \"stft\": stft_b,  \"m\": mB},\n",
    "        \"D1\": {\"wav\": wav_d1, \"stft\": stft_d1, \"m\": mD1},\n",
    "        \"D2\": {\"wav\": wav_d2, \"stft\": stft_d2, \"m\": mD2},\n",
    "    })\n",
    "\n",
    "    # VRAM hygiene\n",
    "    del stft_gt, wav_gt, stft_b, stft_d1, stft_d2, wav_b, wav_d1, wav_d2\n",
    "    if device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "# ==== SORT by Baseline's chosen metric (worst→best), then take top N ====\n",
    "if SORT_BY_METRIC not in (\"SPL\",\"T60\",\"C50\",\"EDT\",\"EDC\"):\n",
    "    raise ValueError(\"SORT_BY_METRIC must be one of: SPL, T60, C50, EDT, EDC\")\n",
    "rows_sorted = sorted(rows, key=lambda r: _score_for_sort(r, SORT_BY_METRIC), reverse=True)\n",
    "rows = rows_sorted[:NUM_SAMPLES]\n",
    "\n",
    "# ==== FIGURE A: WAVEFORMS (no ticks, no idx labels) ====\n",
    "figA, axsA = plt.subplots(len(rows), 4, figsize=(FIG_W, FIG_H_WAVE*len(rows)), dpi=FIG_DPI)\n",
    "if len(rows) == 1: axsA = np.expand_dims(axsA, 0)\n",
    "\n",
    "col_keys   = [\"GT\", \"B\", \"D1\", \"D2\"]\n",
    "col_titles = [\"Ground Truth\", \"Baseline (NeRAF)\", \"Design 1: Feature Fusion\", \"Design 2: Output Modification\"]\n",
    "\n",
    "for r, row in enumerate(rows):\n",
    "    for c, key in enumerate(col_keys):\n",
    "        ax = axsA[r, c]\n",
    "        w = row[key][\"wav\"].detach().cpu().float().numpy()\n",
    "        t = np.arange(len(w))/SAMPLE_RATE\n",
    "        ax.plot(t, w, linewidth=0.8)\n",
    "        ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_linewidth(0.6); sp.set_alpha(0.8)\n",
    "\n",
    "        # Top-row: bold headers only; metrics in normal weight just below header (avoid collisions)\n",
    "        if r == 0:\n",
    "            ax.set_title(col_titles[c], fontweight=\"bold\", fontsize=COL_TITLE_SIZE, pad=18)\n",
    "            if key != \"GT\":\n",
    "                ax.text(0.5, 1.02, _fmt_one_line(row[key][\"m\"]),\n",
    "                        transform=ax.transAxes, ha=\"center\", va=\"bottom\",\n",
    "                        fontsize=METRIC_FONTSIZE, fontweight=\"normal\")\n",
    "        else:\n",
    "            if key != \"GT\":\n",
    "                ax.set_title(_fmt_one_line(row[key][\"m\"]), fontsize=METRIC_FONTSIZE, pad=4)\n",
    "\n",
    "figA.suptitle(\"Waveforms — rows: samples | cols: GT | Baseline | D1 | D2\", y=0.995, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==== FIGURE B: STFTs (no ticks, same rules) ====\n",
    "figB, axsB = plt.subplots(len(rows), 4, figsize=(FIG_W, FIG_H_STFT*len(rows)), dpi=FIG_DPI)\n",
    "if len(rows) == 1: axsB = np.expand_dims(axsB, 0)\n",
    "\n",
    "for r, row in enumerate(rows):\n",
    "    Sarr = [row[k][\"stft\"].detach().cpu().float().numpy() for k in col_keys]\n",
    "    vmin, vmax = _collect_vmin_vmax(*Sarr)\n",
    "\n",
    "    for c, key in enumerate(col_keys):\n",
    "        ax = axsB[r, c]\n",
    "        ax.imshow(Sarr[c], origin=\"lower\", aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "        ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "        for sp in ax.spines.values():\n",
    "            sp.set_linewidth(0.6); sp.set_alpha(0.8)\n",
    "\n",
    "        if r == 0:\n",
    "            ax.set_title(col_titles[c], fontweight=\"bold\", fontsize=COL_TITLE_SIZE, pad=18)\n",
    "            if key != \"GT\":\n",
    "                ax.text(0.5, 1.02, _fmt_one_line(row[key][\"m\"]),\n",
    "                        transform=ax.transAxes, ha=\"center\", va=\"bottom\",\n",
    "                        fontsize=METRIC_FONTSIZE, fontweight=\"normal\")\n",
    "        else:\n",
    "            if key != \"GT\":\n",
    "                ax.set_title(_fmt_one_line(row[key][\"m\"]), fontsize=METRIC_FONTSIZE, pad=4)\n",
    "\n",
    "figB.suptitle(\"STFT log-mag — rows: samples | cols: GT | Baseline | D1 | D2\", y=0.995, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
