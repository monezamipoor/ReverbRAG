{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e3e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /media/scratch/projects/labuser/msc_user/MoNezami/ReverbRAG/camera_path.json with 47484 / 47484 paired cameras (stride=1, limit=None).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, math\n",
    "import numpy as np\n",
    "\n",
    "# --- inputs ---\n",
    "RX_POS_TXT = Path(\"../NeRAF/data/RAF/EmptyRoom/metadata/all_rx_pos.txt\")\n",
    "TX_POS_TXT = Path(\"../NeRAF/data/RAF/EmptyRoom/metadata/all_tx_pos.txt\")\n",
    "RUN_DIR_JSON = Path(\"./outputs/EmptyRoom/images-jpeg-1k/nerfacto/2025-11-04_172911/dataparser_transforms.json\")\n",
    "\n",
    "OUT_JSON = Path(\"camera_path.json\")\n",
    "\n",
    "# Viewer header\n",
    "DEFAULT_FOV = 100.0\n",
    "ASPECT = 1.5\n",
    "RENDER_H, RENDER_W = 256,256\n",
    "SECONDS = 999\n",
    "IS_CYCLE = False\n",
    "SMOOTHNESS = 0\n",
    "CAMERA_TYPE = \"perspective\"\n",
    "\n",
    "GLOBAL_UP = np.array([0.0, 0.0, 1.0], dtype=float)  # Z-up\n",
    "\n",
    "# Control how many you keep\n",
    "LIMIT = None     # None = all\n",
    "STRIDE = 1       # keep every STRIDE-th pair (e.g., 5)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def float_no_sci(x, ndigits=12):\n",
    "    s = f\"{x:.{ndigits}f}\"\n",
    "    if \".\" in s:\n",
    "        s = s.rstrip(\"0\").rstrip(\".\")\n",
    "        if s == \"-0\":\n",
    "            s = \"0\"\n",
    "        if \".\" not in s:\n",
    "            s += \".0\"\n",
    "    return float(s)\n",
    "\n",
    "def matrix_row_major_list(m4):\n",
    "    return [float_no_sci(m4[i, j]) for i in range(4) for j in range(4)]\n",
    "\n",
    "def load_dataparser_transform(run_dir_json):\n",
    "    with open(run_dir_json, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    T_3x4 = np.array(meta[\"transform\"], dtype=float)  # (3,4)\n",
    "    s = float(meta[\"scale\"])\n",
    "    R = T_3x4[:, :3]\n",
    "    t = T_3x4[:, 3]\n",
    "    return R, t, s\n",
    "\n",
    "def normalize(v, eps=1e-9):\n",
    "    n = np.linalg.norm(v)\n",
    "    return v / n if n > eps else v * 0.0\n",
    "\n",
    "def build_upright_look_at(cam_pos, target_pos, global_up=np.array([0,0,1.0])):\n",
    "    f = normalize(target_pos - cam_pos)\n",
    "    r = np.cross(global_up, f)\n",
    "    if np.linalg.norm(r) < 1e-6:\n",
    "        r = np.cross(np.array([1.0,0.0,0.0]), f)\n",
    "        if np.linalg.norm(r) < 1e-6:\n",
    "            r = np.array([0.0,1.0,0.0])\n",
    "    r = normalize(r)\n",
    "    u = np.cross(f, r)\n",
    "    c2w = np.eye(4, dtype=float)\n",
    "    c2w[:3, 0] = r\n",
    "    c2w[:3, 1] = u\n",
    "    c2w[:3, 2] = f\n",
    "    c2w[:3, 3] = cam_pos\n",
    "    return c2w\n",
    "\n",
    "def parse_rx_all(path):\n",
    "    \"\"\"Yield RX positions: each line is y,z,x -> xyz; skip NaNs.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = [p.strip() for p in line.strip().split(\",\")]\n",
    "            if len(parts) < 3: \n",
    "                continue\n",
    "            try:\n",
    "                y = float(parts[0]); z = float(parts[1]); x = float(parts[2])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if all(math.isfinite(v) for v in (x,y,z)):\n",
    "                yield np.array([x,y,z], dtype=float)\n",
    "\n",
    "def parse_tx_all(path):\n",
    "    \"\"\"Yield TX quaternion (HypA yzxW -> xyzW) + TX position (yzx -> xyz).\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = [p.strip() for p in line.strip().split(\",\")]\n",
    "            if len(parts) < 7:\n",
    "                continue\n",
    "            try:\n",
    "                qy = float(parts[0]); qz = float(parts[1]); qx = float(parts[2]); qw = float(parts[3])  # yzxW\n",
    "                py = float(parts[4]); pz = float(parts[5]); px = float(parts[6])                      # yzx\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if not all(math.isfinite(v) for v in (qx,qy,qz,qw,px,py,pz)):\n",
    "                continue\n",
    "            quat_xyzW = (qx, qy, qz, qw)\n",
    "            pos_xyz   = np.array([px, py, pz], dtype=float)\n",
    "            yield quat_xyzW, pos_xyz\n",
    "\n",
    "def world_to_nerf_point(p_world, R, t, s):\n",
    "    return s * (R @ p_world + t)\n",
    "\n",
    "# ---------- main ----------\n",
    "R_dp, t_dp, s_dp = load_dataparser_transform(RUN_DIR_JSON)\n",
    "\n",
    "rx_iter = parse_rx_all(RX_POS_TXT)\n",
    "tx_iter = parse_tx_all(TX_POS_TXT)\n",
    "\n",
    "camera_entries = []\n",
    "count_total = 0\n",
    "count_kept = 0\n",
    "\n",
    "for idx, (rx_p, tx_tuple) in enumerate(zip(rx_iter, tx_iter)):\n",
    "    count_total += 1\n",
    "    if idx % STRIDE != 0:\n",
    "        continue\n",
    "    _, tx_p = tx_tuple  # quat parsed but unused for upright mode\n",
    "    # world -> NeRF\n",
    "    rx_n = world_to_nerf_point(rx_p, R_dp, t_dp, s_dp)\n",
    "    tx_n = world_to_nerf_point(tx_p, R_dp, t_dp, s_dp)\n",
    "    # upright look-at\n",
    "    c2w = build_upright_look_at(rx_n, tx_n, GLOBAL_UP)\n",
    "    camera_entries.append({\n",
    "        \"camera_to_world\": matrix_row_major_list(c2w),\n",
    "        \"fov\": float_no_sci(DEFAULT_FOV),\n",
    "        \"aspect\": float_no_sci(ASPECT),\n",
    "    })\n",
    "    count_kept += 1\n",
    "    if LIMIT is not None and count_kept >= LIMIT:\n",
    "        break\n",
    "\n",
    "camera_path = {\n",
    "    \"default_fov\": float_no_sci(DEFAULT_FOV),\n",
    "    \"default_transition_sec\": 2,\n",
    "    \"camera_type\": CAMERA_TYPE,\n",
    "    \"render_height\": RENDER_H,\n",
    "    \"render_width\": RENDER_W,\n",
    "    \"seconds\": SECONDS,\n",
    "    \"is_cycle\": IS_CYCLE,\n",
    "    \"smoothness_value\": SMOOTHNESS,\n",
    "    \"camera_path\": camera_entries,\n",
    "}\n",
    "\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(camera_path, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {OUT_JSON.resolve()} with {count_kept} / {count_total} paired cameras (stride={STRIDE}, limit={LIMIT}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f29b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STFT] Items/shard≈17442 → #shards=3\n",
      "[STFT] writing shard 0: 17442 items → ../NeRAF/data/RAF/EmptyRoom/feats/shard_000_stft.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[STFT] 1/3: 100%|██████████| 17442/17442 [01:08<00:00, 255.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STFT] writing shard 1: 17442 items → ../NeRAF/data/RAF/EmptyRoom/feats/shard_001_stft.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[STFT] 2/3: 100%|██████████| 17442/17442 [01:08<00:00, 253.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STFT] writing shard 2: 12600 items → ../NeRAF/data/RAF/EmptyRoom/feats/shard_002_stft.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[STFT] 3/3: 100%|██████████| 12600/12600 [00:48<00:00, 260.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STFT] Done. Index saved to ../NeRAF/data/RAF/EmptyRoom/feats/index.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, numpy as np, torch, torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "SCENE_ROOT = \"../NeRAF/data/RAF/EmptyRoom\"\n",
    "DATA_DIR   = os.path.join(SCENE_ROOT, \"data\")\n",
    "FEATS_DIR  = os.path.join(SCENE_ROOT, \"feats\")\n",
    "os.makedirs(FEATS_DIR, exist_ok=True)\n",
    "\n",
    "# STFT params (RAF)\n",
    "sr = 48000\n",
    "n_fft, win_length, hop_length = 1024, 512, 256\n",
    "T = 60\n",
    "F = n_fft//2 + 1\n",
    "DT_STFT = np.float16\n",
    "MAX_SHARD_MB = 1024\n",
    "\n",
    "stft_tf = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=n_fft, win_length=win_length, hop_length=hop_length,\n",
    "    power=None, center=True, pad_mode=\"reflect\"\n",
    ")\n",
    "def _logmag(x): return torch.log(x.abs() + 1e-3)\n",
    "\n",
    "def _collect_sids(meta_json):\n",
    "    with open(meta_json, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    sids = []\n",
    "    for v in splits.values():\n",
    "        block = v[0] if (isinstance(v, list) and v and isinstance(v[0], list)) else v\n",
    "        for sid in block:\n",
    "            sids.append(f\"{int(sid):06d}\" if str(sid).isdigit() else str(sid))\n",
    "    return sorted(set(sids))\n",
    "\n",
    "def _load_wav(sid):\n",
    "    p = os.path.join(DATA_DIR, sid, \"rir.wav\")\n",
    "    wav, r = torchaudio.load(p)\n",
    "    if r != sr: wav = torchaudio.functional.resample(wav, r, sr)\n",
    "    wav = wav[:, : int(0.32 * sr)]\n",
    "    return wav\n",
    "\n",
    "def _stft60(wav):\n",
    "    spec = stft_tf(wav)  # [1,F,T_full]\n",
    "    if spec.shape[-1] > T:\n",
    "        spec = spec[:, :, :T]\n",
    "    elif spec.shape[-1] < T:\n",
    "        minval = float(spec.abs().min())\n",
    "        spec = torch.nn.functional.pad(spec, (0, T - spec.shape[-1]), value=minval)\n",
    "    return _logmag(spec).squeeze(0)  # [F,T]\n",
    "\n",
    "# Discover SIDs and (maybe) an existing index.json\n",
    "SPLIT_JSON = os.path.join(SCENE_ROOT, \"metadata\", \"data-split.json\")\n",
    "sids = _collect_sids(SPLIT_JSON)\n",
    "idx_path = os.path.join(FEATS_DIR, \"index.json\")\n",
    "index_meta = {\"shards\": [], \"sid_to_ptr\": {}}\n",
    "if os.path.exists(idx_path):\n",
    "    with open(idx_path, \"r\") as f:\n",
    "        index_meta = json.load(f)\n",
    "\n",
    "# Compute shard sizing\n",
    "bytes_per_stft = F*T*np.dtype(DT_STFT).itemsize\n",
    "items_per_shard = max(1, (MAX_SHARD_MB*1024*1024)//bytes_per_stft)\n",
    "N = len(sids)\n",
    "num_shards = math.ceil(N/items_per_shard)\n",
    "print(f\"[STFT] Items/shard≈{items_per_shard} → #shards={num_shards}\")\n",
    "\n",
    "def shard_paths(k):\n",
    "    base = os.path.join(FEATS_DIR, f\"shard_{k:03d}\")\n",
    "    return base+\"_stft.npy\"\n",
    "\n",
    "# Build shards\n",
    "for k in range(num_shards):\n",
    "    start = k*items_per_shard\n",
    "    end   = min(N, (k+1)*items_per_shard)\n",
    "    n_k   = end - start\n",
    "    st_p  = shard_paths(k)\n",
    "\n",
    "    if os.path.exists(st_p):\n",
    "        print(f\"[STFT] shard {k} exists, skipping write.\")\n",
    "        st_mm = np.memmap(st_p, dtype=DT_STFT, mode=\"r+\", shape=(n_k, F, T))\n",
    "    else:\n",
    "        print(f\"[STFT] writing shard {k}: {n_k} items → {st_p}\")\n",
    "        st_mm = np.memmap(st_p, dtype=DT_STFT, mode=\"w+\", shape=(n_k, F, T))\n",
    "\n",
    "    for i, sid in tqdm(list(enumerate(sids[start:end], start=0)), total=n_k, desc=f\"[STFT] {k+1}/{num_shards}\"):\n",
    "        if sid in index_meta[\"sid_to_ptr\"]:\n",
    "            continue\n",
    "        x = _stft60(_load_wav(sid)).cpu().numpy().astype(DT_STFT)\n",
    "        st_mm[i] = x\n",
    "        index_meta[\"sid_to_ptr\"][sid] = [k, i]\n",
    "    del st_mm\n",
    "\n",
    "# Write/merge index\n",
    "# ensure one entry per shard with at least 'stft' path\n",
    "existing = {sh[\"id\"]: sh for sh in index_meta.get(\"shards\", [])}\n",
    "for k in range(num_shards):\n",
    "    st_p = shard_paths(k)\n",
    "    if k in existing:\n",
    "        existing[k][\"stft\"] = st_p\n",
    "        existing[k][\"count\"] = existing[k].get(\"count\", 0) or sum(1 for v in index_meta[\"sid_to_ptr\"].values() if v[0]==k)\n",
    "        existing[k][\"F\"] = F; existing[k][\"T\"] = T\n",
    "        existing[k].setdefault(\"dtypes\", {})[\"stft\"] = str(DT_STFT)\n",
    "    else:\n",
    "        existing[k] = {\"id\": k, \"stft\": st_p, \"count\": sum(1 for v in index_meta[\"sid_to_ptr\"].values() if v[0]==k),\n",
    "                       \"F\": F, \"T\": T, \"dtypes\": {\"stft\": str(DT_STFT)}}\n",
    "index_meta[\"shards\"] = [existing[k] for k in sorted(existing.keys())]\n",
    "\n",
    "with open(idx_path, \"w\") as f:\n",
    "    json.dump(index_meta, f)\n",
    "print(\"[STFT] Done. Index saved to\", idx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3196529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDC] shard 0: writing 17442 rows into 17442-row memmap -> ../NeRAF/data/RAF/EmptyRoom/feats/shard_000_edc.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EDC] 000: 100%|██████████| 17442/17442 [00:33<00:00, 523.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDC] shard 1: writing 17442 rows into 17442-row memmap -> ../NeRAF/data/RAF/EmptyRoom/feats/shard_001_edc.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EDC] 001: 100%|██████████| 17442/17442 [00:35<00:00, 491.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDC] shard 2: writing 12600 rows into 12600-row memmap -> ../NeRAF/data/RAF/EmptyRoom/feats/shard_002_edc.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EDC] 002: 100%|██████████| 12600/12600 [00:24<00:00, 510.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDC] Repair complete. index.json updated.\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, torch, torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "SCENE_ROOT = \"../NeRAF/data/RAF/EmptyRoom\"\n",
    "DATA_DIR   = os.path.join(SCENE_ROOT, \"data\")\n",
    "FEATS_DIR  = os.path.join(SCENE_ROOT, \"feats\")\n",
    "IDX_PATH   = os.path.join(FEATS_DIR, \"index.json\")\n",
    "\n",
    "assert os.path.exists(IDX_PATH), \"Missing feats/index.json — build STFT shards first.\"\n",
    "\n",
    "# EDC settings\n",
    "sr = 48000\n",
    "T = 60\n",
    "DT_EDC = np.float32\n",
    "\n",
    "def _load_wav(sid):\n",
    "    p = os.path.join(DATA_DIR, sid, \"rir.wav\")\n",
    "    wav, r = torchaudio.load(p)\n",
    "    if r != sr: wav = torchaudio.functional.resample(wav, r, sr)\n",
    "    return wav.squeeze(0)[: int(0.32 * sr)]  # [S]\n",
    "\n",
    "def _edc_db_60(w1d):\n",
    "    e = (w1d.float()**2)\n",
    "    edc = torch.flip(torch.cumsum(torch.flip(e, [0]), 0), [0])\n",
    "    edc = edc / (edc[0] + 1e-12)\n",
    "    edc_db = 10*torch.log10(edc + 1e-12)\n",
    "    idx = torch.linspace(0, edc_db.numel()-1, steps=T).long()\n",
    "    return edc_db[idx]  # [T]\n",
    "\n",
    "with open(IDX_PATH, \"r\") as f:\n",
    "    idx = json.load(f)\n",
    "\n",
    "sid_to_ptr = {k: tuple(v) for k, v in idx[\"sid_to_ptr\"].items()}\n",
    "# Gather SIDs per shard using the EXISTING mapping\n",
    "shard_to_sidrows = {}\n",
    "for sid, (k, row) in sid_to_ptr.items():\n",
    "    shard_to_sidrows.setdefault(k, []).append((sid, row))\n",
    "\n",
    "# Build/overwrite EDC shard files using the STFT shard counts\n",
    "for sh in idx[\"shards\"]:\n",
    "    k = sh[\"id\"]\n",
    "    count = int(sh[\"count\"])\n",
    "    edc_path = os.path.join(FEATS_DIR, f\"shard_{k:03d}_edc.npy\")\n",
    "\n",
    "    # Create memmap with shape matching the STFT shard\n",
    "    ed_mm = np.memmap(edc_path, dtype=DT_EDC, mode=\"w+\", shape=(count, T))\n",
    "    rows = shard_to_sidrows.get(k, [])\n",
    "    print(f\"[EDC] shard {k}: writing {len(rows)} rows into {count}-row memmap -> {edc_path}\")\n",
    "\n",
    "    for sid, row in tqdm(rows, total=len(rows), desc=f\"[EDC] {k:03d}\"):\n",
    "        # guard: row must be < count (if not, your index.json is already inconsistent with STFT shards)\n",
    "        if not (0 <= row < count):\n",
    "            raise RuntimeError(f\"Index mismatch: SID {sid} maps to row {row} but shard {k} has count {count}\")\n",
    "        w = _load_wav(sid)\n",
    "        ed_mm[row] = _edc_db_60(w).cpu().numpy().astype(DT_EDC)\n",
    "\n",
    "    del ed_mm\n",
    "\n",
    "    # annotate shard record with EDC path & dtype\n",
    "    sh[\"edc\"] = edc_path\n",
    "    sh.setdefault(\"dtypes\", {})[\"edc\"] = str(DT_EDC)\n",
    "\n",
    "with open(IDX_PATH, \"w\") as f:\n",
    "    json.dump(idx, f)\n",
    "\n",
    "print(\"[EDC] Repair complete. index.json updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
