{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e3e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /media/scratch/projects/labuser/msc_user/MoNezami/ReverbRAG/camera_path.json with 47484 / 47484 paired cameras (stride=1, limit=None).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, math\n",
    "import numpy as np\n",
    "\n",
    "# --- inputs ---\n",
    "RX_POS_TXT = Path(\"../NeRAF/data/RAF/EmptyRoom/metadata/all_rx_pos.txt\")\n",
    "TX_POS_TXT = Path(\"../NeRAF/data/RAF/EmptyRoom/metadata/all_tx_pos.txt\")\n",
    "RUN_DIR_JSON = Path(\"./outputs/EmptyRoom/images-jpeg-1k/nerfacto/2025-11-04_172911/dataparser_transforms.json\")\n",
    "\n",
    "OUT_JSON = Path(\"camera_path.json\")\n",
    "\n",
    "# Viewer header\n",
    "DEFAULT_FOV = 100.0\n",
    "ASPECT = 1.5\n",
    "RENDER_H, RENDER_W = 256,256\n",
    "SECONDS = 999\n",
    "IS_CYCLE = False\n",
    "SMOOTHNESS = 0\n",
    "CAMERA_TYPE = \"perspective\"\n",
    "\n",
    "GLOBAL_UP = np.array([0.0, 0.0, 1.0], dtype=float)  # Z-up\n",
    "\n",
    "# Control how many you keep\n",
    "LIMIT = None     # None = all\n",
    "STRIDE = 1       # keep every STRIDE-th pair (e.g., 5)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def float_no_sci(x, ndigits=12):\n",
    "    s = f\"{x:.{ndigits}f}\"\n",
    "    if \".\" in s:\n",
    "        s = s.rstrip(\"0\").rstrip(\".\")\n",
    "        if s == \"-0\":\n",
    "            s = \"0\"\n",
    "        if \".\" not in s:\n",
    "            s += \".0\"\n",
    "    return float(s)\n",
    "\n",
    "def matrix_row_major_list(m4):\n",
    "    return [float_no_sci(m4[i, j]) for i in range(4) for j in range(4)]\n",
    "\n",
    "def load_dataparser_transform(run_dir_json):\n",
    "    with open(run_dir_json, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    T_3x4 = np.array(meta[\"transform\"], dtype=float)  # (3,4)\n",
    "    s = float(meta[\"scale\"])\n",
    "    R = T_3x4[:, :3]\n",
    "    t = T_3x4[:, 3]\n",
    "    return R, t, s\n",
    "\n",
    "def normalize(v, eps=1e-9):\n",
    "    n = np.linalg.norm(v)\n",
    "    return v / n if n > eps else v * 0.0\n",
    "\n",
    "def build_upright_look_at(cam_pos, target_pos, global_up=np.array([0,0,1.0])):\n",
    "    f = normalize(target_pos - cam_pos)\n",
    "    r = np.cross(global_up, f)\n",
    "    if np.linalg.norm(r) < 1e-6:\n",
    "        r = np.cross(np.array([1.0,0.0,0.0]), f)\n",
    "        if np.linalg.norm(r) < 1e-6:\n",
    "            r = np.array([0.0,1.0,0.0])\n",
    "    r = normalize(r)\n",
    "    u = np.cross(f, r)\n",
    "    c2w = np.eye(4, dtype=float)\n",
    "    c2w[:3, 0] = r\n",
    "    c2w[:3, 1] = u\n",
    "    c2w[:3, 2] = f\n",
    "    c2w[:3, 3] = cam_pos\n",
    "    return c2w\n",
    "\n",
    "def parse_rx_all(path):\n",
    "    \"\"\"Yield RX positions: each line is y,z,x -> xyz; skip NaNs.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = [p.strip() for p in line.strip().split(\",\")]\n",
    "            if len(parts) < 3: \n",
    "                continue\n",
    "            try:\n",
    "                y = float(parts[0]); z = float(parts[1]); x = float(parts[2])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if all(math.isfinite(v) for v in (x,y,z)):\n",
    "                yield np.array([x,y,z], dtype=float)\n",
    "\n",
    "def parse_tx_all(path):\n",
    "    \"\"\"Yield TX quaternion (HypA yzxW -> xyzW) + TX position (yzx -> xyz).\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = [p.strip() for p in line.strip().split(\",\")]\n",
    "            if len(parts) < 7:\n",
    "                continue\n",
    "            try:\n",
    "                qy = float(parts[0]); qz = float(parts[1]); qx = float(parts[2]); qw = float(parts[3])  # yzxW\n",
    "                py = float(parts[4]); pz = float(parts[5]); px = float(parts[6])                      # yzx\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if not all(math.isfinite(v) for v in (qx,qy,qz,qw,px,py,pz)):\n",
    "                continue\n",
    "            quat_xyzW = (qx, qy, qz, qw)\n",
    "            pos_xyz   = np.array([px, py, pz], dtype=float)\n",
    "            yield quat_xyzW, pos_xyz\n",
    "\n",
    "def world_to_nerf_point(p_world, R, t, s):\n",
    "    return s * (R @ p_world + t)\n",
    "\n",
    "# ---------- main ----------\n",
    "R_dp, t_dp, s_dp = load_dataparser_transform(RUN_DIR_JSON)\n",
    "\n",
    "rx_iter = parse_rx_all(RX_POS_TXT)\n",
    "tx_iter = parse_tx_all(TX_POS_TXT)\n",
    "\n",
    "camera_entries = []\n",
    "count_total = 0\n",
    "count_kept = 0\n",
    "\n",
    "for idx, (rx_p, tx_tuple) in enumerate(zip(rx_iter, tx_iter)):\n",
    "    count_total += 1\n",
    "    if idx % STRIDE != 0:\n",
    "        continue\n",
    "    _, tx_p = tx_tuple  # quat parsed but unused for upright mode\n",
    "    # world -> NeRF\n",
    "    rx_n = world_to_nerf_point(rx_p, R_dp, t_dp, s_dp)\n",
    "    tx_n = world_to_nerf_point(tx_p, R_dp, t_dp, s_dp)\n",
    "    # upright look-at\n",
    "    c2w = build_upright_look_at(rx_n, tx_n, GLOBAL_UP)\n",
    "    camera_entries.append({\n",
    "        \"camera_to_world\": matrix_row_major_list(c2w),\n",
    "        \"fov\": float_no_sci(DEFAULT_FOV),\n",
    "        \"aspect\": float_no_sci(ASPECT),\n",
    "    })\n",
    "    count_kept += 1\n",
    "    if LIMIT is not None and count_kept >= LIMIT:\n",
    "        break\n",
    "\n",
    "camera_path = {\n",
    "    \"default_fov\": float_no_sci(DEFAULT_FOV),\n",
    "    \"default_transition_sec\": 2,\n",
    "    \"camera_type\": CAMERA_TYPE,\n",
    "    \"render_height\": RENDER_H,\n",
    "    \"render_width\": RENDER_W,\n",
    "    \"seconds\": SECONDS,\n",
    "    \"is_cycle\": IS_CYCLE,\n",
    "    \"smoothness_value\": SMOOTHNESS,\n",
    "    \"camera_path\": camera_entries,\n",
    "}\n",
    "\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(camera_path, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {OUT_JSON.resolve()} with {count_kept} / {count_total} paired cameras (stride={STRIDE}, limit={LIMIT}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f29b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STFT] Items/shard≈17442 → #shards=3\n",
      "[STFT] writing shard 0: 17442 items → ../NeRAF/data/RAF/EmptyRoom/feats/shard_000_stft.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[STFT] 1/3: 100%|██████████| 17442/17442 [01:08<00:00, 255.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STFT] writing shard 1: 17442 items → ../NeRAF/data/RAF/EmptyRoom/feats/shard_001_stft.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[STFT] 2/3: 100%|██████████| 17442/17442 [01:08<00:00, 253.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STFT] writing shard 2: 12600 items → ../NeRAF/data/RAF/EmptyRoom/feats/shard_002_stft.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[STFT] 3/3: 100%|██████████| 12600/12600 [00:48<00:00, 260.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STFT] Done. Index saved to ../NeRAF/data/RAF/EmptyRoom/feats/index.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, numpy as np, torch, torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "SCENE_ROOT = \"../NeRAF/data/RAF/EmptyRoom\"\n",
    "DATA_DIR   = os.path.join(SCENE_ROOT, \"data\")\n",
    "FEATS_DIR  = os.path.join(SCENE_ROOT, \"feats\")\n",
    "os.makedirs(FEATS_DIR, exist_ok=True)\n",
    "\n",
    "# STFT params (RAF)\n",
    "sr = 48000\n",
    "n_fft, win_length, hop_length = 1024, 512, 256\n",
    "T = 60\n",
    "F = n_fft//2 + 1\n",
    "DT_STFT = np.float16\n",
    "MAX_SHARD_MB = 1024\n",
    "\n",
    "stft_tf = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=n_fft, win_length=win_length, hop_length=hop_length,\n",
    "    power=None, center=True, pad_mode=\"reflect\"\n",
    ")\n",
    "def _logmag(x): return torch.log(x.abs() + 1e-3)\n",
    "\n",
    "def _collect_sids(meta_json):\n",
    "    with open(meta_json, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    sids = []\n",
    "    for v in splits.values():\n",
    "        block = v[0] if (isinstance(v, list) and v and isinstance(v[0], list)) else v\n",
    "        for sid in block:\n",
    "            sids.append(f\"{int(sid):06d}\" if str(sid).isdigit() else str(sid))\n",
    "    return sorted(set(sids))\n",
    "\n",
    "def _load_wav(sid):\n",
    "    p = os.path.join(DATA_DIR, sid, \"rir.wav\")\n",
    "    wav, r = torchaudio.load(p)\n",
    "    if r != sr: wav = torchaudio.functional.resample(wav, r, sr)\n",
    "    wav = wav[:, : int(0.32 * sr)]\n",
    "    return wav\n",
    "\n",
    "def _stft60(wav):\n",
    "    spec = stft_tf(wav)  # [1,F,T_full]\n",
    "    if spec.shape[-1] > T:\n",
    "        spec = spec[:, :, :T]\n",
    "    elif spec.shape[-1] < T:\n",
    "        minval = float(spec.abs().min())\n",
    "        spec = torch.nn.functional.pad(spec, (0, T - spec.shape[-1]), value=minval)\n",
    "    return _logmag(spec).squeeze(0)  # [F,T]\n",
    "\n",
    "# Discover SIDs and (maybe) an existing index.json\n",
    "SPLIT_JSON = os.path.join(SCENE_ROOT, \"metadata\", \"data-split.json\")\n",
    "sids = _collect_sids(SPLIT_JSON)\n",
    "idx_path = os.path.join(FEATS_DIR, \"index.json\")\n",
    "index_meta = {\"shards\": [], \"sid_to_ptr\": {}}\n",
    "if os.path.exists(idx_path):\n",
    "    with open(idx_path, \"r\") as f:\n",
    "        index_meta = json.load(f)\n",
    "\n",
    "# Compute shard sizing\n",
    "bytes_per_stft = F*T*np.dtype(DT_STFT).itemsize\n",
    "items_per_shard = max(1, (MAX_SHARD_MB*1024*1024)//bytes_per_stft)\n",
    "N = len(sids)\n",
    "num_shards = math.ceil(N/items_per_shard)\n",
    "print(f\"[STFT] Items/shard≈{items_per_shard} → #shards={num_shards}\")\n",
    "\n",
    "def shard_paths(k):\n",
    "    base = os.path.join(FEATS_DIR, f\"shard_{k:03d}\")\n",
    "    return base+\"_stft.npy\"\n",
    "\n",
    "# Build shards\n",
    "for k in range(num_shards):\n",
    "    start = k*items_per_shard\n",
    "    end   = min(N, (k+1)*items_per_shard)\n",
    "    n_k   = end - start\n",
    "    st_p  = shard_paths(k)\n",
    "\n",
    "    if os.path.exists(st_p):\n",
    "        print(f\"[STFT] shard {k} exists, skipping write.\")\n",
    "        st_mm = np.memmap(st_p, dtype=DT_STFT, mode=\"r+\", shape=(n_k, F, T))\n",
    "    else:\n",
    "        print(f\"[STFT] writing shard {k}: {n_k} items → {st_p}\")\n",
    "        st_mm = np.memmap(st_p, dtype=DT_STFT, mode=\"w+\", shape=(n_k, F, T))\n",
    "\n",
    "    for i, sid in tqdm(list(enumerate(sids[start:end], start=0)), total=n_k, desc=f\"[STFT] {k+1}/{num_shards}\"):\n",
    "        if sid in index_meta[\"sid_to_ptr\"]:\n",
    "            continue\n",
    "        x = _stft60(_load_wav(sid)).cpu().numpy().astype(DT_STFT)\n",
    "        st_mm[i] = x\n",
    "        index_meta[\"sid_to_ptr\"][sid] = [k, i]\n",
    "    del st_mm\n",
    "\n",
    "# Write/merge index\n",
    "# ensure one entry per shard with at least 'stft' path\n",
    "existing = {sh[\"id\"]: sh for sh in index_meta.get(\"shards\", [])}\n",
    "for k in range(num_shards):\n",
    "    st_p = shard_paths(k)\n",
    "    if k in existing:\n",
    "        existing[k][\"stft\"] = st_p\n",
    "        existing[k][\"count\"] = existing[k].get(\"count\", 0) or sum(1 for v in index_meta[\"sid_to_ptr\"].values() if v[0]==k)\n",
    "        existing[k][\"F\"] = F; existing[k][\"T\"] = T\n",
    "        existing[k].setdefault(\"dtypes\", {})[\"stft\"] = str(DT_STFT)\n",
    "    else:\n",
    "        existing[k] = {\"id\": k, \"stft\": st_p, \"count\": sum(1 for v in index_meta[\"sid_to_ptr\"].values() if v[0]==k),\n",
    "                       \"F\": F, \"T\": T, \"dtypes\": {\"stft\": str(DT_STFT)}}\n",
    "index_meta[\"shards\"] = [existing[k] for k in sorted(existing.keys())]\n",
    "\n",
    "with open(idx_path, \"w\") as f:\n",
    "    json.dump(index_meta, f)\n",
    "print(\"[STFT] Done. Index saved to\", idx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3196529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDC] shard 0: writing 17442 rows into 17442-row memmap -> ../NeRAF/data/RAF/EmptyRoom/feats/shard_000_edc.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EDC] 000: 100%|██████████| 17442/17442 [00:33<00:00, 523.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDC] shard 1: writing 17442 rows into 17442-row memmap -> ../NeRAF/data/RAF/EmptyRoom/feats/shard_001_edc.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EDC] 001: 100%|██████████| 17442/17442 [00:35<00:00, 491.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDC] shard 2: writing 12600 rows into 12600-row memmap -> ../NeRAF/data/RAF/EmptyRoom/feats/shard_002_edc.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EDC] 002: 100%|██████████| 12600/12600 [00:24<00:00, 510.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDC] Repair complete. index.json updated.\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, torch, torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "SCENE_ROOT = \"../NeRAF/data/RAF/EmptyRoom\"\n",
    "DATA_DIR   = os.path.join(SCENE_ROOT, \"data\")\n",
    "FEATS_DIR  = os.path.join(SCENE_ROOT, \"feats\")\n",
    "IDX_PATH   = os.path.join(FEATS_DIR, \"index.json\")\n",
    "\n",
    "assert os.path.exists(IDX_PATH), \"Missing feats/index.json — build STFT shards first.\"\n",
    "\n",
    "# EDC settings\n",
    "sr = 48000\n",
    "T = 60\n",
    "DT_EDC = np.float32\n",
    "\n",
    "def _load_wav(sid):\n",
    "    p = os.path.join(DATA_DIR, sid, \"rir.wav\")\n",
    "    wav, r = torchaudio.load(p)\n",
    "    if r != sr: wav = torchaudio.functional.resample(wav, r, sr)\n",
    "    return wav.squeeze(0)[: int(0.32 * sr)]  # [S]\n",
    "\n",
    "def _edc_db_60(w1d):\n",
    "    e = (w1d.float()**2)\n",
    "    edc = torch.flip(torch.cumsum(torch.flip(e, [0]), 0), [0])\n",
    "    edc = edc / (edc[0] + 1e-12)\n",
    "    edc_db = 10*torch.log10(edc + 1e-12)\n",
    "    idx = torch.linspace(0, edc_db.numel()-1, steps=T).long()\n",
    "    return edc_db[idx]  # [T]\n",
    "\n",
    "with open(IDX_PATH, \"r\") as f:\n",
    "    idx = json.load(f)\n",
    "\n",
    "sid_to_ptr = {k: tuple(v) for k, v in idx[\"sid_to_ptr\"].items()}\n",
    "# Gather SIDs per shard using the EXISTING mapping\n",
    "shard_to_sidrows = {}\n",
    "for sid, (k, row) in sid_to_ptr.items():\n",
    "    shard_to_sidrows.setdefault(k, []).append((sid, row))\n",
    "\n",
    "# Build/overwrite EDC shard files using the STFT shard counts\n",
    "for sh in idx[\"shards\"]:\n",
    "    k = sh[\"id\"]\n",
    "    count = int(sh[\"count\"])\n",
    "    edc_path = os.path.join(FEATS_DIR, f\"shard_{k:03d}_edc.npy\")\n",
    "\n",
    "    # Create memmap with shape matching the STFT shard\n",
    "    ed_mm = np.memmap(edc_path, dtype=DT_EDC, mode=\"w+\", shape=(count, T))\n",
    "    rows = shard_to_sidrows.get(k, [])\n",
    "    print(f\"[EDC] shard {k}: writing {len(rows)} rows into {count}-row memmap -> {edc_path}\")\n",
    "\n",
    "    for sid, row in tqdm(rows, total=len(rows), desc=f\"[EDC] {k:03d}\"):\n",
    "        # guard: row must be < count (if not, your index.json is already inconsistent with STFT shards)\n",
    "        if not (0 <= row < count):\n",
    "            raise RuntimeError(f\"Index mismatch: SID {sid} maps to row {row} but shard {k} has count {count}\")\n",
    "        w = _load_wav(sid)\n",
    "        ed_mm[row] = _edc_db_60(w).cpu().numpy().astype(DT_EDC)\n",
    "\n",
    "    del ed_mm\n",
    "\n",
    "    # annotate shard record with EDC path & dtype\n",
    "    sh[\"edc\"] = edc_path\n",
    "    sh.setdefault(\"dtypes\", {})[\"edc\"] = str(DT_EDC)\n",
    "\n",
    "with open(IDX_PATH, \"w\") as f:\n",
    "    json.dump(idx, f)\n",
    "\n",
    "print(\"[EDC] Repair complete. index.json updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f499be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Queries (K) = 20\n",
      "[info] Reference bank: split='reference' | #items=20000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a2cd3eaed049b8846f33e0d6e3f837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load refs:   0%|          | 0/20000 [00:00<?, ?ref/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Reference STFT grid: F=513, T=60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2995875f1f5d46f18767df4da3f0bb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load queries:   0%|          | 0/20 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[edc] Computing EDC curves for queries & refs (once each) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3284dfc0528c4faebe0bcd0e31eafff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EDC(Q):   0%|          | 0/20 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aca98da8343477382acbf031e25e23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EDC(R):   0%|          | 0/20000 [00:00<?, ?ref/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955246c9e3d446e19e559af6770b47f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decays(Q):   0%|          | 0/20 [00:00<?, ?wav/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4194065ade04436ba0fe16194dcdf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decays(R):   0%|          | 0/20000 [00:00<?, ?wav/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[metric] EDC(L1): broadcasting (K=20, R=20000, B=60) ...\n",
      "[saved] EDC: ./dist_cache/EDC.npz | mu=0.141915 sd=0.065945\n",
      "\n",
      "[metric] SPL: computing (K=20, R=20000) via hub ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57261dccbe5849b9aab8b9897964e61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SPL per-query:   0%|          | 0/20 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] SPL: ./dist_cache/SPL.npz | mu=0.587531 sd=0.340065\n",
      "\n",
      "[metric] T60: broadcasting (K=20, R=20000) ...\n",
      "[saved] T60: ./dist_cache/T60.npz | mu=0.169088 sd=0.135457\n",
      "\n",
      "[metric] EDT: broadcasting (K=20, R=20000) ...\n",
      "[saved] EDT: ./dist_cache/EDT.npz | mu=0.256786 sd=0.219765\n",
      "\n",
      "[metric] C50: broadcasting (K=20, R=20000) ...\n",
      "[saved] C50: ./dist_cache/C50.npz | mu=8.121808 sd=6.199946\n",
      "\n",
      "[metric] DR: broadcasting (K=20, R=20000) ...\n",
      "[saved] DR: ./dist_cache/DR.npz | mu=5.458077 sd=4.075886\n",
      "\n",
      "[done] All requested metrics cached globally in ./dist_cache/\n"
     ]
    }
   ],
   "source": [
    "# CELL 1 — Global cache (fast):\n",
    "#   • EDC via precomputed curves + broadcast distances (L1/L2, globally consistent)\n",
    "#   • SPL via GPU hub (compute_audio_distance)\n",
    "#   • Decays (T60/EDT/C50/DR) via one-shot scalars + broadcasting\n",
    "#\n",
    "# Saves: ./dist_cache/{METRIC}.npz with RAW & global-Z matrices aligned across metrics\n",
    "\n",
    "import os, sys, glob, json, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------- KNOBS ----------------\n",
    "SCENE_ROOT      = \"../NeRAF/data/RAF/FurnishedRoom\"                      # adjust\n",
    "RENDERS_GLOB    = \"../NeRAF/eval_results/furnishedroom/renders/eval_*.npy\"\n",
    "K_QUERIES       = 20                   # first K renders as queries\n",
    "REF_SPLIT_TRY   = \"reference\"        # fallback to \"train\" if missing\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Which metrics to compute/cache\n",
    "METRICS = [\"EDC\", \"SPL\", \"T60\", \"EDT\", \"C50\", \"DR\"]\n",
    "\n",
    "# EDC options (to mimic your hub normalization by default)\n",
    "EVAL_FS         = 48000\n",
    "EDC_BINS        = 60                 # fixed bins for all signals\n",
    "EDC_ANCHOR_ZERO = True               # subtract first bin (0 dB at start)\n",
    "EDC_ZSCORE      = True               # per-curve z-score after anchoring\n",
    "EDC_DIST_MODE   = \"l1\"               # choose \"l1\" or \"l2\"\n",
    "# scale normalization for distances so magnitudes are comparable\n",
    "EDC_L2_SCALE    = \"sqrtN\"            # \"sqrtN\" -> divide by sqrt(EDC_BINS); None to disable\n",
    "EDC_L1_SCALE    = \"mean\"             # \"mean\"  -> divide by N; None to disable\n",
    "\n",
    "# GPU chunking over references (for SPL only)\n",
    "REF_BATCH       = 2048\n",
    "\n",
    "# Save dir\n",
    "DIST_CACHE_DIR  = \"./dist_cache\"\n",
    "\n",
    "# ---------------------------------------\n",
    "# Project imports\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"/mnt/data\")\n",
    "from evaluator import compute_audio_distance, compute_edc_db\n",
    "try:\n",
    "    from evaluator import compute_t60, evaluate_edt, evaluate_clarity\n",
    "    HAVE_DECAY_HELPERS = True\n",
    "except Exception:\n",
    "    HAVE_DECAY_HELPERS = False\n",
    "\n",
    "try:\n",
    "    from data import RAFDataset\n",
    "except Exception:\n",
    "    raise RuntimeError(\"Cannot import RAFDataset; fix sys.path to your repo.\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def _ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _global_z(M: np.ndarray):\n",
    "    m = np.isfinite(M)\n",
    "    if not m.any():\n",
    "        mu, sd = 0.0, 1.0\n",
    "        Z = np.zeros_like(M, dtype=np.float32)\n",
    "    else:\n",
    "        mu = float(np.mean(M[m]))\n",
    "        sd = float(np.std(M[m]) + 1e-6)\n",
    "        Z = (M - mu) / sd\n",
    "        Z[~np.isfinite(Z)] = 0.0\n",
    "    return Z.astype(np.float32), float(mu), float(sd)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _pad_batch_1d(wavs_list, device, dtype=torch.float32):\n",
    "    if len(wavs_list) == 0:\n",
    "        return torch.zeros(0, 1, dtype=dtype, device=device)\n",
    "    Tmax = max(int(w.numel()) for w in wavs_list)\n",
    "    out = torch.zeros(len(wavs_list), Tmax, dtype=dtype, device=device)\n",
    "    for i, w in enumerate(wavs_list):\n",
    "        t = int(w.numel())\n",
    "        out[i, :t] = w.to(device=device, dtype=dtype)\n",
    "    return out\n",
    "\n",
    "def _dr_db(x_1d_np: np.ndarray) -> float:\n",
    "    rms = np.sqrt(np.mean(x_1d_np**2) + 1e-12)\n",
    "    peak = np.max(np.abs(x_1d_np)) + 1e-12\n",
    "    return float(20.0 * np.log10(peak / max(rms, 1e-12)))\n",
    "\n",
    "# ---------- Scan renders (queries) ----------\n",
    "render_files = sorted(glob.glob(RENDErs_GLOB := RENDERS_GLOB))\n",
    "assert len(render_files) > 0, f\"No NPY renders found at: {RENDErs_GLOB}\"\n",
    "if K_QUERIES is not None:\n",
    "    render_files = render_files[:int(K_QUERIES)]\n",
    "query_ids = [os.path.basename(fp) for fp in render_files]\n",
    "Q = len(query_ids)\n",
    "print(f\"[info] Queries (K) = {Q}\")\n",
    "\n",
    "# ---------- Reference bank ----------\n",
    "try:\n",
    "    ds_ref = RAFDataset(scene_root=SCENE_ROOT, split=REF_SPLIT_TRY,\n",
    "                        model_kind=\"neraf\", sample_rate=EVAL_FS, dataset_mode=\"full\")\n",
    "    ref_split_name = REF_SPLIT_TRY\n",
    "except Exception as e:\n",
    "    print(f\"[warn] '{REF_SPLIT_TRY}' split missing ({e}); using 'train' as reference bank.\")\n",
    "    ds_ref = RAFDataset(scene_root=SCENE_ROOT, split=\"train\",\n",
    "                        model_kind=\"neraf\", sample_rate=EVAL_FS, dataset_mode=\"full\")\n",
    "    ref_split_name = \"train\"\n",
    "\n",
    "ref_ids    = list(ds_ref.ids)\n",
    "id2idx_ref = ds_ref.id2idx\n",
    "R = len(ref_ids)\n",
    "print(f\"[info] Reference bank: split='{ref_split_name}' | #items={R}\")\n",
    "\n",
    "# ---------- Preload references (CPU) ----------\n",
    "REF_STFTS, REF_WAVS = [], []\n",
    "for sid in tqdm(ref_ids, desc=\"Load refs\", unit=\"ref\"):\n",
    "    it = ds_ref[id2idx_ref[sid]]\n",
    "    REF_STFTS.append(it[\"stft\"].squeeze(0).contiguous())  # [F,60]\n",
    "    REF_WAVS.append(it[\"wav\"].squeeze(0).contiguous())    # [T]\n",
    "F, T = REF_STFTS[0].shape\n",
    "print(f\"[info] Reference STFT grid: F={F}, T={T}\")\n",
    "\n",
    "# ---------- Load queries (CPU) ----------\n",
    "Q_STFTS, Q_WAVS = [], []\n",
    "for fp in tqdm(render_files, desc=\"Load queries\", unit=\"file\"):\n",
    "    pack = np.load(fp, allow_pickle=True).item()\n",
    "    Q_STFTS.append(torch.from_numpy(pack[\"data\"]).float().squeeze(0).contiguous())      # [F,60] GT\n",
    "    Q_WAVS.append(torch.from_numpy(pack[\"waveform\"]).float().squeeze(0).contiguous())   # [T]\n",
    "\n",
    "# ---------- EDC CURVES: compute ONCE for all signals ----------\n",
    "@torch.no_grad()\n",
    "def _edc_curve(wav_1d: torch.Tensor, bins: int = EDC_BINS) -> torch.Tensor:\n",
    "    # compute_edc_db returns shape [T_edc]; we fix T_edc=bins for consistency\n",
    "    return compute_edc_db(wav_1d.float(), T_target=bins)  # on current device of wav_1d\n",
    "\n",
    "def _normalize_edc_curves(mat: np.ndarray) -> np.ndarray:\n",
    "    # mat: (N, BINS), in dB\n",
    "    X = mat.copy()\n",
    "    if EDC_ANCHOR_ZERO:\n",
    "        X = X - X[:, :1]   # anchor at 0 dB at start\n",
    "    if EDC_ZSCORE:\n",
    "        mu = X.mean(axis=1, keepdims=True)\n",
    "        sd = X.std(axis=1, keepdims=True) + 1e-6\n",
    "        X = (X - mu) / sd\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def _edc_scalers():\n",
    "    # Return scaling factor for distances to make magnitudes comparable\n",
    "    if EDC_DIST_MODE.lower() == \"l2\":\n",
    "        if EDC_L2_SCALE == \"sqrtN\":\n",
    "            return 1.0 / math.sqrt(EDC_BINS)\n",
    "        return 1.0\n",
    "    elif EDC_DIST_MODE.lower() == \"l1\":\n",
    "        if EDC_L1_SCALE == \"mean\":\n",
    "            return 1.0 / float(EDC_BINS)\n",
    "        return 1.0\n",
    "    else:\n",
    "        raise ValueError(\"EDC_DIST_MODE must be 'l1' or 'l2'\")\n",
    "\n",
    "print(\"\\n[edc] Computing EDC curves for queries & refs (once each) ...\")\n",
    "Q_EDC = np.zeros((Q, EDC_BINS), dtype=np.float32)\n",
    "for i, w in enumerate(tqdm(Q_WAVS, desc=\"EDC(Q)\", unit=\"q\")):\n",
    "    edc = _edc_curve(w.to(DEVICE)).detach().cpu().numpy()\n",
    "    Q_EDC[i, :] = edc\n",
    "\n",
    "R_EDC = np.zeros((R, EDC_BINS), dtype=np.float32)\n",
    "for j, w in enumerate(tqdm(REF_WAVS, desc=\"EDC(R)\", unit=\"ref\")):\n",
    "    edc = _edc_curve(w.to(DEVICE)).detach().cpu().numpy()\n",
    "    R_EDC[j, :] = edc\n",
    "\n",
    "# Normalize per-curve (mimic hub path)\n",
    "Q_EDC = _normalize_edc_curves(Q_EDC)  # (Q,B)\n",
    "R_EDC = _normalize_edc_curves(R_EDC)  # (R,B)\n",
    "\n",
    "# ---------- DECAY SCALARS: compute ONCE ----------\n",
    "def _decay_scalars_for_wavs(wavs_list, tag):\n",
    "    if not HAVE_DECAY_HELPERS:\n",
    "        raise RuntimeError(\"Decay helpers missing in evaluator.py; cannot compute T60/EDT/C50/DR.\")\n",
    "    out = np.zeros((len(wavs_list), 4), dtype=np.float32)\n",
    "    for i, w in enumerate(tqdm(wavs_list, desc=f\"Decays({tag})\", unit=\"wav\")):\n",
    "        w_np = w.detach().cpu().numpy()[None, :]\n",
    "        t60, _ = compute_t60(w_np, w_np, fs=EVAL_FS, advanced=True)\n",
    "        edt, _ = evaluate_edt(w_np, w_np, fs=EVAL_FS)\n",
    "        c50, _ = evaluate_clarity(w_np, w_np, fs=EVAL_FS)\n",
    "        t60 = float(np.atleast_1d(t60)[0]); edt = float(np.atleast_1d(edt)[0]); c50 = float(np.atleast_1d(c50)[0])\n",
    "        dr = _dr_db(w.detach().cpu().numpy())\n",
    "        out[i, :] = [t60, c50, edt, dr]\n",
    "    return out  # (N,4)\n",
    "\n",
    "DEC_Q = _decay_scalars_for_wavs(Q_WAVS, tag=\"Q\")   # (Q,4)\n",
    "DEC_R = _decay_scalars_for_wavs(REF_WAVS, tag=\"R\") # (R,4)\n",
    "\n",
    "# ---------- helper: save a matrix with global-Z ----------\n",
    "def _save_matrix(metric_name: str, M: np.ndarray, meta_base: dict):\n",
    "    Z, mu, sd = _global_z(M)\n",
    "    os.makedirs(DIST_CACHE_DIR, exist_ok=True)\n",
    "    path = os.path.join(DIST_CACHE_DIR, f\"{metric_name}.npz\")\n",
    "    meta = {\n",
    "        **meta_base, \"metric\": metric_name, \"global_mu\": mu, \"global_sd\": sd,\n",
    "        \"edc\": {\n",
    "            \"bins\": EDC_BINS, \"anchor0\": EDC_ANCHOR_ZERO, \"zscore\": EDC_ZSCORE,\n",
    "            \"dist_mode\": EDC_DIST_MODE, \"l2_scale\": EDC_L2_SCALE, \"l1_scale\": EDC_L1_SCALE\n",
    "        } if metric_name == \"EDC\" else None\n",
    "    }\n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        Z=Z.astype(np.float32),\n",
    "        RAW=M.astype(np.float32),\n",
    "        query_ids=np.array(query_ids, dtype=object),\n",
    "        ref_ids=np.array(ref_ids, dtype=object),\n",
    "        meta=json.dumps(meta)\n",
    "    )\n",
    "    print(f\"[saved] {metric_name}: {path} | mu={mu:.6f} sd={sd:.6f}\")\n",
    "\n",
    "# ---------- META ----------\n",
    "meta = {\n",
    "    \"scene_root\": SCENE_ROOT,\n",
    "    \"renders_glob\": RENDERS_GLOB,\n",
    "    \"fs\": EVAL_FS,\n",
    "    \"ref_split\": ref_split_name,\n",
    "    \"device\": DEVICE,\n",
    "    \"notes\": (\"Rows=first K npys, Cols=all refs. RAW + global-Z. \"\n",
    "              \"EDC via per-signal curves + broadcast distances; \"\n",
    "              \"SPL via compute_audio_distance; Decays via broadcasted |ref-query|.\")\n",
    "}\n",
    "\n",
    "# ---------- EDC matrix (K,R) via broadcasting over precomputed curves ----------\n",
    "if \"EDC\" in METRICS:\n",
    "    print(f\"\\n[metric] EDC({EDC_DIST_MODE.upper()}): broadcasting (K={Q}, R={R}, B={EDC_BINS}) ...\")\n",
    "    scale = _edc_scalers()\n",
    "\n",
    "    # Q_EDC: (Q,B), R_EDC: (R,B) —> pairwise distances (Q,R)\n",
    "    # Use memory-efficient trick: process refs in chunks if RAM is tight\n",
    "    M = np.zeros((Q, R), dtype=np.float32)\n",
    "    CH = max(1, 131072 // EDC_BINS)  # rough chunking heuristic to bound RAM\n",
    "    for start in range(0, R, CH):\n",
    "        end = min(start + CH, R)\n",
    "        Rblk = R_EDC[start:end, :]                 # (b,B)\n",
    "        # Expand and compute |Q[:,None,:] - Rblk[None,:,:]|\n",
    "        diff = Q_EDC[:, None, :] - Rblk[None, :, :]   # (Q,b,B)\n",
    "        if EDC_DIST_MODE.lower() == \"l2\":\n",
    "            d = np.linalg.norm(diff, axis=2)          # (Q,b)\n",
    "            if EDC_L2_SCALE == \"sqrtN\":\n",
    "                d = d * scale\n",
    "        elif EDC_DIST_MODE.lower() == \"l1\":\n",
    "            d = np.sum(np.abs(diff), axis=2)          # (Q,b)\n",
    "            if EDC_L1_SCALE == \"mean\":\n",
    "                d = d * scale\n",
    "        else:\n",
    "            raise ValueError(\"EDC_DIST_MODE must be 'l1' or 'l2'\")\n",
    "        M[:, start:end] = d.astype(np.float32)\n",
    "\n",
    "    _save_matrix(\"EDC\", M, meta)\n",
    "\n",
    "# ---------- SPL matrix (K,R) via GPU hub ----------\n",
    "if \"SPL\" in METRICS:\n",
    "    print(f\"\\n[metric] SPL: computing (K={Q}, R={R}) via hub ...\")\n",
    "    M = np.zeros((Q, R), dtype=np.float32)\n",
    "    for qi in tqdm(range(Q), desc=\"SPL per-query\", unit=\"q\"):\n",
    "        q_stft = Q_STFTS[qi].to(DEVICE, non_blocking=True)\n",
    "        row_parts = []\n",
    "        for start in range(0, R, REF_BATCH):\n",
    "            end = min(start + REF_BATCH, R)\n",
    "            ref_stfts_b = torch.stack(REF_STFTS[start:end], dim=0).to(DEVICE, non_blocking=True) # [B,F,T]\n",
    "            stft_blk = torch.cat([q_stft.unsqueeze(0), ref_stfts_b], dim=0)                       # [1+B,F,T]\n",
    "            with torch.no_grad():\n",
    "                D = compute_audio_distance(stft_blk, wavs=None, edc_curves=None,\n",
    "                                           decay_feats=None, metric=\"SPL\", fs=EVAL_FS)\n",
    "            row_parts.append(D[0, 1:].detach().cpu().numpy())\n",
    "            del ref_stfts_b, stft_blk, D\n",
    "            torch.cuda.empty_cache()\n",
    "        M[qi, :] = np.concatenate(row_parts, axis=0)\n",
    "    _save_matrix(\"SPL\", M, meta)\n",
    "\n",
    "# ---------- Decay matrices (K,R) via broadcasted |ref - query| ----------\n",
    "col = {\"T60\":0, \"C50\":1, \"EDT\":2, \"DR\":3}\n",
    "for m in [\"T60\",\"EDT\",\"C50\",\"DR\"]:\n",
    "    if m not in METRICS: continue\n",
    "    print(f\"\\n[metric] {m}: broadcasting (K={Q}, R={R}) ...\")\n",
    "    qi = DEC_Q[:, col[m]].reshape(Q, 1)     # (Q,1)\n",
    "    rj = DEC_R[:, col[m]].reshape(1, R)     # (1,R)\n",
    "    M = np.abs(rj - qi)                      # (Q,R)\n",
    "    _save_matrix(m, M, meta)\n",
    "\n",
    "print(\"\\n[done] All requested metrics cached globally in ./dist_cache/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6356fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active metrics (global-z): ['EDC', 'SPL'] | weights=[0.6, 0.4]\n",
      "[q=eval_000000.npy] TOP3: ['036096', '036106', '036105']  scores=[-1.6228878498077393, -1.5719029903411865, -1.551689624786377]\n",
      "[q=eval_000001.npy] TOP3: ['036438', '036441', '036443']  scores=[-1.5854880809783936, -1.525365948677063, -1.5044968128204346]\n",
      "[q=eval_000002.npy] TOP3: ['029754', '029744', '029737']  scores=[-1.611403465270996, -1.5437387228012085, -1.472914218902588]\n",
      "[q=eval_000003.npy] TOP3: ['010023', '027124', '019589']  scores=[-1.394456386566162, -1.3839342594146729, -1.3662970066070557]\n",
      "[q=eval_000004.npy] TOP3: ['033850', '033871', '033873']  scores=[-1.6325452327728271, -1.5399271249771118, -1.5194473266601562]\n",
      "[info] Will load only 15 referenced STFTs out of R=20000.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f9f58315574c5ab9fee7c8811a18f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load Top-k refs:   0%|          | 0/15 [00:00<?, ?ref/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab62c41f653f4b51aabf6c2f7c80edec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval summary:   0%|          | 0/5 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUMMARY (means over processed files) =====\n",
      "Files processed: 5 | Reference pool: 20000 | Active metrics: ['EDC', 'SPL']\n",
      "                  stft       edc       t60       edt       c50\n",
      "Baseline      0.170139  0.118459  8.222797  0.034875  0.358303\n",
      "Top1          0.213854  0.064593  3.754443  0.019775  0.255973\n",
      "ΔTop1-Base    0.043714 -0.053866 -4.468353 -0.015100 -0.102330\n",
      "Top2          0.213429  0.077529  3.190934  0.031950  0.619630\n",
      "ΔTop2-Base    0.043290 -0.040930 -5.031863 -0.002925  0.261327\n",
      "Top3          0.217224  0.112896  5.471308  0.030350  0.463162\n",
      "ΔTop3-Base    0.047085 -0.005563 -2.751488 -0.004525  0.104859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 2 — Mix cached global-z metrics (first K queries), lazy-evaluate Top-k only\n",
    "# --------------------------------------------------------------------------------\n",
    "# - Loads ./dist_cache/*.npz built by Cell 1\n",
    "# - Restricts to first K queries (same K used in Cell 1)\n",
    "# - Combines any subset of metrics with normalized weights\n",
    "# - Retrieves Top-k per query\n",
    "# - Optional: fast evaluation summary that loads ONLY the Top-k refs (lazy), not all refs\n",
    "\n",
    "import os, sys, glob, json, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------- KNOBS ----------------\n",
    "CACHE_DIR       = \"./dist_cache\"\n",
    "\n",
    "# Choose any subset and weights (renormalized to sum=1 over the active subset present on disk)\n",
    "WEIGHTS         = {\n",
    "    \"EDC\": 0.6,\n",
    "    \"SPL\": 0.4,\n",
    "    # \"T60\": 0.0,\n",
    "    # \"EDT\": 0.0,\n",
    "    # \"C50\": 0.0,\n",
    "    # \"DR\":  0.0,\n",
    "}\n",
    "\n",
    "TOPK            = 3\n",
    "DO_EVAL_SUMMARY = True      # set False to skip evaluator (just prints indices)\n",
    "\n",
    "# IMPORTANT: limit to first K queries (same K used in Cell 1). None => use full cache size.\n",
    "K_LIMIT         = 20\n",
    "\n",
    "# ---------------------------------------\n",
    "# Repo imports for evaluation only\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"/mnt/data\")\n",
    "from evaluator import UnifiedEvaluator\n",
    "try:\n",
    "    from data import RAFDataset\n",
    "except Exception:\n",
    "    raise RuntimeError(\"Cannot import RAFDataset; adjust sys.path to your repo.\")\n",
    "\n",
    "def _load_metric(name: str):\n",
    "    path = os.path.join(CACHE_DIR, f\"{name}.npz\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    pack = np.load(path, allow_pickle=True)\n",
    "    Z = pack[\"Z\"].astype(np.float32)                     # (Q,R)\n",
    "    query_ids = list(pack[\"query_ids\"])\n",
    "    ref_ids   = list(pack[\"ref_ids\"])\n",
    "    meta = json.loads(str(pack[\"meta\"]))\n",
    "    return {\"Z\": Z, \"query_ids\": query_ids, \"ref_ids\": ref_ids, \"meta\": meta}\n",
    "\n",
    "# ---- Resolve active metrics present on disk with positive weight\n",
    "candidates = [m for m,w in WEIGHTS.items() if w > 0.0]\n",
    "present = [m for m in candidates if os.path.exists(os.path.join(CACHE_DIR, f\"{m}.npz\"))]\n",
    "assert len(present) > 0, f\"No active metrics available in {CACHE_DIR} for weights={candidates}\"\n",
    "\n",
    "root = _load_metric(present[0])\n",
    "Z0, query_ids_cache, ref_ids_cache, meta0 = root[\"Z\"], root[\"query_ids\"], root[\"ref_ids\"], root[\"meta\"]\n",
    "Q_full, R = Z0.shape\n",
    "\n",
    "# ---- Enforce K_LIMIT consistently (rows of all matrices)\n",
    "if K_LIMIT is None:\n",
    "    K = Q_full\n",
    "else:\n",
    "    K = int(min(K_LIMIT, Q_full))\n",
    "\n",
    "# Slice helper for matrices and query id list\n",
    "def _take_first_k_rows(M): return M[:K, :] if M.shape[0] >= K else M\n",
    "query_ids = query_ids_cache[:K]\n",
    "\n",
    "# Load remaining matrices and align\n",
    "MATS = {present[0]: _take_first_k_rows(Z0)}\n",
    "for m in present[1:]:\n",
    "    dat = _load_metric(m)\n",
    "    assert dat is not None, f\"{m} vanished from cache.\"\n",
    "    # Consistency checks (ordering must be identical across metrics)\n",
    "    assert dat[\"ref_ids\"] == ref_ids_cache, f\"ref_ids mismatch between metrics ({present[0]} vs {m})\"\n",
    "    assert dat[\"query_ids\"] == query_ids_cache, f\"query_ids mismatch between metrics ({present[0]} vs {m})\"\n",
    "    MATS[m] = _take_first_k_rows(dat[\"Z\"])\n",
    "\n",
    "# Combine with normalized weights over the 'present' keys\n",
    "ws = np.array([WEIGHTS[m] for m in present], dtype=np.float64)\n",
    "ws = ws / (ws.sum() + 1e-12)\n",
    "\n",
    "COMB = np.zeros((K, R), dtype=np.float32)\n",
    "for i, m in enumerate(present):\n",
    "    COMB += float(ws[i]) * MATS[m]\n",
    "\n",
    "# ---- Top-k per query (smallest scores)\n",
    "topk_indices = []\n",
    "topk_ids     = []\n",
    "for qi in range(K):\n",
    "    idx = np.argpartition(COMB[qi], TOPK)[:TOPK]\n",
    "    idx = idx[np.argsort(COMB[qi, idx])]  # stable sort of the k smallest\n",
    "    topk_indices.append(idx.tolist())\n",
    "    topk_ids.append([ref_ids_cache[j] for j in idx.tolist()])\n",
    "\n",
    "print(f\"Active metrics (global-z): {present} | weights={ws.round(6).tolist()}\")\n",
    "for qi in range(K):\n",
    "    print(f\"[q={query_ids[qi]}] TOP{TOPK}: {topk_ids[qi]}  scores={[float(COMB[qi,j]) for j in topk_indices[qi]]}\")\n",
    "\n",
    "# ---------------- Optional: evaluation summary (Baseline vs Top-k) ----------------\n",
    "if DO_EVAL_SUMMARY:\n",
    "    # Pull scene_root and renders_glob from cache meta to avoid mismatches with Cell 1\n",
    "    SCENE_ROOT   = meta0.get(\"scene_root\", \"../NeRAF/data/RAF/FurnishedRoom\")\n",
    "    RENDERS_GLOB = meta0.get(\"renders_glob\", \"../NeRAF/eval_results/furnishedroom/renders/eval_*.npy\")\n",
    "    EVAL_FS      = int(meta0.get(\"fs\", 48000))\n",
    "    DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Build a lookup from filename to full path using the glob from cache meta\n",
    "    render_all = sorted(glob.glob(RENDEERS_GLOB := RENDERS_GLOB))  # store str for messages\n",
    "    name2path = {os.path.basename(p): p for p in render_all}\n",
    "\n",
    "    # Use EXACT same first-K filenames from cache (fallback to position-based if missing)\n",
    "    render_files = []\n",
    "    for nm in query_ids:\n",
    "        if nm in name2path:\n",
    "            render_files.append(name2path[nm])\n",
    "        else:\n",
    "            print(f\"[warn] {nm} not found under {RENDEERS_GLOB}; falling back to glob order.\")\n",
    "    if len(render_files) < K:\n",
    "        render_files = sorted(glob.glob(RENDEERS_GLOB))[:K]\n",
    "\n",
    "    # ===== SPEED FIX: only load the references we actually need (Top-k set) =====\n",
    "    needed_ref_idx = sorted(set(j for row in topk_indices for j in row))  # unique global indices\n",
    "    idxmap = {j:i for i,j in enumerate(needed_ref_idx)}  # map global gidx -> compact 0..M-1\n",
    "    print(f\"[info] Will load only {len(needed_ref_idx)} referenced STFTs out of R={R}.\")\n",
    "\n",
    "    ds_ref = RAFDataset(scene_root=SCENE_ROOT, split=meta0.get(\"ref_split\",\"reference\"),\n",
    "                        model_kind=\"neraf\", sample_rate=EVAL_FS, dataset_mode=\"full\")\n",
    "    id2idx_ref = ds_ref.id2idx\n",
    "\n",
    "    # Load STFTs just for needed refs (CPU, lazily)\n",
    "    SMALL_REF_STFTS = {}\n",
    "    for gidx in tqdm(needed_ref_idx, desc=\"Load Top-k refs\", unit=\"ref\"):\n",
    "        sid = ref_ids_cache[gidx]\n",
    "        it = ds_ref[id2idx_ref[sid]]\n",
    "        SMALL_REF_STFTS[gidx] = it[\"stft\"].squeeze(0).contiguous()\n",
    "\n",
    "    evaluator = UnifiedEvaluator(fs=EVAL_FS, edc_bins=60, edc_dist=\"l2\")\n",
    "\n",
    "    agg_keys = [\"stft\",\"edc\",\"t60\",\"edt\",\"c50\"]\n",
    "    def _zero_agg():\n",
    "        return {k:[0.0,0] for k in (\n",
    "            [\"base_\"+x for x in agg_keys] +\n",
    "            sum([[f\"t{i}_{x}\", f\"d{i}_{x}\"] for i in (1,2,3) for x in agg_keys], [])\n",
    "        )}\n",
    "\n",
    "    def _add(agg, k, v):\n",
    "        vv = float(v)\n",
    "        if math.isfinite(vv):\n",
    "            agg[k][0] += vv; agg[k][1] += 1\n",
    "\n",
    "    def _mean(pair): s,n = pair; return (s/max(n,1)) if n>0 else float(\"nan\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_pair(stft_a_2d, stft_b_2d):\n",
    "        return evaluator.evaluate(\n",
    "            stft_a_2d.view(1,1,*stft_a_2d.shape).to(DEVICE),\n",
    "            stft_b_2d.view(1,1,*stft_b_2d.shape).to(DEVICE)\n",
    "        )\n",
    "\n",
    "    # Evaluate baseline and only Top-k refs\n",
    "    agg = _zero_agg()\n",
    "    for qi, fp in enumerate(tqdm(render_files[:K], desc=\"Eval summary\", unit=\"q\")):\n",
    "        pack = np.load(fp, allow_pickle=True).item()\n",
    "        stft_gt  = torch.from_numpy(pack[\"data\"]).float().squeeze(0)\n",
    "        stft_pr  = torch.from_numpy(pack[\"pred_stft\"]).float().squeeze(0)\n",
    "\n",
    "        base = _eval_pair(stft_pr, stft_gt)\n",
    "        for k in agg_keys: _add(agg, f\"base_{k}\", base[k])\n",
    "\n",
    "        # Top-k from combined matrix (already aligned to cache ref_ids)\n",
    "        sel = topk_indices[qi]\n",
    "        for rank, tag in enumerate((\"t1\",\"t2\",\"t3\"), start=1):\n",
    "            if len(sel) < rank: continue\n",
    "            gidx = sel[rank-1]\n",
    "            ref_st = SMALL_REF_STFTS[gidx].to(DEVICE)\n",
    "            top = _eval_pair(ref_st, stft_gt)\n",
    "            for k in agg_keys:\n",
    "                _add(agg, f\"{tag}_{k}\", top[k])\n",
    "                _add(agg, f\"d{rank}_{k}\", float(top[k]-base[k]))\n",
    "\n",
    "    # Print compact summary\n",
    "    print(\"\\n===== SUMMARY (means over processed files) =====\")\n",
    "    print(f\"Files processed: {K} | Reference pool: {R} | Active metrics: {present}\")\n",
    "    print(f\"{'':12s}{'stft':>10s}{'edc':>10s}{'t60':>10s}{'edt':>10s}{'c50':>10s}\")\n",
    "\n",
    "    def _line(title, keys):\n",
    "        vals = [_mean(agg[k]) for k in keys]\n",
    "        print(f\"{title:<12s}\" + \"\".join([f\"{v:10.6f}\" for v in vals]))\n",
    "\n",
    "    _line(\"Baseline\",  [f\"base_{k}\" for k in agg_keys])\n",
    "    for r, tag in enumerate((\"t1\",\"t2\",\"t3\"), start=1):\n",
    "        _line(f\"Top{r}\",       [f\"{tag}_{k}\" for k in agg_keys])\n",
    "        _line(f\"ΔTop{r}-Base\", [f\"d{r}_{k}\"  for k in agg_keys])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3bd7b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load queries: 100%|██████████| 50/50 [00:00<00:00, 2292.42it/s]\n",
      "Load refs[train]: 100%|██████████| 31305/31305 [01:00<00:00, 519.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Queries: K=50, STFT=(513,60), Refs: R=31305, WAV_T=15360\n",
      "Precompute EDC/decays ...\n",
      "Pairwise STFT L2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STFT: 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] STFT     -> ./dist_cache/STFT.npz | shape=(50, 31305)\n",
      "Pairwise EDC (L1/L2) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EDC_L1: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] EDC_L1   -> ./dist_cache/EDC_L1.npz | shape=(50, 31305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EDC_L2: 100%|██████████| 1/1 [00:00<00:00, 24.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] EDC_L2   -> ./dist_cache/EDC_L2.npz | shape=(50, 31305)\n",
      "Decay |Δ| matrices ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEC_T60: 100%|██████████| 1/1 [00:00<00:00, 83.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] DEC_T60  -> ./dist_cache/DEC_T60.npz | shape=(50, 31305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEC_EDT: 100%|██████████| 1/1 [00:00<00:00, 53.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] DEC_EDT  -> ./dist_cache/DEC_EDT.npz | shape=(50, 31305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEC_C50: 100%|██████████| 1/1 [00:00<00:00, 87.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] DEC_C50  -> ./dist_cache/DEC_C50.npz | shape=(50, 31305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEC_DR: 100%|██████████| 1/1 [00:00<00:00, 80.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] DEC_DR   -> ./dist_cache/DEC_DR.npz | shape=(50, 31305)\n",
      "All distance caches built.\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1: Build per-metric distance caches (Numba + tqdm + chunking) ===\n",
    "import os, glob, json, math, numpy as np, torch, numba\n",
    "from tqdm import tqdm\n",
    "from data import RAFDataset\n",
    "\n",
    "# ---------------- KNOBS ----------------\n",
    "SCENE_ROOT  = \"../NeRAF/data/RAF/FurnishedRoom\"\n",
    "RENDS_GLOB  = \"../NeRAF/eval_results/furnishedroom/renders/eval_*.npy\"\n",
    "REF_SPLIT   = \"reference\"       # reference bank\n",
    "K_QUERIES   = 50            # first-K npy files; -1 => use all\n",
    "EDC_BINS    = 60\n",
    "DIST_DIR    = \"./dist_cache\"\n",
    "os.makedirs(DIST_DIR, exist_ok=True)\n",
    "\n",
    "# Chunk sizes for progress + lower peak RAM (tune if needed)\n",
    "Q_CHUNK = 64     # rows per chunk (queries)\n",
    "R_CHUNK = None   # keep None (we compute vs. all refs per q-chunk); set to int for 2D tiling if memory is tight\n",
    "\n",
    "# ---------------- Numba kernels ----------------\n",
    "@numba.njit(fastmath=True)\n",
    "def _zscore_rowwise(M):\n",
    "    Q,R = M.shape\n",
    "    Z = np.empty_like(M)\n",
    "    for i in range(Q):\n",
    "        mu = np.mean(M[i]); sd = np.std(M[i]) + 1e-12\n",
    "        Z[i] = (M[i] - mu) / sd\n",
    "    return Z\n",
    "\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def _pairwise_l1(A, B):\n",
    "    Q, R = A.shape[0], B.shape[0]\n",
    "    D = np.empty((Q, R), np.float64)\n",
    "    for i in numba.prange(Q):\n",
    "        for j in range(R):\n",
    "            D[i, j] = np.sum(np.abs(A[i] - B[j]))\n",
    "    return D\n",
    "\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def _pairwise_l2(A, B):\n",
    "    Q, R = A.shape[0], B.shape[0]\n",
    "    D = np.empty((Q, R), np.float64)\n",
    "    for i in numba.prange(Q):\n",
    "        for j in range(R):\n",
    "            diff = A[i] - B[j]\n",
    "            D[i, j] = math.sqrt(np.sum(diff * diff))\n",
    "    return D\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def _edc_db(wav, bins=60):\n",
    "    e = wav * wav\n",
    "    edc = np.cumsum(e[::-1])[::-1]\n",
    "    edc = edc / (edc[0] + 1e-12)\n",
    "    edc_db = 10.0 * np.log10(edc + 1e-12)\n",
    "    idx = np.linspace(0, edc_db.size - 1, bins).astype(np.int64)\n",
    "    return edc_db[idx]\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def _decay_from_edc(edc_db):\n",
    "    # EDT via -10 dB crossing, T60 via -60 dB crossing (fallbacks if not reached)\n",
    "    n = edc_db.size; s0 = edc_db[0]\n",
    "    edt_f = -1; t60_f = -1\n",
    "    for i in range(n):\n",
    "        drop = s0 - edc_db[i]\n",
    "        if edt_f < 0 and drop >= 10.0: edt_f = i\n",
    "        if t60_f < 0 and drop >= 60.0:\n",
    "            t60_f = i; break\n",
    "    if t60_f < 0: t60_f = n\n",
    "    if edt_f < 0: edt_f = min(n, int(t60_f / 6))\n",
    "    edc_lin = 10.0 ** (edc_db / 10.0)\n",
    "    split = n // 12  # crude ~50ms if ~0.32s total at 60 bins\n",
    "    c50_db = 10.0 * np.log10((np.sum(edc_lin[:split]) + 1e-12) / (np.sum(edc_lin[split:]) + 1e-12))\n",
    "    dr_db = 10.0 * np.log10((np.max(edc_lin) + 1e-12) / (np.mean(edc_lin) + 1e-12))\n",
    "    # return T60 (frames), EDT (~ms proxy via *6), C50 (dB), DR (dB)\n",
    "    return float(t60_f), float(edt_f * 6), float(c50_db), float(dr_db)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=True)\n",
    "def _batch_edc_decay(wavs, bins=60):\n",
    "    N = wavs.shape[0]\n",
    "    E = np.empty((N, bins), np.float64)\n",
    "    D = np.empty((N, 4),   np.float64)  # T60, EDT, C50, DR\n",
    "    for i in numba.prange(N):\n",
    "        edc = _edc_db(wavs[i], bins)\n",
    "        E[i] = edc\n",
    "        t60, edt, c50, dr = _decay_from_edc(edc)\n",
    "        D[i, 0] = t60; D[i, 1] = edt; D[i, 2] = c50; D[i, 3] = dr\n",
    "    return E, D\n",
    "\n",
    "def _save_small(name, Z, qids, rids):\n",
    "    out = {\n",
    "        \"Z\": Z.astype(np.float32),\n",
    "        \"query_ids\": np.array(qids, dtype=object),\n",
    "        \"ref_ids\":   np.array(rids, dtype=object),\n",
    "        \"meta\": json.dumps({\"metric\": name, \"norm\": \"row_zscore\",\n",
    "                            \"bins\": EDC_BINS, \"K\": int(Z.shape[0]), \"R\": int(Z.shape[1])})\n",
    "    }\n",
    "    path = os.path.join(DIST_DIR, f\"{name}.npz\")\n",
    "    np.savez_compressed(path, **out)\n",
    "    print(f\"[saved] {name:8s} -> {path} | shape={Z.shape}\")\n",
    "\n",
    "def _iter_chunks(n, chunk):\n",
    "    if chunk is None or chunk >= n:\n",
    "        yield 0, n\n",
    "    else:\n",
    "        for s in range(0, n, chunk):\n",
    "            e = min(s + chunk, n)\n",
    "            yield s, e\n",
    "\n",
    "# ---------------- Load queries/refs ----------------\n",
    "files_all = sorted(glob.glob(RENDS_GLOB))\n",
    "assert files_all, f\"No npy renders under {RENDS_GLOB}\"\n",
    "files = files_all if (K_QUERIES is None or K_QUERIES <= 0) else files_all[:K_QUERIES]\n",
    "query_ids = [os.path.basename(f) for f in files]\n",
    "\n",
    "Q_STFT, Q_WAV = [], []\n",
    "for f in tqdm(files, desc=\"Load queries\"):\n",
    "    d = np.load(f, allow_pickle=True).item()\n",
    "    # If your npy keys differ, adjust here\n",
    "    q_stft = torch.from_numpy(d[\"data\"]).float().squeeze(0).numpy().astype(np.float64)\n",
    "    q_wav  = torch.from_numpy(d[\"waveform\"]).float().squeeze(0).numpy().astype(np.float64)\n",
    "    Q_STFT.append(q_stft)\n",
    "    Q_WAV.append(q_wav)\n",
    "Q_STFT = np.stack(Q_STFT)                         # (K, F, T)\n",
    "Q_WAV  = np.stack(Q_WAV)                          # (K, Tw)\n",
    "\n",
    "ds_ref = RAFDataset(scene_root=SCENE_ROOT, split=REF_SPLIT,\n",
    "                    model_kind=\"neraf\", sample_rate=48000, dataset_mode=\"full\")\n",
    "ref_ids = list(ds_ref.ids)\n",
    "R = len(ref_ids)\n",
    "\n",
    "REF_STFT, REF_WAV = [], []\n",
    "for sid in tqdm(ref_ids, desc=f\"Load refs[{REF_SPLIT}]\"):\n",
    "    it = ds_ref[ds_ref.id2idx[sid]]\n",
    "    REF_STFT.append(it[\"stft\"].squeeze(0).numpy().astype(np.float64))\n",
    "    REF_WAV.append(it[\"wav\"].squeeze(0).numpy().astype(np.float64))\n",
    "REF_STFT = np.stack(REF_STFT)                     # (R, F, T)\n",
    "REF_WAV  = np.stack(REF_WAV)                      # (R, Tw)\n",
    "\n",
    "K, F, T = Q_STFT.shape\n",
    "print(f\"[info] Queries: K={K}, STFT=({F},{T}), Refs: R={R}, WAV_T={REF_WAV.shape[1]}\")\n",
    "\n",
    "# ---------------- Precompute EDC & decays ----------------\n",
    "print(\"Precompute EDC/decays ...\")\n",
    "Q_EDC, Q_DEC = _batch_edc_decay(Q_WAV, bins=EDC_BINS)    # Q_DEC: (K, 4) = [T60, EDT, C50, DR]\n",
    "R_EDC, R_DEC = _batch_edc_decay(REF_WAV, bins=EDC_BINS)  # R_DEC: (R, 4)\n",
    "\n",
    "# ---------------- Helper: chunked pairwise with tqdm ----------------\n",
    "def _chunked_rowwise_zscore_and_save(name, row_builder):\n",
    "    \"\"\"\n",
    "    row_builder(i0, i1) -> np.ndarray[(i1-i0), R] of float64 distances\n",
    "    We z-score per row and write into full Z.\n",
    "    \"\"\"\n",
    "    Z = np.empty((K, R), dtype=np.float32)\n",
    "    num_chunks = sum(1 for _ in _iter_chunks(K, Q_CHUNK))\n",
    "    pbar = tqdm(total=num_chunks, desc=name)\n",
    "    for i0, i1 in _iter_chunks(K, Q_CHUNK):\n",
    "        D_chunk = row_builder(i0, i1)               # float64 (rows, R)\n",
    "        Z_chunk = _zscore_rowwise(D_chunk)\n",
    "        Z[i0:i1] = Z_chunk.astype(np.float32)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    _save_small(name, Z, query_ids, ref_ids)\n",
    "\n",
    "# ---------------- STFT distances (L2 over log-mag) ----------------\n",
    "print(\"Pairwise STFT L2 ...\")\n",
    "Q_STFT_F = Q_STFT.reshape(K, -1)                  # (K, F*T)\n",
    "R_STFT_F = REF_STFT.reshape(R, -1)                # (R, F*T)\n",
    "def _stft_row_builder(i0, i1):\n",
    "    return _pairwise_l2(Q_STFT_F[i0:i1], R_STFT_F)\n",
    "_chunked_rowwise_zscore_and_save(\"STFT\", _stft_row_builder)\n",
    "\n",
    "# ---------------- EDC distances (L1 + L2) ----------------\n",
    "print(\"Pairwise EDC (L1/L2) ...\")\n",
    "\n",
    "def _edc_l1_row_builder(i0, i1):\n",
    "    return _pairwise_l1(Q_EDC[i0:i1], R_EDC)\n",
    "_chunked_rowwise_zscore_and_save(\"EDC_L1\", _edc_l1_row_builder)\n",
    "\n",
    "def _edc_l2_row_builder(i0, i1):\n",
    "    return _pairwise_l2(Q_EDC[i0:i1], R_EDC)\n",
    "_chunked_rowwise_zscore_and_save(\"EDC_L2\", _edc_l2_row_builder)\n",
    "\n",
    "# ---------------- Decay distances (each metric separate) ----------------\n",
    "print(\"Decay |Δ| matrices ...\")\n",
    "names = [\"DEC_T60\", \"DEC_EDT\", \"DEC_C50\", \"DEC_DR\"]\n",
    "for c, name in enumerate(names):\n",
    "    def _decay_row_builder(i0, i1, c=c):\n",
    "        q = Q_DEC[i0:i1, c].reshape(-1, 1)   # (chunk,1)\n",
    "        r = R_DEC[:, c].reshape(1, -1)       # (1,R)\n",
    "        return np.abs(q - r)                 # (chunk,R)\n",
    "    _chunked_rowwise_zscore_and_save(name, _decay_row_builder)\n",
    "\n",
    "print(\"All distance caches built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active metrics: ['STFT', 'EDC_L1'] | weights (norm) = [0.5, 0.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summary eval: 100%|██████████| 50/50 [00:52<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUMMARY (means over processed files) =====\n",
      "Files processed: 50 | Reference pool: 31305 | Active metrics: ['STFT', 'EDC_L1']\n",
      "                    stft       edc       t60       edt       c50\n",
      "Baseline        0.177603  0.123689  6.534929  0.017785  0.585330\n",
      "Top1            0.213988  0.110675  4.692591  0.012855  0.409413\n",
      "ΔTop1-Base      0.036386 -0.013014 -1.842338 -0.004930 -0.175917\n",
      "Top2            0.218874  0.100802  5.119940  0.012923  0.432830\n",
      "ΔTop2-Base      0.041272 -0.022888 -1.414988 -0.004862 -0.152501\n",
      "Top3            0.221983  0.114606  5.694140  0.020872  0.564453\n",
      "ΔTop3-Base      0.044381 -0.009084 -0.840789  0.003088 -0.020877\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: Mix metrics, Top-K retrieval, summary ===========================\n",
    "import os, json, glob, math, numpy as np, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data import RAFDataset\n",
    "from evaluator import UnifiedEvaluator  # uses your Griffin-Lim/RAF helpers\n",
    "\n",
    "DIST_DIR   = \"./dist_cache\"\n",
    "TOPK       = 3\n",
    "# Example: mix T60 & EDT only → set both to 0.5; set unused to 0.0\n",
    "WEIGHTS = {\n",
    "    \"STFT\":   0.5,\n",
    "    \"EDC_L1\": 0.5,\n",
    "    \"EDC_L2\": 0.0,\n",
    "    \"DEC_T60\":0.0,\n",
    "    \"DEC_EDT\":0.0,\n",
    "    \"DEC_C50\":0.0,\n",
    "    \"DEC_DR\": 0.0,\n",
    "}\n",
    "\n",
    "def _load_npz(name):\n",
    "    p = os.path.join(DIST_DIR, f\"{name}.npz\")\n",
    "    if not os.path.exists(p): return None\n",
    "    d = np.load(p, allow_pickle=True)\n",
    "    return d[\"Z\"].astype(np.float64), list(d[\"query_ids\"]), list(d[\"ref_ids\"]), json.loads(str(d[\"meta\"]))\n",
    "\n",
    "# ---- Load available metrics\n",
    "present = []\n",
    "stacks  = []\n",
    "query_ids = ref_ids = None\n",
    "for m, w in WEIGHTS.items():\n",
    "    if w <= 0: continue\n",
    "    obj = _load_npz(m)\n",
    "    if obj is None: continue\n",
    "    Z, qids, rids, meta = obj\n",
    "    if query_ids is None:\n",
    "        query_ids, ref_ids = qids, rids\n",
    "    else:\n",
    "        assert qids == query_ids and rids == ref_ids, f\"ID mismatch in {m}\"\n",
    "    present.append(m); stacks.append(Z)\n",
    "\n",
    "assert present, \"No active metrics found in cache.\"\n",
    "\n",
    "ws = np.array([WEIGHTS[m] for m in present], dtype=np.float64)\n",
    "ws = ws / (ws.sum() + 1e-12)\n",
    "\n",
    "# ---- Combine\n",
    "COMB = np.zeros_like(stacks[0])\n",
    "for w, Z in zip(ws, stacks): COMB += w * Z\n",
    "\n",
    "# ---- Top-k indices\n",
    "topk_idx = []\n",
    "for i in range(COMB.shape[0]):\n",
    "    row = COMB[i]\n",
    "    idx = np.argpartition(row, TOPK)[:TOPK]\n",
    "    idx = idx[np.argsort(row[idx])]\n",
    "    topk_idx.append(idx.tolist())\n",
    "\n",
    "print(f\"Active metrics: {present} | weights (norm) = {ws.round(3).tolist()}\")\n",
    "\n",
    "# ==== Evaluation Summary (Baseline vs Top-1/2/3) ============================\n",
    "# Use the same scene/splits as Cell 1 (read from meta of first metric)\n",
    "meta0 = json.loads(str(np.load(os.path.join(DIST_DIR, f\"{present[0]}.npz\"), allow_pickle=True)[\"meta\"]))\n",
    "SCENE_ROOT = \"../NeRAF/data/RAF/FurnishedRoom\"\n",
    "RENDS_GLOB = \"../NeRAF/eval_results/furnishedroom/renders/eval_*.npy\"\n",
    "K = int(meta0.get(\"K\", len(query_ids)))\n",
    "R = int(meta0.get(\"R\", len(ref_ids)))\n",
    "\n",
    "# Build robust item fetcher across splits to avoid KeyErrors\n",
    "_ds_cache = {}\n",
    "def _get_ds(split):\n",
    "    if split not in _ds_cache:\n",
    "        _ds_cache[split] = RAFDataset(scene_root=SCENE_ROOT, split=split,\n",
    "                                      model_kind=\"neraf\", sample_rate=48000, dataset_mode=\"full\")\n",
    "    return _ds_cache[split]\n",
    "\n",
    "def fetch_ref_stft_by_id(sid):\n",
    "    for split in (\"test\",\"reference\",\"train\"):\n",
    "        ds = _get_ds(split)\n",
    "        if sid in ds.id2idx:\n",
    "            it = ds[ds.id2idx[sid]]\n",
    "            return it[\"stft\"].squeeze(0).numpy().astype(np.float64)\n",
    "    raise KeyError(f\"Ref id {sid} not found in any split\")\n",
    "\n",
    "# Resolve npy paths for the same first-K queries\n",
    "render_all = sorted(glob.glob(RENDS_GLOB))\n",
    "name2path  = {os.path.basename(p): p for p in render_all}\n",
    "render_files = []\n",
    "for nm in query_ids[:K]:\n",
    "    if nm in name2path: render_files.append(name2path[nm])\n",
    "\n",
    "evaluator = UnifiedEvaluator(fs=48000, edc_bins=60, edc_dist=\"l2\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "agg_keys = [\"stft\",\"edc\",\"t60\",\"edt\",\"c50\"]\n",
    "def _zero():\n",
    "    return {k:[0.0,0] for k in (\n",
    "        [\"base_\"+x for x in agg_keys] +\n",
    "        sum([[f\"t{i}_{x}\", f\"d{i}_{x}\"] for i in (1,2,3) for x in agg_keys], [])\n",
    "    )}\n",
    "\n",
    "def _add(agg,k,v):\n",
    "    vv = float(v)\n",
    "    if math.isfinite(vv): agg[k][0]+=vv; agg[k][1]+=1\n",
    "\n",
    "def _mean(pair): s,n = pair; return (s/max(n,1)) if n>0 else float(\"nan\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def _eval_pair(stft_a_2d, stft_b_2d):\n",
    "    # inputs: (F,T) float64 -> torch (1,1,F,T)\n",
    "    A = torch.from_numpy(stft_a_2d).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    B = torch.from_numpy(stft_b_2d).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    out = evaluator.evaluate(A, B)\n",
    "    return {k: float(out[k]) for k in agg_keys}\n",
    "\n",
    "# Evaluate\n",
    "agg = _zero()\n",
    "for qi, qname in enumerate(tqdm(query_ids[:K], desc=\"Summary eval\")):\n",
    "    pack = np.load(name2path[qname], allow_pickle=True).item()\n",
    "    stft_gt  = pack[\"data\"].astype(np.float32).squeeze(0)       # (F,T)\n",
    "    stft_pr  = pack[\"pred_stft\"].astype(np.float32).squeeze(0)\n",
    "\n",
    "    base = _eval_pair(stft_pr, stft_gt)\n",
    "    for k in agg_keys: _add(agg, f\"base_{k}\", base[k])\n",
    "\n",
    "    sel = topk_idx[qi]\n",
    "    for rank, tag in enumerate((\"t1\",\"t2\",\"t3\"), start=1):\n",
    "        if len(sel) < rank: continue\n",
    "        gidx = sel[rank-1]\n",
    "        sid  = ref_ids[gidx]\n",
    "        try:\n",
    "            ref_st = fetch_ref_stft_by_id(sid)\n",
    "        except KeyError:\n",
    "            # skip missing refs silently; this avoids hard failure if splits differ\n",
    "            continue\n",
    "        top = _eval_pair(ref_st, stft_gt)\n",
    "        for k in agg_keys:\n",
    "            _add(agg, f\"{tag}_{k}\", top[k])\n",
    "            _add(agg, f\"d{rank}_{k}\", float(top[k]-base[k]))\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n===== SUMMARY (means over processed files) =====\")\n",
    "print(f\"Files processed: {K} | Reference pool: {R} | Active metrics: {present}\")\n",
    "print(f\"{'':14s}{'stft':>10s}{'edc':>10s}{'t60':>10s}{'edt':>10s}{'c50':>10s}\")\n",
    "\n",
    "def _line(title, keys):\n",
    "    vals = [_mean(agg[k]) for k in keys]\n",
    "    print(f\"{title:<14s}\" + \"\".join([f\"{v:10.6f}\" for v in vals]))\n",
    "\n",
    "_line(\"Baseline\",  [f\"base_{k}\" for k in agg_keys])\n",
    "for r, tag in enumerate((\"t1\",\"t2\",\"t3\"), start=1):\n",
    "    _line(f\"Top{r}\",       [f\"{tag}_{k}\" for k in agg_keys])\n",
    "    _line(f\"ΔTop{r}-Base\", [f\"d{r}_{k}\"  for k in agg_keys])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ce98f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading reference set…\n",
      "[info] #Queries=47484 (dry-run=OFF), #Refs=37987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load refs (stft+wav): 100%|██████████| 37987/37987 [28:28<00:00, 22.24it/s]  \n",
      "Load queries: 100%|██████████| 47484/47484 [07:45<00:00, 101.99it/s]\n",
      "Compute & rank (chunked): 100%|██████████| 186/186 [40:34<00:00, 13.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] Top-10 retrieval JSON -> ./references_empty.json\n",
      "Example item: ('026251', ['026256', '005596', '005587', '026258', '026244', '026253', '026261', '026870', '005585', '005589'])\n"
     ]
    }
   ],
   "source": [
    "# === CELL: Build top-10 retrieval JSON (CPU + Numba, per-row zscore) ===\n",
    "import os, json, math, glob\n",
    "import numpy as np\n",
    "import torch, numba\n",
    "from tqdm import tqdm\n",
    "from data import RAFDataset\n",
    "\n",
    "# ---------------- KNOBS ----------------\n",
    "SCENE_ROOT      = \"../NeRAF/data/RAF/EmptyRoom\"\n",
    "SPLIT_JSON_PATH = os.path.join(SCENE_ROOT, \"metadata\", \"data-split.json\")\n",
    "OUT_JSON_PATH   = \"./references_empty.json\"\n",
    "\n",
    "REF_SPLIT   = \"reference\"   # which split is your reference bank\n",
    "EDC_BINS    = 60            # downsample EDC curve to this many bins\n",
    "EDC_METRIC  = \"L1\"          # \"L2\" or \"L1\" on EDC vectors\n",
    "W_EDC, W_STFT = 0.6, 0.4    # weights\n",
    "\n",
    "# Performance / memory knobs\n",
    "Q_CHUNK     = 256            # process queries in batches of this many for distance computation\n",
    "DRY_RUN_K   = None            # e.g., 10 to sanity-check; set to None to use ALL queries\n",
    "\n",
    "# ---------------- Helpers: Numba kernels ----------------\n",
    "@numba.njit(fastmath=True)\n",
    "def _zscore_rowwise(M):\n",
    "    Q, R = M.shape\n",
    "    Z = np.empty_like(M)\n",
    "    for i in range(Q):\n",
    "        mu = 0.0\n",
    "        for j in range(R):\n",
    "            mu += M[i, j]\n",
    "        mu /= R\n",
    "        sd = 0.0\n",
    "        for j in range(R):\n",
    "            d = M[i, j] - mu\n",
    "            sd += d * d\n",
    "        sd = math.sqrt(sd / (R + 1e-12)) + 1e-12\n",
    "        for j in range(R):\n",
    "            Z[i, j] = (M[i, j] - mu) / sd\n",
    "    return Z\n",
    "\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def _pairwise_l1(A, B):\n",
    "    Q, R = A.shape[0], B.shape[0]\n",
    "    D = np.empty((Q, R), np.float64)\n",
    "    for i in numba.prange(Q):\n",
    "        for j in range(R):\n",
    "            s = 0.0\n",
    "            ai = A[i]\n",
    "            bj = B[j]\n",
    "            for k in range(ai.size):\n",
    "                s += abs(ai[k] - bj[k])\n",
    "            D[i, j] = s\n",
    "    return D\n",
    "\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def _pairwise_l2(A, B):\n",
    "    Q, R = A.shape[0], B.shape[0]\n",
    "    D = np.empty((Q, R), np.float64)\n",
    "    for i in numba.prange(Q):\n",
    "        for j in range(R):\n",
    "            s = 0.0\n",
    "            ai = A[i]\n",
    "            bj = B[j]\n",
    "            for k in range(ai.size):\n",
    "                d = ai[k] - bj[k]\n",
    "                s += d * d\n",
    "            D[i, j] = math.sqrt(s)\n",
    "    return D\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def _edc_db(wav, bins=60):\n",
    "    # Schroeder integral (linear energy -> cumulative backward), normalize, convert to dB, then sample to bins\n",
    "    e = wav * wav\n",
    "    edc = np.cumsum(e[::-1])[::-1]\n",
    "    edc = edc / (edc[0] + 1e-12)\n",
    "    edc_db = 10.0 * np.log10(edc + 1e-12)\n",
    "    idx = np.linspace(0, edc_db.size - 1, bins).astype(np.int64)\n",
    "    return edc_db[idx]\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=True)\n",
    "def _batch_edc_db(wavs, bins=60):\n",
    "    N = wavs.shape[0]\n",
    "    E = np.empty((N, bins), np.float64)\n",
    "    for i in numba.prange(N):\n",
    "        E[i] = _edc_db(wavs[i], bins)\n",
    "    return E\n",
    "\n",
    "# ---------------- Load splits & datasets ----------------\n",
    "# Build query id list = all splits except REF_SPLIT\n",
    "with open(SPLIT_JSON_PATH, \"r\") as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "query_id_list = []\n",
    "query_splits = []\n",
    "for k, v in splits.items():\n",
    "    if k.lower() == REF_SPLIT.lower():\n",
    "        continue\n",
    "    # v can be a list or nested under [0] depending on your writer – normalize\n",
    "    ids_k = v[0] if (isinstance(v, list) and len(v) > 0 and isinstance(v[0], list)) else v\n",
    "    for sid in ids_k:\n",
    "        query_id_list.append(sid)\n",
    "        query_splits.append(k)\n",
    "\n",
    "# Build reference dataset ids\n",
    "ref_ids_src = splits[REF_SPLIT]\n",
    "ref_ids = ref_ids_src[0] if (isinstance(ref_ids_src, list) and len(ref_ids_src) > 0 and isinstance(ref_ids_src[0], list)) else ref_ids_src\n",
    "ref_ids = list(ref_ids)\n",
    "\n",
    "# For data access we use RAFDataset to read tensors\n",
    "# We'll instantiate per split to reuse its indexing and transforms\n",
    "def _make_ds(split):\n",
    "    return RAFDataset(scene_root=SCENE_ROOT, split=split, model_kind=\"neraf\",\n",
    "                      sample_rate=48000, dataset_mode=\"full\")\n",
    "\n",
    "print(\"[info] Loading reference set…\")\n",
    "ds_ref = _make_ds(REF_SPLIT)\n",
    "id2idx_ref = ds_ref.id2idx\n",
    "\n",
    "# Map query ids to a dataset: simplest is to route per-split through a cache of RAFDataset objects\n",
    "ds_cache = {}\n",
    "def _get_ds_for_split(split_name):\n",
    "    if split_name not in ds_cache:\n",
    "        ds_cache[split_name] = _make_ds(split_name)\n",
    "    return ds_cache[split_name]\n",
    "\n",
    "# Optional dry-run trimming\n",
    "if isinstance(DRY_RUN_K, int) and DRY_RUN_K > 0:\n",
    "    query_id_list = query_id_list[:DRY_RUN_K]\n",
    "    query_splits = query_splits[:DRY_RUN_K]\n",
    "print(f\"[info] #Queries={len(query_id_list)} (dry-run={DRY_RUN_K if DRY_RUN_K else 'OFF'}), #Refs={len(ref_ids)}\")\n",
    "\n",
    "# ---------------- Collect features to RAM ----------------\n",
    "# Refs: STFT (F,T) -> flatten, WAV -> EDC\n",
    "REF_STFT, REF_WAV = [], []\n",
    "for sid in tqdm(ref_ids, desc=\"Load refs (stft+wav)\"):\n",
    "    it = ds_ref[id2idx_ref[sid]]\n",
    "    REF_STFT.append(it[\"stft\"].squeeze(0).numpy().astype(np.float64))\n",
    "    REF_WAV.append(it[\"wav\"].squeeze(0).numpy().astype(np.float64))\n",
    "REF_STFT = np.stack(REF_STFT)              # (R, F, T)\n",
    "R, F, T = REF_STFT.shape\n",
    "REF_WAV  = np.stack(REF_WAV)               # (R, Tw)\n",
    "REF_STFT_F = REF_STFT.reshape(R, F*T)      # (R, F*T)\n",
    "REF_EDC    = _batch_edc_db(REF_WAV, bins=EDC_BINS)  # (R, EDC_BINS)\n",
    "\n",
    "# Queries: gather STFT/WAV in the original per-split datasets\n",
    "Q_STFT_F = np.empty((len(query_id_list), F*T), dtype=np.float64)\n",
    "Q_EDC    = np.empty((len(query_id_list), EDC_BINS), dtype=np.float64)\n",
    "Q_IDS    = []\n",
    "for qi, (sid, split_name) in enumerate(tqdm(zip(query_id_list, query_splits),\n",
    "                                            total=len(query_id_list), desc=\"Load queries\")):\n",
    "    ds = _get_ds_for_split(split_name)\n",
    "    it = ds[ds.id2idx[sid]]\n",
    "    st = it[\"stft\"].squeeze(0).numpy().astype(np.float64).reshape(-1)  # (F*T,)\n",
    "    Q_STFT_F[qi] = st\n",
    "    wav = it[\"wav\"].squeeze(0).numpy().astype(np.float64)\n",
    "    Q_EDC[qi] = _edc_db(wav, bins=EDC_BINS)\n",
    "    Q_IDS.append(sid)\n",
    "\n",
    "# ---------------- Chunked distance + rowwise z-score + fusion ----------------\n",
    "def _iter_chunks(n, chunk):\n",
    "    if chunk is None or chunk >= n:\n",
    "        yield 0, n\n",
    "    else:\n",
    "        for s in range(0, n, chunk):\n",
    "            e = min(s + chunk, n)\n",
    "            yield s, e\n",
    "\n",
    "def _compute_topk_for_chunk(i0, i1, Ktop=10):\n",
    "    # STFT distances (L2 on log-mag)\n",
    "    D_stft = _pairwise_l2(Q_STFT_F[i0:i1], REF_STFT_F).astype(np.float64)   # (q,R)\n",
    "    # EDC distances (L1 or L2)\n",
    "    if EDC_METRIC.upper() == \"L1\":\n",
    "        D_edc = _pairwise_l1(Q_EDC[i0:i1], REF_EDC).astype(np.float64)\n",
    "    else:\n",
    "        D_edc = _pairwise_l2(Q_EDC[i0:i1], REF_EDC).astype(np.float64)\n",
    "\n",
    "    # Rowwise z-score for each metric independently\n",
    "    Z_stft = _zscore_rowwise(D_stft)\n",
    "    Z_edc  = _zscore_rowwise(D_edc)\n",
    "\n",
    "    # Weighted fusion\n",
    "    Z_mix = W_STFT * Z_stft + W_EDC * Z_edc  # lower is better\n",
    "\n",
    "    # Top-K indices per row\n",
    "    topk_idx = np.argpartition(Z_mix, Ktop, axis=1)[:, :Ktop]  # unsorted top-K\n",
    "    # sort those K by fused distance\n",
    "    rows = Z_mix.shape[0]\n",
    "    out_idx = np.empty_like(topk_idx)\n",
    "    for r in range(rows):\n",
    "        idxs = topk_idx[r]\n",
    "        vals = Z_mix[r, idxs]\n",
    "        order = np.argsort(vals)\n",
    "        out_idx[r] = idxs[order]\n",
    "    return out_idx, Z_mix\n",
    "\n",
    "# Build mapping query_id -> top-10 ref IDs, excluding self if present in ref bank\n",
    "result = {}\n",
    "KTOP = 10\n",
    "num_chunks = sum(1 for _ in _iter_chunks(len(Q_IDS), Q_CHUNK))\n",
    "pbar = tqdm(total=num_chunks, desc=\"Compute & rank (chunked)\")\n",
    "for i0, i1 in _iter_chunks(len(Q_IDS), Q_CHUNK):\n",
    "    topk_idx_chunk, _ = _compute_topk_for_chunk(i0, i1, Ktop=KTOP + 5)  # take a few extra to filter leakage\n",
    "    for row, qi in enumerate(range(i0, i1)):\n",
    "        qid = Q_IDS[qi]\n",
    "        # map indices to ref IDs and filter out self if it appears\n",
    "        cand = [ref_ids[j] for j in topk_idx_chunk[row]]\n",
    "        cand_noself = [c for c in cand if c != qid]\n",
    "        result[qid] = cand_noself[:KTOP]\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "# ---------------- Save JSON ----------------\n",
    "with open(OUT_JSON_PATH, \"w\") as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print(f\"[saved] Top-10 retrieval JSON -> {OUT_JSON_PATH}\")\n",
    "print(\"Example item:\", next(iter(result.items())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
